{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8788d2",
   "metadata": {},
   "source": [
    "# Experimental Design for Gaussian Processes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd6c65",
   "metadata": {},
   "source": [
    "## Model-Based Design for GPs \n",
    "While space-filling designs like latin hypercube or maximim are agnostic to the model being fit to the data, another approach is to develop design strategies that are tailored to the specific model being used - in our case, a GP. \n",
    "\n",
    "We begin with the following setup. Suppose we are interesting in fitting a GP \n",
    "\n",
    "$$ y(\\cdot) \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$$\n",
    "\n",
    "over a $D$-dimensional input space. Let $\\tilde{\\mathbf{X}}$ denote the $M \\times D$ matrix containing a set of input points $\\tilde{\\mathbf{x}}_1, \\dots, \\tilde{\\mathbf{x}}_M$. From these $M$ inputs, we seek to choose an $N \\times D$ design $\\mathbf{X}$ consisting of inputs $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$ that is \"optimal\" in some sense.\n",
    "\n",
    "It will be helpful to establish some notation here. First let $\\overline{\\mathbf{X}}$ be the $(M - N) \\times D$ matrix contain the points not selected in the design $\\mathbf{X}$. I will denote the kernel matrices by $\\mathbf{K} := k(\\mathbf{X})$, $\\mathbf{\\tilde{K}} := k(\\mathbf{\\tilde{X}})$, $\\mathbf{\\overline{K}} := k(\\overline{\\mathbf{X}})$ and the random vectors $\\mathbf{y} := y(\\mathbf{X})$, $\\mathbf{\\tilde{y}} := y(\\mathbf{\\tilde{X}})$, $\\overline{\\mathbf{y}} := y(\\overline{\\mathbf{X}})$. \n",
    "\n",
    "Note that once we have selected $\\mathbf{X}$, then conditioning on the observed $\\mathbf{y}$ yields the standard GP predictive distribution over the unobserved points. \n",
    "\n",
    "$$ \\mathbf{\\overline{y}}|\\mathbf{y}, \\mathbf{X}, \\mathbf{\\overline{X}} \\sim \\mathcal{N}_{M - N}\\left(\\mu_{\\mathbf{X}}(\\overline{\\mathbf{X}}), k_{\\mathbf{X}}(\\overline{\\mathbf{X}})  \\right)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mu_{\\mathbf{X}}(\\overline{\\mathbf{X}}) &= k\\left(\\overline{\\mathbf{X}}, \\mathbf{X}\\right) \\mathbf{K}^{-1} \\mathbf{y} \\\\\n",
    "k_{\\mathbf{X}}(\\overline{\\mathbf{X}}) &= k\\left(\\overline{\\mathbf{X}}\\right) - k\\left(\\overline{\\mathbf{X}}, \\mathbf{X}\\right) \\mathbf{K}^{-1} k\\left(\\mathbf{X}, \\overline{\\mathbf{X}}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "I will let $\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}$ denote the random vector with this predictive distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad141668",
   "metadata": {},
   "source": [
    "### Maximum Entropy Design\n",
    "Intuitively, we seek to select $\\mathbf{X}$ such that it yields the most information about the predictive distribution over the unobserved points; that is, the selection that makes $\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}$ as uncertain as possible. As our measure of uncertainty, we will first consider Shannon's entropy\n",
    "\n",
    "$$ H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right) = -\\mathbb{E}\\left[\\log p_{\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}}\\left(\\overline{\\mathbf{y}}\\right)\\right]$$\n",
    "\n",
    "where the expectation is with respect to the $(M - N)$-dimensional Gaussian distribution of $\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}$. The negative of entropy is called *information*, and hence minimizing entropy to equivalent to maximizing information \n",
    "\n",
    "$$ I_{\\overline{\\mathbf{X}}|\\mathbf{X}} := -H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right)$$\n",
    "\n",
    " Since everything is Gaussian here, the formula for entropy of a Gaussian distribution will come up a lot. This is derived in the appendix for reference. Applying that formula to $\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}$, we find that the entropy of the predictive distribution over the outputs at the unobserved locations is given by \n",
    " \n",
    "$$\n",
    "\\begin{align*}\n",
    "H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right) &= \\frac{1}{2}\\log\\det\\left(2\\pi e k_{\\mathbf{X}}(\\overline{\\mathbf{X}})\\right) \\\\\n",
    "&= \\frac{1}{2}\\log\\det\\left(2\\pi e \\left[k\\left(\\overline{\\mathbf{X}}\\right) - k\\left(\\overline{\\mathbf{X}}, \\mathbf{X}\\right) \\mathbf{K}^{-1} k\\left(\\mathbf{X}, \\overline{\\mathbf{X}}\\right) \\right] \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now, as mentioned above, the goal is to select $\\mathbf{X}$ to minimize $H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right)$. Notice that the entropy of Gaussian distributions does not depend on the mean of the distributions. Since the GP predictive covariance $k_{\\mathbf{X}}$ does not depend on $\\mathbf{y}$ (the observed valued being conditioned on), this implies that \n",
    "$H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right)$ does not actually depend on $\\mathbf{y}$. The relation to $\\mathbf{y}$ is captured entirely through the kernel. In more general non-Gaussian settings this is not the case, and hence it is typical to instead consider minimizing $\\mathbb{E} H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right)$, where the expectation is with respect to the prior on $\\mathbf{y}$. Again, this is not required here but I state this in the interest of providing a more general picture. \n",
    " \n",
    "We will now show that minimizing $H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right)$ is equivalent to maximizing $H(\\mathbf{y})$; that is, the optimal design is to choose input points $\\mathbf{X}$ corresponding to the locations that are *most uncertain* under the GP prior. To verify this claim, we show that the prior entropy $H(\\tilde{\\mathbf{y}})$ can be decomposed as a sum of the prior entropy at the selected inputs $H(\\mathbf{y})$ and the entropy of the predictive distribution at the remaining inputs $H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right)$. To this end, note that the entropy of the prior at locations $\\tilde{\\mathbf{X}}$ is \n",
    "\n",
    "$$ H(\\tilde{\\mathbf{y}}) = \\frac{1}{2} \\log\\det\\left(2\\pi e \\tilde{\\mathbf{K}} \\right) = \\frac{N + M}{2} \\log(2\\pi e) + \\frac{1}{2} \\log\\det(\\tilde{\\mathbf{K}})$$\n",
    "\n",
    "where $\\tilde{\\mathbf{K}}$ can be written in block form as \n",
    "\n",
    "$$\\tilde{\\mathbf{K}} = \\begin{pmatrix} \\mathbf{K} & k(\\mathbf{X}, \\overline{\\mathbf{X}}) \\\\ \n",
    "k(\\overline{\\mathbf{X}}, \\mathbf{X}) & \\overline{\\mathbf{K}} \\end{pmatrix} $$\n",
    "\n",
    "We can re-write the determinant $\\det(\\tilde{\\mathbf{K}})$ in terms of each block using the formula for determinants of block matrices; see e.g., $\\href{https://math.stackexchange.com/questions/1905652/proofs-of-determinants-of-block-matrices}{this}$ StackExchange post. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\det(\\tilde{\\mathbf{K}}) &= \\det(\\mathbf{K}) \\cdot \\det\\left(\\overline{\\mathbf{K}} - k(\\overline{\\mathbf{X}}, \\mathbf{X}) \\mathbf{K}^{-1} k(\\mathbf{X}, \\overline{\\mathbf{X}})\\right) \\\\\n",
    "&= \\det(\\mathbf{K}) \\cdot \\det(k_{\\mathbf{X}}(\\overline{\\mathbf{X}}))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Plugging this back into the expression for $H(\\tilde{\\mathbf{y}})$, we obtain\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(\\tilde{\\mathbf{y}}) &= \\left\\{\\frac{N}{2} \\log(2\\pi e) + \\log\\det(\\mathbf{K}) \\right\\} + \\left\\{\\frac{M}{2} \\log(2\\pi e) + \\log\\det(k_{\\mathbf{X}}(\\overline{\\mathbf{X}})) \\right\\} \\\\\n",
    "&= H(\\mathbf{y}) + H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since the lefthand side - the entropy of the prior at locations $\\tilde{\\mathbf{X}}$ - is fixed, then we see that minimizing $H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right)$ is equivalent to maximizing $H(\\mathbf{y})$. This sort of entropy decomposition shows up in a variety of contexts; I emphasize that the decomposition is typically of the form \n",
    "\n",
    "$$ H(\\tilde{\\mathbf{y}}) = H(\\mathbf{y}) + \\mathbb{E}_{\\overline{\\mathbf{y}}} H\\left(\\overline{\\mathbf{y}}_{\\overline{\\mathbf{X}}|\\mathbf{X}}\\right)$$ \n",
    "where the expectation is with respect to the prior, but that in this special Gaussian setting th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca94cc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b228bd29",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae28837",
   "metadata": {},
   "source": [
    "## Information and Entropy of multivariate Gaussian\n",
    "Here we derive the Shannon information content of a multivariate Gaussian \n",
    "\n",
    "$$ X \\sim \\mathcal{N}_D(\\mu, \\Sigma)$$\n",
    "\n",
    "We have \n",
    "$$\n",
    "\\begin{align*}\n",
    "I(X) &= \\mathbb{E}\\left[\\log p_X(X) \\right] \\\\\n",
    "     &= -\\frac{1}{2}\\log\\det(2\\pi \\Sigma) - \\frac{1}{2} \\mathbb{E}\\left[(X - \\mu)^T \\Sigma^{-1} (X - \\mu) \\right] \\\\\n",
    "     &= -\\frac{1}{2}\\log\\det(2\\pi \\Sigma) - \\frac{1}{2} \\mu^T \\Sigma^{-1}\\mu - \\frac{1}{2}\\mathbb{E}\\left[X^T \\Sigma^{-1} X \\right] + \\mathbb{E}\\left[X^T \\Sigma^{-1} \\mu \\right] \\\\\n",
    "     &= -\\frac{1}{2}\\log\\det(2\\pi \\Sigma) - \\frac{1}{2} \\mu^T \\Sigma^{-1}\\mu - \\frac{1}{2}\\left[\\text{tr}(\\Sigma^{-1}\\Sigma) + \\mu^T \\Sigma^{-1} \\mu \\right] + \\mu^T \\Sigma^{-1} \\mu \\\\\n",
    "     &= -\\frac{1}{2}\\log\\det(2\\pi \\Sigma) - \\frac{1}{2} \\mu^T \\Sigma^{-1}\\mu - \\frac{D}{2} - \\frac{1}{2}\\mu^T \\Sigma^{-1} \\mu + \\mu^T \\Sigma^{-1} \\mu \\\\\n",
    "     &= -\\frac{1}{2}\\log\\det(2\\pi \\Sigma) - \\frac{D}{2} \\\\\n",
    "     &= -\\frac{1}{2}\\log\\det(2\\pi e \\Sigma)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The entropy is thus \n",
    "$$ H(X) = -I(X) = \\frac{1}{2}\\log\\det(2\\pi e \\Sigma)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
