---
title: "Experimental Design for GPs"
author: "Andrew Roberts"
date: '2023-05-15'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lhs)
library(laGP)
library(plgp)
library(hetGP)
```

# Space-Filling Designs

## Latin Hypercube Sampling (LHS)

## Maximin Sampling
The idea of maximin sampling is to maximize the minimum distance between two points - quite an intuitive and reasonable criterion if all we want to do is spread points out. Specifically, for some distance metric $d(\cdot, \cdot)$ the maximin criterion is 
$$ \mathbf{X} := \text{argmax}_{\mathbf{x}_1, \dots, \mathbf{x}_N} \min_{1 \leq i < j \leq N} d(\mathbf{x}_i, \mathbf{x}_j)$$
Although conceptually simple, maximin procedures offer a challenging optimization problem. Below, I paste code implementing a stochastic exchange algorithm from Grmacy's *Surrogates*. This is a very simple algorithm that proceeds as follows: start by generating a design uniformly at random. 
Then randomly select a design point to consider updating. Generate a proposed replacement uniformly at random. If the proposal reduces the maximin 
criterion accept it, otherwise reject and stick with the existing design. This procedure is then iterated many times. Clearly, there is a lot of room for improving the efficiency of this algorithm, but for our purposes this is sufficient. 

```{r}
mymaximin <- function(n, m, T=100000, Xorig = NULL) 
 {  
  
  if(is.null(Xorig)) X <- matrix(runif(n*m), ncol=m)  ## initial design
  
  d <- plgp::distance(X)
  d <- d[upper.tri(d)]
  md <- min(d)

  for(t in 1:T) {
    row <- sample(1:n, 1)
    xold <- X[row,]                   ## random row selection
    X[row,] <- runif(m)               ## random new row
    d <- plgp::distance(X)
    d <- d[upper.tri(d)]
    mdprime <- min(d)
    if(mdprime > md) { md <- mdprime  ## accept
    } else { X[row,] <- xold }        ## reject
  }

  return(X)
}
```



# Model-Based Design for GPs 
While space-filling designs like latin hypercube or maximim are agnostic to the model being fit to the data, another approach is to develop design strategies that are tailored to the specific model being used - in our case, a GP. 

We begin with the following setup. Suppose we are interesting in fitting a GP 

$$ y(\cdot) \sim \mathcal{GP}(0, k(\cdot, \cdot))$$

over a $D$-dimensional input space. Let $\tilde{\mathbf{X}}$ denote the $M \times D$ matrix containing a set of input points $\tilde{\mathbf{x}}_1, \dots, \tilde{\mathbf{x}}_M$. From these $M$ inputs, we seek to choose an $N \times D$ design $\mathbf{X}$ consisting of inputs $\mathbf{x}_1, \dots, \mathbf{x}_N$ that is "optimal" in some sense.

It will be helpful to establish some notation here. First let $\overline{\mathbf{X}}$ be the $(M - N) \times D$ matrix contain the points not selected in the design $\mathbf{X}$. I will denote the kernel matrices by $\mathbf{K} := k(\mathbf{X})$, $\mathbf{\tilde{K}} := k(\mathbf{\tilde{X}})$, $\mathbf{\overline{K}} := k(\overline{\mathbf{X}})$ and the random vectors $\mathbf{y} := y(\mathbf{X})$, $\mathbf{\tilde{y}} := y(\mathbf{\tilde{X}})$, $\overline{\mathbf{y}} := y(\overline{\mathbf{X}})$. 

Note that once we have selected $\mathbf{X}$, then conditioning on the observed $\mathbf{y}$ yields the standard GP predictive distribution over the unobserved points. 

$$ \mathbf{\overline{y}}|\mathbf{y}, \mathbf{X}, \mathbf{\overline{X}} \sim \mathcal{N}_{M - N}\left(\mu_{\mathbf{X}}(\overline{\mathbf{X}}), k_{\mathbf{X}}(\overline{\mathbf{X}})  \right)$$

where 

$$
\begin{align*}
\mu_{\mathbf{X}}(\overline{\mathbf{X}}) &= k\left(\overline{\mathbf{X}}, \mathbf{X}\right) \mathbf{K}^{-1} \mathbf{y} \\
k_{\mathbf{X}}(\overline{\mathbf{X}}) &= k\left(\overline{\mathbf{X}}\right) - k\left(\overline{\mathbf{X}}, \mathbf{X}\right) \mathbf{K}^{-1} k\left(\mathbf{X}, \overline{\mathbf{X}}\right)
\end{align*}
$$

I will let $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$ denote the random vector with this predictive distribution. 

Throughout I will assume a squared exponential kernel, following the parameterization utilized in Gramacy's *Surrogates* book. 
$$ k(\mathbf{x}_i, \mathbf{x}_j) = \tau^2 \left(c(\mathbf{x_i}, \mathbf{x}_j) + g \mathbb{1}_{ij} \right)$$
where 
$$c(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\sum_{d = 1}^{D} \frac{(\mathbf{x}_{id} - \mathbf{x}_{jd})^2}{\theta_d} \right)$$
Here, $\tau^2$ is the *marginal variance*, $g$ the *nugget*, and $\theta_1, \dots, \theta_d$ the *lengthscales*.  


### Maximum Entropy Design
Intuitively, we seek to select $\mathbf{X}$ such that it yields the most information about the predictive distribution over the unobserved points; that is, the selection that makes $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$ as uncertain as possible. As our measure of uncertainty, we will first consider Shannon's entropy

$$ H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right) = -\mathbb{E}\left[\log p_{\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}}\left(\overline{\mathbf{y}}\right)\right]$$

where the expectation is with respect to the $(M - N)$-dimensional Gaussian distribution of $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$. The negative of entropy is called *information*, and hence minimizing entropy to equivalent to maximizing information 

$$ I_{\overline{\mathbf{X}}|\mathbf{X}} := -H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$$

 Since everything is Gaussian here, the formula for entropy of a Gaussian distribution will come up a lot. This is derived in the appendix for reference. Applying that formula to $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$, we find that the entropy of the predictive distribution over the outputs at the unobserved locations is given by 
 
$$
\begin{align*}
H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right) &= \frac{1}{2}\log\det\left(2\pi e k_{\mathbf{X}}(\overline{\mathbf{X}})\right) \\
&= \frac{1}{2}\log\det\left(2\pi e \left[k\left(\overline{\mathbf{X}}\right) - k\left(\overline{\mathbf{X}}, \mathbf{X}\right) \mathbf{K}^{-1} k\left(\mathbf{X}, \overline{\mathbf{X}}\right) \right] \right)
\end{align*}
$$

Now, as mentioned above, the goal is to select $\mathbf{X}$ to minimize $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$. Notice that the entropy of Gaussian distributions does not depend on the mean of the distributions. Since the GP predictive covariance $k_{\mathbf{X}}$ does not depend on $\mathbf{y}$ (the observed valued being conditioned on), this implies that 
$H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$ does not actually depend on $\mathbf{y}$. The relation to $\mathbf{y}$ is captured entirely through the kernel. In more general non-Gaussian settings this is not the case, and hence it is typical to instead consider minimizing $\mathbb{E} H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$, where the expectation is with respect to the prior on $\mathbf{y}$. Again, this is not required here but I state this in the interest of providing a more general picture. 
 
We will now show that minimizing $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$ is equivalent to maximizing $H(\mathbf{y})$; that is, the optimal design is to choose input points $\mathbf{X}$ corresponding to the locations that are *most uncertain* under the GP prior. To verify this claim, we show that the prior entropy $H(\tilde{\mathbf{y}})$ can be decomposed as a sum of the prior entropy at the selected inputs $H(\mathbf{y})$ and the entropy of the predictive distribution at the remaining inputs $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$. To this end, note that the entropy of the prior at locations $\tilde{\mathbf{X}}$ is 

$$ H(\tilde{\mathbf{y}}) = \frac{1}{2} \log\det\left(2\pi e \tilde{\mathbf{K}} \right) = \frac{N + M}{2} \log(2\pi e) + \frac{1}{2} \log\det(\tilde{\mathbf{K}})$$

where $\tilde{\mathbf{K}}$ can be written in block form as 

$$\tilde{\mathbf{K}} = \begin{pmatrix} \mathbf{K} & k(\mathbf{X}, \overline{\mathbf{X}}) \\ 
k(\overline{\mathbf{X}}, \mathbf{X}) & \overline{\mathbf{K}} \end{pmatrix} $$

We can re-write the determinant $\det(\tilde{\mathbf{K}})$ in terms of each block using the formula for determinants of block matrices; see e.g., $\href{https://math.stackexchange.com/questions/1905652/proofs-of-determinants-of-block-matrices}{this}$ StackExchange post. 

$$
\begin{align*}
\det(\tilde{\mathbf{K}}) &= \det(\mathbf{K}) \cdot \det\left(\overline{\mathbf{K}} - k(\overline{\mathbf{X}}, \mathbf{X}) \mathbf{K}^{-1} k(\mathbf{X}, \overline{\mathbf{X}})\right) \\
&= \det(\mathbf{K}) \cdot \det(k_{\mathbf{X}}(\overline{\mathbf{X}}))
\end{align*}
$$

Plugging this back into the expression for $H(\tilde{\mathbf{y}})$, we obtain
$$
\begin{align*}
H(\tilde{\mathbf{y}}) &= \left\{\frac{N}{2} \log(2\pi e) + \log\det(\mathbf{K}) \right\} + \left\{\frac{M}{2} \log(2\pi e) + \log\det(k_{\mathbf{X}}(\overline{\mathbf{X}})) \right\} \\
&= H(\mathbf{y}) + H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)
\end{align*}
$$

Since the lefthand side - the entropy of the prior at locations $\tilde{\mathbf{X}}$ - is fixed, then we see that minimizing $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$ is equivalent to maximizing $H(\mathbf{y})$. This sort of entropy decomposition shows up in a variety of contexts; I emphasize that the decomposition is typically of the form 

$$ H(\tilde{\mathbf{y}}) = H(\mathbf{y}) + \mathbb{E}_{\overline{\mathbf{y}}} H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$$ 

where the expectation is with respect to the prior, but that in this special Gaussian setting the equality holds without the expectation. The *maximum entropy (maxent) design* of size $N$ is thus the set of input locations satisfying

$$ 
\begin{align*}
\mathbf{X} &= \text{argmax} H(\mathbf{y}) \\
           &= \text{argmax} \log\det(\mathbf{K}) \\
           &= \text{argmax} \det(\mathbf{K})
\end{align*}
$$

This is good and all, but there is a glaring problem. The kernel matrix $\mathbf{K}$ depends on the kernel hyperparameters. The whole point of what we're doing here is to choose a design well-suited to learning the kernel hyperparameters, so we've run into a circular issue here. To address this, one reasonable solution is to first initialize a small space-filling design (e.g. LHS or maximin), learn the kernel hyperparameters from this design, then append to this design in a sequential fashion using the maxent strategy. 


The following function *maxent* assumes the hyperparameters have already been estimated, and performs the sequential maxent procedure to refine the design. The implementation below is as simple as possible, and certainly not efficient. The sequential procedure proceeds by randomly selecting a design point to update, proposing a new design point uniformly at random, and then accepting the proposal if the objective function (the determinant of the kernel matrix at the design points) has increased. This will naturally lead to many rejections.  
```{r}

maxent <- function(N, D, theta = 0.01, g = 0.01, N_itr = 100000) {
  
  if(length(theta) == 1) theta <- rep(theta, D)
  X <- matrix(runif(N*D), ncol = D) # Initial design is generated uniformly at random. 
  K <- covar.sep(X, d=theta, g=g) # Correlation matrix plus nugget. 
  ldetK <- determinant(K, logarithm = TRUE)$modulus # Current value of objective function. 
  
  for(itr in 1:N_itr) {
    row <- sample(1:N, 1) # Randomly sample design point to update. 
    x_curr <- X[row,]
    X[row,] <- runif(D) # Generate new proposed design point uniformly at random. 
    K_prop <- covar.sep(X, d=theta, g=g)
    ldetK_prop <- determinant(K_prop, logarithm = TRUE)$modulus
    if(ldetK_prop > ldetK) {
      ldetK <- ldetK_prop
    } else {
      X[row,] <- x_curr
    }
    
  }
  
  return(X)
  
}

```


We test the function below, simply consider the hyperparameters to be fixed at their default values, rather than estimating their values from an initial design. We run the function for differnt numbers of iterations. When running for fewer iterations, we still see the "clumpiness" resulting from the uniform random initialization, but for higher numbers of iterations the maxent procedure removes this clumpiness and spreads the points out. 

```{r}

N_itr_vec <- c(1, 100, 1000, 10000)

for(N_itr in N_itr_vec) {
  X <- maxent(N = 25, D = 2, N_itr = N_itr)
  plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("maxent design; N_itr = ", N_itr))
}
```

Notice that the results are much more reminiscent of a maximin, rather than LHS, design. In particular, the marginal distributions look quite clumpy; the one-dimensional projections tend to cluster at a handful of values, rather than being spread out. What if we instead initialized the kernel hyperparameters with anisitropic lengthscales? If a lengthscale is much longer in, say, dimension 1 then dimension 2, then this expresses a prior belief that the underlying function is much more wiggly in dimension 2 than dimension 1. We would therefore expect that the resulting design would place points closer together in dimension 2 to allow for a finer grain view of the function, while in dimension 1 we could get away with points more spaced out. This is indeed the behavior we see in the below example. 
```{r}
N_itr <- 10000
X <- maxent(N = 25, D = 2, N_itr = N_itr, theta = c(0.1, 0.8))
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("maxent design; N_itr = ", N_itr))
```

### Predictive Uncertainty Design
We now consider an alternative criterion; instead of minimizing the entropy of the predictive distribution at unobserved points, 
we consider minimizing the predictive uncertainty over these unobserved points. Recall that the predictive GP variance at at unobserved point $\mathbf{x}$ is given by
by 
$$ k_{\mathbf{X}}(\mathbf{x}) = k(\mathbf{x}) - k(\mathbf{x}, \mathbf{X})\mathbf{K}^{-1}k(\mathbf{X}, \mathbf{x}) = \tau^2\left(1 + g - c(\mathbf{x}, \mathbf{X})\mathbf{C}^{-1}c(\mathbf{X}, \mathbf{x}) \right)$$
By definition of variance, this is equivalent to 
$$ k_{\mathbf{X}}(\mathbf{x}) = \mathbb{E}\left[\left(y(\mathbf{x}) - \mu_{\mathbf{X}}(\mathbf{x}) \right)^2 | \mathbf{X}, \mathbf{y} \right]$$
For this reason, the predictive variance is also often called the *mean-squared prediction error (MSPE)*. The MSPE is the average squared deviation between the GP prediction and its mean at the input point $\mathbf{x}$. We can then consider averaging the MSPE over the input space $\mathcal{X}$. We might consider a simple average that treats all input points $\mathbf{x}$ as equally important, or alternative consider a density $w(\mathbf{x})$ on $\mathbf{X}$ that designates certain inputs as more important than others. An *integrated MSPE (IMSPE)* criterion (as a function of the design $\mathbf{X}$) might then look like
$$ J(\mathbf{X}) = \int_{\mathcal{X}} \frac{k_{\mathbf{X}}(\mathbf{x})}{\tau^2} w(\mathbf{x}) d\mathbf{x}$$
where $\tau^2$ is the prior marginal variance of the GP. The division by $\tau^2$ is not that important; it simply normalizes the predictive variance by the prior variance. While this integral typically requires numerical methods, it is available in closed-form for simple cases, where
$\mathcal{X}$ is a simple domain such as a hypercube and the covariance function is isotropic or separable Gaussian or Matern 
(see Binois et al, 2019 for derivations). Below we utilize the ISMPE function implemented in *hetGP*. This function is implemented more generally to work with heteroskedastic noise, so we define a wrapper that simplifies it to our setting. 

```{r}
imspe_criteria <- function(X, theta, g, ...) {
  IMSPE(X, theta=theta, Lambda=diag(g, nrow(X)), covtype="Gaussian", mult=rep(1, nrow(X)), nu=1) 
}
```

Given this, we can simply replace the log determinant objective (derived from the maxent criterion) with the IMSPE objection, noting that we must slightly adjust the code to minimize instead of maximize. 

```{r}

min_IMSPE <- function(N, D, theta = 0.1, g = 0.01, N_itr = 100000, ...) {
  
  if(length(theta) == 1) theta <- rep(theta, D)
  X <- matrix(runif(N*D), ncol = D) # Initial design is generated uniformly at random. 
  IMSPE_curr <- imspe_criteria(X = X, theta = theta, g = g, ...) # Current value of objective function.
  
  for(itr in 1:N_itr) {
    row <- sample(1:N, 1) # Randomly sample design point to update. 
    x_curr <- X[row,]
    X[row,] <- runif(D) # Generate new proposed design point uniformly at random. 
    IMSPE_prop <- imspe_criteria(X = X, theta = theta, g = g, ...)
    if(IMSPE_prop < IMSPE_curr) {
      IMSPE_curr <- IMSPE_prop
    } else {
      X[row,] <- x_curr
    }
    
  }
  
  return(X)
  
}

```

We now run this on the same examples as before, increasing the number of iterations each time. 

```{r}

N_itr_vec <- c(1, 100, 1000, 10000)

for(N_itr in N_itr_vec) {
  X <- min_IMSPE(N = 25, D = 2, N_itr = N_itr)
  plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design; N_itr = ", N_itr))
}
```
And the example with non-isotropic covariance. 
```{r}
N_itr <- 10000
X <- min_IMSPE(N = 25, D = 2, N_itr = N_itr, theta = c(0.1, 0.8))
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design; N_itr = ", N_itr))
```
The results look pretty similar to the maxent design; however, the min IMSPE design tends to shy away from the boundaried of the input space more. The effect is minimal in two dimensions but will become much more prominent as the dimension increases. This is due to the fact that the criterion for choosing a single point $\mathcal{x}$ takes into account the error averaged over the entire input space. Choosing more central points will tend to have a greater effect than boundary points in reducing error averaged over the whole space, so IMSPE tends to favor interior points.

#### Extending to cases where IMSPE is not analytically tractable 
There are many cases that might break analytic tractability; in particular, here we consider the case of non-uniform weights $w(x)$ or the case where $\mathcal{X}$ is non-rectangular. The obvious solution here is to simply consider a Monte Carlo estimate of the 
integral, 

$$ J(\mathbf{X}) \approx \frac{1}{T} \sum_{t = 1}^{T} \frac{k_{\mathbf{X}}(\mathbf{x}_t)}{\tau^2} w(\mathbf{x}_t), \text{ where } \mathbf{x}_t \sim \mathcal{U}(\mathcal{X})$$

or 

$$ J(\mathbf{X}) \approx \frac{1}{T} \sum_{t = 1}^{T} \frac{k_{\mathbf{X}}(\mathbf{x}_t)}{\tau^2}, \text{ where } \mathbf{x}_t \sim w $$

Alternatively, we might consider a numerical integration or quadrature-based scheme. For example, instead of uniformly sampling points from $\mathcal{X}$ we might instead fix a grid of points and then average the integrand over these fixed points to estimate the integral. Such an appraoch is suitable in low dimensions, as in the two-dimensional example we have been considering. Let's apply such a grid-based scheme to this example, and compare the result to the result produced using the closed-form expression for the integral.  


```{r}
# Grid approximation of IMSPE. 
imspe_criteria <- function(X, theta, g, X_grid) {
  
  # Closed-form method.
  if(missing(X_grid)) {
    return(IMSPE(X, theta=theta, Lambda=diag(g, nrow(X)), covtype="Gaussian", mult=rep(1, nrow(X)), nu=1))
  }
  
  # Grid approximation. 
  C <- covar.sep(X, d=theta, g=g) 
  C_inv <- solve(C)
  C_cross <- covar.sep(X_grid, X, d=theta, g=0)
  integrand_evaluations <- 1 + g - diag(C_cross %*% C_inv %*% t(C_cross))
  
  return(mean(integrand_evaluations))
}
```

We now test the grid approximation. In addition to the sampled design points, the fixed grid points are plotted as small gray dots. 

```{r}
X_grid <- as.matrix(expand.grid(seq(0, 1, length=10), seq(0, 1, length=10)))
X <- min_IMSPE(N = 25, D = 2, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design grid approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

The approximate result looks pretty good. Let's think about what kind of error the grid approximation might induce. At each iteration, a new design point $\mathbf{x}$ is proposed as a replacement for a current one. The criterion that determines whether it is replaced or not is 
$$ \frac{1}{T} \sum_{t = 1}^{T} \frac{k_{\mathbf{X}}(\mathbf{x}_t)}{\tau^2} $$
where $\mathbf{x}_1, \dots, \mathbf{x}_T$ are the fixed grid points. Since the mean squared prediction error is only considered at the grid points $\mathbf{x}_t$ then naturally this method will tend to favor design points that are close to the grid points. When the number of grid points is large this might not be a big deal, but this error may become very noticeable when the number of grid points is small. Let's consider an example with a much smaller number of grid points. 
```{r}
X_grid <- as.matrix(expand.grid(seq(0, 1, length=3), seq(0, 1, length=3)))
X <- min_IMSPE(N = 25, D = 2, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design grid approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

Indeed, we observe that the design points are clustered around the grid points. Trying to approximate the IMSPE integral over a two-dimensional input space with 9 grid points unsurprisingly results in a poor approximation. Another interesting consequence 
of this grid approximation is that design points will tend to be pulled to the boundaries, whereas we noticed that the exact IMSPE results tended to shy away from the boundaries. This is due to the nature of selecting a grid. The ratio of the number of grid points on the boundaries vs. the number of interior points is much higher than the ratio of the "volume" of the boundaries vs. the volume of the interior. Thus, the boundary grid points will pull design points towards the boundaries moreso than the exact solution. 

We now return to the denser grid, but entertain increasing the lengthscale. Intuitively, we expect a longer lengthscale to yield even more spread out points. However, this will come into conflict with the tendency for design points to want to stay close to the grid points. We present results for the closed-form and grid approximated solutions below. We observe some odd behavior from the grid approximation. 

```{r}
X <- min_IMSPE(N = 25, D = 2, theta = 0.5, N_itr = 10000)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design closed-form; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

```{r}
X_grid <- as.matrix(expand.grid(seq(0, 1, length=10), seq(0, 1, length=10)))
X <- min_IMSPE(N = 25, D = 2, theta = 0.5, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design grid approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

Gramacy presents an interesting example illustrating the case where the approximate solution allows sampling desing points on irregularly shaped domains; in particular, he presents an example where the domain consists of two intersecting ellipsoids. 
In essence, his example boils down to assuming the density $w(\mathbf{x})$ weighting the input space is a mixture of Gaussians. 
$$ w(\mathbf{x}) = \frac{1}{2}\mathcal{N}_2(\mathbf{x}|\mathbf{\mu}_1, \mathbf{\Sigma}_1) + \frac{1}{2}\mathcal{N}_2(\mathbf{x}|\mathbf{\mu}_2, \mathbf{\Sigma}_2)$$
The below code more-or-less samples from $\mathbf{w}(\mathbf{x})$ (instead of randomly choosing which Gaussian to sample from, 
it simply chooses equal sample sizes from each) to construct the grid, and then proceeds as before. 
```{r}
X_grid_1 <- rmvnorm(100, mean=c(0.25, 0.25), sigma=0.005*rbind(c(2, 0.35), c(0.35, 0.1)))
X_grid_2 <- rmvnorm(100, mean=c(0.25, 0.25), sigma=0.005*rbind(c(0.1, -0.35), c(-0.35, 2)))
X_grid <- rbind(X_grid_1, X_grid_2)

X <- min_IMSPE(N = 25, D = 2, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design Monte Carlo approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

# Sequential Design/Active Learning
We have currently entertained the notion of *batch design*; that is, selecting the $N$ design points $\mathbf{X}$ in one fell swoop. An alternative, often very practical approach, is to choose that points one at a time - at each iteration selecting the next point based on the criterion 
$J(\mathbf{x})$. The general idea is as follows:

1. Start with a small design $\mathbf{X}_{N_0}$ consisting of $N_0$ inputs typically chosen via some sort of model-free, space-filling design 
(e.g. LHS). Run the forward model at these inputs and fit the GP on this initial dataset $\mathcal{D}_{N_0}$ (e.g. estimate the hyperparameters via MLE). 
Set $n := N_0$. 
2. Select the next design point via 
$$ \mathbf{x}_{n + 1} := \text{argmax}_{\mathbf{x}} J(\mathbf{x})|\mathcal{D}_n$$
Augment the design $\mathbf{X}_{n + 1} := \mathbf{X}_n \cup \mathbf{x}_{n + 1}$ and run the forward model at the new input point. Denote the 
new augmented dataset $\mathcal{D}_{n + 1}$. Re-fit the GP on the new dataset. 
3. Repeat step 2 until a desired number of design points $N$ is reached. 

There are many optimization tricks and pitfalls when considering such sequential schemes. For example, one might not update the MLE GP fit every iteration to save on computation, but this could potentially lead to unsatisfactory results. It is also common to *warm start* the MLE hyperparameter 
optimization but setting the initial values in the optimization to the values of the hyperparameters that were optimized in the previous step. 

Perhaps the simplest sequential design criterion is to choose the point that maximizes the predictive variance. The idea is that if the predictive variance is very high at a point, then the GP is very uncertain about the latent function at that point so we should run the model at that point to 
reduce that uncertainty. The objective thus looks like
$$ \mathbf{x}_{n + 1} := \text{argmax}_{\mathbf{x}} J(\mathbf{x})|\mathcal{D}_n = \text{argmax}_{\mathbf{x}} k_{\mathbf{X}_n}(\mathbf{x})$$
This method is known as *Active Learning MacKay (ALM)*, after David MacKay, who explored this notion of design for neural networks and argued that this sequential design strategy tends to approximate a maxent design. 

Let's consider a toy example with 2D input space. The below chode chunk defines the latent function we seek to learn. The function is a scaled 
and shifted version of 
$$ f(x_1, x_2) = x_1 \exp(-x_1^2 - x_2^2)$$
A heat map provides a visualization of this function below. 

```{r}
f <- function(X, sd=0.01) {
  X[,1] <- (X[,1] - 0.5)*6 + 1
  X[,2] <- (X[,2] - 0.5)*6 + 1
  f_vals <- X[,1] * exp(-X[,1]^2 - X[,2]^2)
  y_vals <- f_vals + rnorm(nrow(X), sd=sd)
  
  return(list(f = f_vals, y = y_vals))
}
```


```{r}
# Evaluate function at initial space-filling design.
N0 <- 12
X <- randomLHS(N0, 2)
f_list <- f(X)
f_vals <- f_list$f
y <- f_list$y

# Also evaluate at dense grid for plotting. 
X_grid <- as.matrix(expand.grid(seq(0, 1, length=51), seq(0, 1, length=51)))
f_grid <- f(X_grid)$f
```

```{r}
cols <- heat.colors(128)

# Latent function, noiseless evaluations. 
image(seq(0, 1, length=51), seq(0, 1, length=51), matrix(f_grid, ncol=51), 
      xlab="x1", ylab="x2", main = "Noiseless evaluations of f(), and initial design points.", col=cols)
points(X[,1], X[,2])
```

We observe that this is a tricky function, with a steep peak right next to a deep trough. Beyond these two modes, the function is quite flat. So there are separate regimes of the input space, corresponding to sub-domains on which the function is highly non-linear and the remaining sub-domain on which the function is flat. 

We now fit a GP using the *laGP* package on this initial design. 
```{r}
g <- garg(list(mle=TRUE, max=1), y)
d <- darg(list(mle=TRUE, max=0.25), X)
gp0 <- newGP(X, y, d=d$start, g=g$start, dK=TRUE)
mle <- jmleGP(gp0, c(d$min, d$max), c(g$min, g$max), d$ab, g$ab)
```

Now, let's turn to the sequential design problem with ALM objective (i.e. activation) function. At each iteration of this procedure, the selection of the next point requires solution of an optimization problem 
$$ \mathbf{x}_{n + 1} := \text{argmax}_{\mathbf{x}} k_{\mathbf{X}_n}(\mathbf{x})$$
This problem may be challenging in and of itself. Recall that predictive GP variances $k_{\mathbf{X}_n}(\mathbf{x})$ form a sort of "sausage" shape, shrinking at the observed locations and increasing as $\mathbf{x}$ moves away from observed locations. This means that the objective $k_{\mathbf{X}_n}(\mathbf{x})$ may have many local minima, in particular on the order of the number of observed locations. This presents a challenge for local optimization methods. To deal with this challenge, a common approach is to run a local optimizer multiple times initialized at different starting locations. The minimum of all the different runs can then be taken as $\mathbf{x}_{n + 1}$. The choice of starting locations for the optimizer is a whole design problem in its own right. Intuitively, we don't want 1.) starting locations to be close to each other and 2.) two starting locations to both be close to the same local minimum. In particular, imagine in a 1d problem that two starting locations are placed on the same "sausage"; we then expect the local minimizers both to converge to the same local minima. Thus, it seems that the ideal starting locations are those corresponding to large predictive variances; that is, the "fat" parts of the sausage. To this end, a space-filling design such as maximin seems a reasonable choice for the starting locations. 

The below code implements the ALM objective, as well as a solution to the ALM search for $\mathbf{x}_{n + 1}$. 

```{r}
# Note that `optim()` minimizes by default, hence the negative. 
obj_alm <- function(x, gp) {-sqrt(predGP(gp, matrix(x, nrow=1), lite=TRUE)$s2)}
```


```{r}
xnp1_2d_search <- function(X, gp, obj=obj_alm, ...) {
  # X = current design (x_1, ..., x_n). 
  
  start_locs <- mymaximin(nrow(X), 2, T=100*nrow(X), Xorig=X) # Starting locations for optimization runs generated by maximin.
  local_optima <- matrix(NA, nrow=nrow(start_locs), ncol=ncol(X) + 1)

  for(i in 1:nrow(start_locs)) {
    optim_results <- optim(start_locs[i,], obj, method="L-BFGS-B", lower=0, upper=1, gp=gp, ...) # Run optimization at each starting location. 
    local_optima[i,] <- c(optim_results$par, -optim_results$value)
  }
  solns <- data.frame(cbind(start_locs, local_optima))
  names(solns) <- c("init1", "init2", "optim1", "optim2", "optim_val")
  return(solns)
  
}

```

We now feed the initial LHS design to the one-step search function, and observe how they are updated. To recap, here is what is happening: 

* We created an initial space-filling design $\mathbf{X}_n$ with $n$ inputs using LHS. 
* The GP kernel hyperparameters are estimated using this initial design. 
* We now seek to choose $\mathbf{x}_{n + 1}$ by maximizing the ALM criterion. To do so, we run an L-BFGS optimizer from multiple starting locations. 
* The starting locations for the optimizer are chosen by applying a *maximin* procedure ot the initial design $\mathbf{X}_n$. 
* In the plot below, the arrows are drawn from the initial locations fed to the optimizer to the optimized values returned by the optimizer. In practice, we would of course choose the point returned by the optimizer that resulted in the largest ALM criterion as the point $\mathbf{x}_{n+1}$. This point is plotted in red below. Arrows with near-zero length are omitted. The points on the plot compose the initial design $\mathbf{X}_n$. 

```{r}
solns <- xnp1_2d_search(X, gp0)
image(seq(0, 1, length=51), seq(0, 1, length=51), matrix(f_grid, ncol=51), 
      xlab="x1", ylab="x2", col=cols)
points(X, xlab="x1", ylab="x2", xlim=c(0,1), ylim=c(0,1))
arrows(solns$init1, solns$init2, solns$optim1, solns$optim2, length=0.1)
m <- which.max(solns$optim_val)
prog <- solns$val[m]
points(solns$optim1[m], solns$optim2[m], col="red", pch=20)
```
We clearly see that there are many local optima; hence, the multiple initialization scheme is essential here. In high dimensions, the number of initialization points for the optimizer would have be very, very large to give us any sort of confidence that we are finding the global maximum. In, say, 100 dimensions, running with 100 initializations essentially gives us no guarantee that we have achieved the global minimum. We also note that, given that there are few points in the initial design and this is only the first point chosen sequentially, it is likely that the point will be close to a boundary. This is due to the fact that the initial design was chosen via LHS, which does not tend to clump at the boundaries. And the GP will typically be more uncertain near the boundaries, so the effect of the ALM criterion is to encourage points that fill out the boundaries. 

With $\mathbf{x}_{n+1}$ in hand we now run the forward model to obtain $y_{n+1} = y(\mathbf{x}_{n})$, augment the dataset to include 
$(\mathbf{x}_{n+1}, y_{n+1})$ to produce $\mathcal{D}_{n+1}$, and then re-fit the GP on the new dataset. If it seems wasteful to re-run the 
entire MLE (an $O(n^3)$ operation) when $\mathcal{D}_{n+1}$ only differs by $\mathcal{D}_{n}$ by a single point, that's because it is. Later on, 
we will show that this update can be performed in $O(n^2)$ time instead. For now, we will use the fast GP update provided by 
`laGP::updateGP`. We also keep track of root mean square error evaluated at the same set of grid points used to plot the heatmap. 

```{r}

# Size of initial design and final design. 
N0 <- 12
N <- 25

# Initial space-filling design. Run forward model on initial design. 
X <- randomLHS(N0, 2)
f_list <- f(X)
f_vals <- f_list$f
y <- f_list$y 

# Initial GP fit.
g <- garg(list(mle=TRUE, max=1), y)
d <- darg(list(mle=TRUE, max=0.25), X)
gp <- newGP(X, y, d=d$start, g=g$start, dK=TRUE)
mle <- jmleGP(gp0, c(d$min, d$max), c(g$min, g$max), d$ab, g$ab)

# Sequential design. 
objective_value <- vector(mode = "numeric", length = N-N0)
for(n in seq(1, N - N0)) {
  
  # Optimize ALM to get new point. 
  solns <- xnp1_2d_search(X, gp)
  m <- which.max(solns$optim_val)
  objective_value[n] <- solns$val[m]
  x_new <- matrix(solns[m, c("optim1", "optim2")], nrow = 1)
  
  # Run forward model at new point and augment dataset. 
  y_new <- f(x_new)
  X <- rbind(X, x_new)
  y <- c(y, y_new)
  
  # Update GP hyperparameter fit. 
  updateGP(gp, x_new, y_new)
  mle <- rbind(mle, jmleGP(gpi, c(d$min, d$max), c(g$min, g$max), d$ab, g$ab))
  
}




image(seq(0, 1, length=51), seq(0, 1, length=51), matrix(f_grid, ncol=51), 
      xlab="x1", ylab="x2", col=cols)
points(X, xlab="x1", ylab="x2", xlim=c(0,1), ylim=c(0,1))
arrows(solns$init1, solns$init2, solns$optim1, solns$optim2, length=0.1)
m <- which.max(solns$optim_val)
prog <- solns$val[m]


```






# Sequential Design for Bayesian Inverse Problems
The model-based design strategies discussed above seek to select a point $\mathbf{x}_{n+1}$ that is optimal in some sense for improving the GP fit. These strategies have presumed that we care about the GP fit uniformly across the input space. However, in the setting of Bayesian inverse problems it is common that 
the posterior distribution is highly concentrated in the input space. Thus, in most of the input space the posterior will effectively be zero, so we care much less about GP predictive performance in these regions. Instead, we would like a design strategy that produces design points in the region of actual importance; that is, the region of significant posterior mass. This is yet another chicken-and-egg type problem, since the ultimate goal of solving a Bayesian inverse problem is to characterize the posterior distribution, which is unknown in advance. However, each time we run the forward model at a new design points $\mathbf{x}_n$ we can also evaluate (up to a normalizing constant) the posterior density; thus, as sequential design proceeds we get more and more information about the posterior. We can thus try for an active learning approach, which chooses $\mathbf{x}_{n + 1}$ based on an approximation to the posterior constructed using 
$\mathbf{x}_{1}, \dots, \mathbf{x}_{n}$. Once $\mathbf{x}_{n + 1}$ is selected and the forward model run at this input point, the posterior approximation can be refined and then used to select the subsequent point. To my knowledge, this type of strategy has not been deeply explored in the literature. We start by outlining an approach proposed by Micheal Sinsbeck and Wolfgang Nowak in 2017. 

First, we establish some notation. The posterior of interest is 
$$ \pi(\mathbf{x}) \propto \mathcal{L}(\mathbf{x})\pi_0(\mathbf{x})$$
Evaluating the likelihood $\mathcal{L}(\mathbf{x})$ requires running the expensive forward model, so we opt to approximate the likelihood with a surrogate 
$\hat{\mathcal{L}}(\mathbf{x})$. We assume a GP surrogate has been used, either to directly emulate the likelihood itself, or to emulate the forward model, or to emulate some sufficient statistic of the likelihood.  
We will entertain a sequential design strategy, so let $\hat{\mathcal{L}}_n$ denote the likelihood approximation resulting from conditioning the GP on the data observed at the first $n$ design points $\mathbf{x}_1, \dots, \mathbf{x}_n$. Let $y^*_n$ denote the GP conditioned on these same points, which may or may not be the same as $\hat{\mathcal{L}}_n$, depending on what specifically the GP is emulating. 


## Sinsbeck and Nowak Approach
The Sinsbeck and Nowak (SN) approach seeks to select design points based on an adaptively-improved likelihood approximation. They comment that this is easier than constructing a posterior approximation, as the posterior has the constraint that it must integrate to 1, among other difficulties. 

Supposing the likelihood is given by a probability density, they consider an $L_2$, prior-weighted error between likelihoods:
$$\ell(f, g) = \mathbb{E}_{\pi_0}\left[(f(\mathbf{x}) - g(\mathbf{x}))^2 \right]$$
Note that the likelihood is the Radon-Nikodym derivative of the posterior with respect to the prior, so assessing the error in the likelihood approximation with respect to the prior measure seems a resonable thing to do. Of course, $\ell(\mathcal{L}, \cdot)$ cannot be computed without knowledge of the true likelihood $\mathcal{L}$. The idea is thus to replace $\mathcal{L}$ with the random approximation $\hat{\mathcal{L}}_n$ and average the whole error over
$y^*_n$.


## My personal notes:
As motivation, first recall the IMSPE activation function from above: 
$$ J(\mathbf{X}) = \int k_{\mathbf{X}}(\mathbf{x}) d\mathbf{x}$$
In the definition provided above, we also normalized by the marginal prior variance and multiplied by weighting function $w(\mathbf{x})$ but the idea is the same. The idea with this activation function is to choose the design $\mathbf{X}$ that minimizes the average predictive uncertainty in the GP. We also explored the 
ALM activation function for sequential design
$$ J(\mathbf{x}) = k_{\mathbf{X}_n}(\mathbf{x})$$
where similarly, the next input is chosen such that it corresponds to the location where the GP is currently most uncertain. Both of these have intuitive appeal, but may not be ideal in the Bayesian inverse problem setting. The IMSPE averages over the entire input space, but the posterior may only be concentrated in a small subset of the input space. Similarly, ALM chooses a point where the GP is most uncertain, but maybe that point corresponds to a location where the posterior is essentially 0. The SN approach addresses this by acknowledging that it is not really the GP itself we care about, but really the approximation of the posterior induced by the GP. As mentioned above, they actually focus on the approximation to the likelihood, but we should keep in mind that the ultimate focus is the posterior. Letting $\mathcal{L}_n$ denote the GP-induced likelihood approximation constructed from a set of inputs $\mathbf{x}_1, \dots, \mathbf{x}_n$ a reasonable modification of the ALM criterion is 
$$ J(\mathbf{x}) = \text{Var}_{y_n}(\mathcal{L}_n(\mathbf{x}))$$
where now the predictive variance of $\mathcal{L}_n$ is considered instead of the predictive variance of $y_n$. This directly targets uncertainty in the likelihood approximation but still doesn't address the issue of favoring regions of the input space of high posterior mass. 


# Appendix 

## Information and Entropy of multivariate Gaussian
Here we derive the Shannon information content of a multivariate Gaussian 

$$ X \sim \mathcal{N}_D(\mu, \Sigma)$$

We have 
$$
\begin{align*}
I(X) &= \mathbb{E}\left[\log p_X(X) \right] \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mathbb{E}\left[(X - \mu)^T \Sigma^{-1} (X - \mu) \right] \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mu^T \Sigma^{-1}\mu - \frac{1}{2}\mathbb{E}\left[X^T \Sigma^{-1} X \right] + \mathbb{E}\left[X^T \Sigma^{-1} \mu \right] \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mu^T \Sigma^{-1}\mu - \frac{1}{2}\left[\text{tr}(\Sigma^{-1}\Sigma) + \mu^T \Sigma^{-1} \mu \right] + \mu^T \Sigma^{-1} \mu \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mu^T \Sigma^{-1}\mu - \frac{D}{2} - \frac{1}{2}\mu^T \Sigma^{-1} \mu + \mu^T \Sigma^{-1} \mu \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{D}{2} \\
     &= -\frac{1}{2}\log\det(2\pi e \Sigma)
\end{align*}
$$

The entropy is thus 
$$ H(X) = -I(X) = \frac{1}{2}\log\det(2\pi e \Sigma)$$

# References 
Binois, M, J Huang, RB Gramacy, and M Ludkovski. 2019. “Replication or Exploration? Sequential Design for Stochastic Simulation Experiments.” Technometrics 27 (4): 808–21. https://doi.org/10.1080/00401706.2018.1469433.






