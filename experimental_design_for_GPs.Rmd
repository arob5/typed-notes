---
title: "Experimental Design for GPs"
author: "Andrew Roberts"
date: '2023-05-15'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lhs)
library(laGP)
library(plgp)
library(hetGP)
```

## Model-Based Design for GPs 
While space-filling designs like latin hypercube or maximim are agnostic to the model being fit to the data, another approach is to develop design strategies that are tailored to the specific model being used - in our case, a GP. 

We begin with the following setup. Suppose we are interesting in fitting a GP 

$$ y(\cdot) \sim \mathcal{GP}(0, k(\cdot, \cdot))$$

over a $D$-dimensional input space. Let $\tilde{\mathbf{X}}$ denote the $M \times D$ matrix containing a set of input points $\tilde{\mathbf{x}}_1, \dots, \tilde{\mathbf{x}}_M$. From these $M$ inputs, we seek to choose an $N \times D$ design $\mathbf{X}$ consisting of inputs $\mathbf{x}_1, \dots, \mathbf{x}_N$ that is "optimal" in some sense.

It will be helpful to establish some notation here. First let $\overline{\mathbf{X}}$ be the $(M - N) \times D$ matrix contain the points not selected in the design $\mathbf{X}$. I will denote the kernel matrices by $\mathbf{K} := k(\mathbf{X})$, $\mathbf{\tilde{K}} := k(\mathbf{\tilde{X}})$, $\mathbf{\overline{K}} := k(\overline{\mathbf{X}})$ and the random vectors $\mathbf{y} := y(\mathbf{X})$, $\mathbf{\tilde{y}} := y(\mathbf{\tilde{X}})$, $\overline{\mathbf{y}} := y(\overline{\mathbf{X}})$. 

Note that once we have selected $\mathbf{X}$, then conditioning on the observed $\mathbf{y}$ yields the standard GP predictive distribution over the unobserved points. 

$$ \mathbf{\overline{y}}|\mathbf{y}, \mathbf{X}, \mathbf{\overline{X}} \sim \mathcal{N}_{M - N}\left(\mu_{\mathbf{X}}(\overline{\mathbf{X}}), k_{\mathbf{X}}(\overline{\mathbf{X}})  \right)$$

where 

$$
\begin{align*}
\mu_{\mathbf{X}}(\overline{\mathbf{X}}) &= k\left(\overline{\mathbf{X}}, \mathbf{X}\right) \mathbf{K}^{-1} \mathbf{y} \\
k_{\mathbf{X}}(\overline{\mathbf{X}}) &= k\left(\overline{\mathbf{X}}\right) - k\left(\overline{\mathbf{X}}, \mathbf{X}\right) \mathbf{K}^{-1} k\left(\mathbf{X}, \overline{\mathbf{X}}\right)
\end{align*}
$$

I will let $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$ denote the random vector with this predictive distribution. 

Throughout I will assume a squared exponential kernel, following the parameterization utilized in Gramacy's *Surrogates* book. 
$$ k(\mathbf{x}_i, \mathbf{x}_j) = \tau^2 \left(c(\mathbf{x_i}, \mathbf{x}_j) + g \mathbb{1}_{ij} \right)$$
where 
$$c(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\sum_{d = 1}^{D} \frac{(\mathbf{x}_{id} - \mathbf{x}_{jd})^2}{\theta_d} \right)$$
Here, $\tau^2$ is the *marginal variance*, $g$ the *nugget*, and $\theta_1, \dots, \theta_d$ the *lengthscales*.  


### Maximum Entropy Design
Intuitively, we seek to select $\mathbf{X}$ such that it yields the most information about the predictive distribution over the unobserved points; that is, the selection that makes $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$ as uncertain as possible. As our measure of uncertainty, we will first consider Shannon's entropy

$$ H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right) = -\mathbb{E}\left[\log p_{\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}}\left(\overline{\mathbf{y}}\right)\right]$$

where the expectation is with respect to the $(M - N)$-dimensional Gaussian distribution of $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$. The negative of entropy is called *information*, and hence minimizing entropy to equivalent to maximizing information 

$$ I_{\overline{\mathbf{X}}|\mathbf{X}} := -H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$$

 Since everything is Gaussian here, the formula for entropy of a Gaussian distribution will come up a lot. This is derived in the appendix for reference. Applying that formula to $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$, we find that the entropy of the predictive distribution over the outputs at the unobserved locations is given by 
 
$$
\begin{align*}
H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right) &= \frac{1}{2}\log\det\left(2\pi e k_{\mathbf{X}}(\overline{\mathbf{X}})\right) \\
&= \frac{1}{2}\log\det\left(2\pi e \left[k\left(\overline{\mathbf{X}}\right) - k\left(\overline{\mathbf{X}}, \mathbf{X}\right) \mathbf{K}^{-1} k\left(\mathbf{X}, \overline{\mathbf{X}}\right) \right] \right)
\end{align*}
$$

Now, as mentioned above, the goal is to select $\mathbf{X}$ to minimize $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$. Notice that the entropy of Gaussian distributions does not depend on the mean of the distributions. Since the GP predictive covariance $k_{\mathbf{X}}$ does not depend on $\mathbf{y}$ (the observed valued being conditioned on), this implies that 
$H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$ does not actually depend on $\mathbf{y}$. The relation to $\mathbf{y}$ is captured entirely through the kernel. In more general non-Gaussian settings this is not the case, and hence it is typical to instead consider minimizing $\mathbb{E} H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$, where the expectation is with respect to the prior on $\mathbf{y}$. Again, this is not required here but I state this in the interest of providing a more general picture. 
 
We will now show that minimizing $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$ is equivalent to maximizing $H(\mathbf{y})$; that is, the optimal design is to choose input points $\mathbf{X}$ corresponding to the locations that are *most uncertain* under the GP prior. To verify this claim, we show that the prior entropy $H(\tilde{\mathbf{y}})$ can be decomposed as a sum of the prior entropy at the selected inputs $H(\mathbf{y})$ and the entropy of the predictive distribution at the remaining inputs $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$. To this end, note that the entropy of the prior at locations $\tilde{\mathbf{X}}$ is 

$$ H(\tilde{\mathbf{y}}) = \frac{1}{2} \log\det\left(2\pi e \tilde{\mathbf{K}} \right) = \frac{N + M}{2} \log(2\pi e) + \frac{1}{2} \log\det(\tilde{\mathbf{K}})$$

where $\tilde{\mathbf{K}}$ can be written in block form as 

$$\tilde{\mathbf{K}} = \begin{pmatrix} \mathbf{K} & k(\mathbf{X}, \overline{\mathbf{X}}) \\ 
k(\overline{\mathbf{X}}, \mathbf{X}) & \overline{\mathbf{K}} \end{pmatrix} $$

We can re-write the determinant $\det(\tilde{\mathbf{K}})$ in terms of each block using the formula for determinants of block matrices; see e.g., $\href{https://math.stackexchange.com/questions/1905652/proofs-of-determinants-of-block-matrices}{this}$ StackExchange post. 

$$
\begin{align*}
\det(\tilde{\mathbf{K}}) &= \det(\mathbf{K}) \cdot \det\left(\overline{\mathbf{K}} - k(\overline{\mathbf{X}}, \mathbf{X}) \mathbf{K}^{-1} k(\mathbf{X}, \overline{\mathbf{X}})\right) \\
&= \det(\mathbf{K}) \cdot \det(k_{\mathbf{X}}(\overline{\mathbf{X}}))
\end{align*}
$$

Plugging this back into the expression for $H(\tilde{\mathbf{y}})$, we obtain
$$
\begin{align*}
H(\tilde{\mathbf{y}}) &= \left\{\frac{N}{2} \log(2\pi e) + \log\det(\mathbf{K}) \right\} + \left\{\frac{M}{2} \log(2\pi e) + \log\det(k_{\mathbf{X}}(\overline{\mathbf{X}})) \right\} \\
&= H(\mathbf{y}) + H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)
\end{align*}
$$

Since the lefthand side - the entropy of the prior at locations $\tilde{\mathbf{X}}$ - is fixed, then we see that minimizing $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$ is equivalent to maximizing $H(\mathbf{y})$. This sort of entropy decomposition shows up in a variety of contexts; I emphasize that the decomposition is typically of the form 

$$ H(\tilde{\mathbf{y}}) = H(\mathbf{y}) + \mathbb{E}_{\overline{\mathbf{y}}} H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$$ 

where the expectation is with respect to the prior, but that in this special Gaussian setting the equality holds without the expectation. The *maximum entropy (maxent) design* of size $N$ is thus the set of input locations satisfying

$$ 
\begin{align*}
\mathbf{X} &= \text{argmax} H(\mathbf{y}) \\
           &= \text{argmax} \log\det(\mathbf{K}) \\
           &= \text{argmax} \det(\mathbf{K})
\end{align*}
$$

This is good and all, but there is a glaring problem. The kernel matrix $\mathbf{K}$ depends on the kernel hyperparameters. The whole point of what we're doing here is to choose a design well-suited to learning the kernel hyperparameters, so we've run into a circular issue here. To address this, one reasonable solution is to first initialize a small space-filling design (e.g. LHS or maximin), learn the kernel hyperparameters from this design, then append to this design in a sequential fashion using the maxent strategy. 


The following function *maxent* assumes the hyperparameters have already been estimated, and performs the sequential maxent procedure to refine the design. The implementation below is as simple as possible, and certainly not efficient. The sequential procedure proceeds by randomly selecting a design point to update, proposing a new design point uniformly at random, and then accepting the proposal if the objective function (the determinant of the kernel matrix at the design points) has increased. This will naturally lead to many rejections.  
```{r}

maxent <- function(N, D, theta = 0.01, g = 0.01, N_itr = 100000) {
  
  if(length(theta) == 1) theta <- rep(theta, D)
  X <- matrix(runif(N*D), ncol = D) # Initial design is generated uniformly at random. 
  K <- covar.sep(X, d=theta, g=g) # Correlation matrix plus nugget. 
  ldetK <- determinant(K, logarithm = TRUE)$modulus # Current value of objective function. 
  
  for(itr in 1:N_itr) {
    row <- sample(1:N, 1) # Randomly sample design point to update. 
    x_curr <- X[row,]
    X[row,] <- runif(D) # Generate new proposed design point uniformly at random. 
    K_prop <- covar.sep(X, d=theta, g=g)
    ldetK_prop <- determinant(K_prop, logarithm = TRUE)$modulus
    if(ldetK_prop > ldetK) {
      ldetK <- ldetK_prop
    } else {
      X[row,] <- x_curr
    }
    
  }
  
  return(X)
  
}

```


We test the function below, simply consider the hyperparameters to be fixed at their default values, rather than estimating their values from an initial design. We run the function for differnt numbers of iterations. When running for fewer iterations, we still see the "clumpiness" resulting from the uniform random initialization, but for higher numbers of iterations the maxent procedure removes this clumpiness and spreads the points out. 

```{r}

N_itr_vec <- c(1, 100, 1000, 10000)

for(N_itr in N_itr_vec) {
  X <- maxent(N = 25, D = 2, N_itr = N_itr)
  plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("maxent design; N_itr = ", N_itr))
}
```

Notice that the results are much more reminiscent of a maximin, rather than LHS, design. In particular, the marginal distributions look quite clumpy; the one-dimensional projections tend to cluster at a handful of values, rather than being spread out. What if we instead initialized the kernel hyperparameters with anisitropic lengthscales? If a lengthscale is much longer in, say, dimension 1 then dimension 2, then this expresses a prior belief that the underlying function is much more wiggly in dimension 2 than dimension 1. We would therefore expect that the resulting design would place points closer together in dimension 2 to allow for a finer grain view of the function, while in dimension 1 we could get away with points more spaced out. This is indeed the behavior we see in the below example. 
```{r}
N_itr <- 10000
X <- maxent(N = 25, D = 2, N_itr = N_itr, theta = c(0.1, 0.8))
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("maxent design; N_itr = ", N_itr))
```

### Predictive Uncertainty Design
We now consider an alternative criterion; instead of minimizing the entropy of the predictive distribution at unobserved points, 
we consider minimizing the predictive uncertainty over these unobserved points. Recall that the predictive GP variance at at unobserved point $\mathbf{x}$ is given by
by 
$$ k_{\mathbf{X}}(\mathbf{x}) = k(\mathbf{x}) - k(\mathbf{x}, \mathbf{X})\mathbf{K}^{-1}k(\mathbf{X}, \mathbf{x}) = \tau^2\left(1 + g - c(\mathbf{x}, \mathbf{X})\mathbf{C}^{-1}c(\mathbf{X}, \mathbf{x}) \right)$$
By definition of variance, this is equivalent to 
$$ k_{\mathbf{X}}(\mathbf{x}) = \mathbb{E}\left[\left(y(\mathbf{x}) - \mu_{\mathbf{X}}(\mathbf{x}) \right)^2 | \mathbf{X}, \mathbf{y} \right]$$
For this reason, the predictive variance is also often called the *mean-squared prediction error (MSPE)*. The MSPE is the average squared deviation between the GP prediction and its mean at the input point $\mathbf{x}$. We can then consider averaging the MSPE over the input space $\mathcal{X}$. We might consider a simple average that treats all input points $\mathbf{x}$ as equally important, or alternative consider a density $w(\mathbf{x})$ on $\mathbf{X}$ that designates certain inputs as more important than others. An *integrated MSPE (IMSPE)* criterion (as a function of the design $\mathbf{X}$) might then look like
$$ J(\mathbf{X}) = \int_{\mathcal{X}} \frac{k_{\mathbf{X}}(\mathbf{x})}{\tau^2} w(\mathbf{x}) d\mathbf{x}$$
where $\tau^2$ is the prior marginal variance of the GP. The division by $\tau^2$ is not that important; it simply normalizes the predictive variance by the prior variance. While this integral typically requires numerical methods, it is available in closed-form for simple cases, where
$\mathcal{X}$ is a simple domain such as a hypercube and the covariance function is isotropic or separable Gaussian or Matern 
(see Binois et al, 2019 for derivations). Below we utilize the ISMPE function implemented in *hetGP*. This function is implemented more generally to work with heteroskedastic noise, so we define a wrapper that simplifies it to our setting. 

```{r}
imspe_criteria <- function(X, theta, g, ...) {
  IMSPE(X, theta=theta, Lambda=diag(g, nrow(X)), covtype="Gaussian", mult=rep(1, nrow(X)), nu=1) 
}
```

Given this, we can simply replace the log determinant objective (derived from the maxent criterion) with the IMSPE objection, noting that we must slightly adjust the code to minimize instead of maximize. 

```{r}

min_IMSPE <- function(N, D, theta = 0.1, g = 0.01, N_itr = 100000, ...) {
  
  if(length(theta) == 1) theta <- rep(theta, D)
  X <- matrix(runif(N*D), ncol = D) # Initial design is generated uniformly at random. 
  IMSPE_curr <- imspe_criteria(X = X, theta = theta, g = g, ...) # Current value of objective function.
  
  for(itr in 1:N_itr) {
    row <- sample(1:N, 1) # Randomly sample design point to update. 
    x_curr <- X[row,]
    X[row,] <- runif(D) # Generate new proposed design point uniformly at random. 
    IMSPE_prop <- imspe_criteria(X = X, theta = theta, g = g, ...)
    if(IMSPE_prop < IMSPE_curr) {
      IMSPE_curr <- IMSPE_prop
    } else {
      X[row,] <- x_curr
    }
    
  }
  
  return(X)
  
}

```

We now run this on the same examples as before, increasing the number of iterations each time. 

```{r}

N_itr_vec <- c(1, 100, 1000, 10000)

for(N_itr in N_itr_vec) {
  X <- min_IMSPE(N = 25, D = 2, N_itr = N_itr)
  plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design; N_itr = ", N_itr))
}
```
And the example with non-isotropic covariance. 
```{r}
N_itr <- 10000
X <- min_IMSPE(N = 25, D = 2, N_itr = N_itr, theta = c(0.1, 0.8))
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design; N_itr = ", N_itr))
```
The results look pretty similar to the maxent design; however, the min IMSPE design tends to shy away from the boundaried of the input space more. The effect is minimal in two dimensions but will become much more prominent as the dimension increases. This is due to the fact that the criterion for choosing a single point $\mathcal{x}$ takes into account the error averaged over the entire input space. Choosing more central points will tend to have a greater effect than boundary points in reducing error averaged over the whole space, so IMSPE tends to favor interior points.

#### Extending to cases where IMSPE is not analytically tractable 
There are many cases that might break analytic tractability; in particular, here we consider the case of non-uniform weights $w(x)$ or the case where $\mathcal{X}$ is non-rectangular. The obvious solution here is to simply consider a Monte Carlo estimate of the 
integral, 

$$ J(\mathbf{X}) \approx \frac{1}{T} \sum_{t = 1}^{T} \frac{k_{\mathbf{X}}(\mathbf{x}_t)}{\tau^2} w(\mathbf{x}_t), \text{ where } \mathbf{x}_t \sim \mathcal{U}(\mathcal{X})$$

or 

$$ J(\mathbf{X}) \approx \frac{1}{T} \sum_{t = 1}^{T} \frac{k_{\mathbf{X}}(\mathbf{x}_t)}{\tau^2}, \text{ where } \mathbf{x}_t \sim w $$

Alternatively, we might consider a numerical integration or quadrature-based scheme. For example, instead of uniformly sampling points from $\mathcal{X}$ we might instead fix a grid of points and then average the integrand over these fixed points to estimate the integral. Such an appraoch is suitable in low dimensions, as in the two-dimensional example we have been considering. Let's apply such a grid-based scheme to this example, and compare the result to the result produced using the closed-form expression for the integral.  


```{r}
# Grid approximation of IMSPE. 
imspe_criteria <- function(X, theta, g, X_grid) {
  
  # Closed-form method.
  if(missing(X_grid)) {
    return(IMSPE(X, theta=theta, Lambda=diag(g, nrow(X)), covtype="Gaussian", mult=rep(1, nrow(X)), nu=1))
  }
  
  # Grid approximation. 
  C <- covar.sep(X, d=theta, g=g) 
  C_inv <- solve(C)
  C_cross <- covar.sep(X_grid, X, d=theta, g=0)
  integrand_evaluations <- 1 + g - diag(C_cross %*% C_inv %*% t(C_cross))
  
  return(mean(integrand_evaluations))
}
```

We now test the grid approximation. In addition to the sampled design points, the fixed grid points are plotted as small gray dots. 

```{r}
X_grid <- as.matrix(expand.grid(seq(0, 1, length=10), seq(0, 1, length=10)))
X <- min_IMSPE(N = 25, D = 2, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design grid approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

The approximate result looks pretty good. Let's think about what kind of error the grid approximation might induce. At each iteration, a new design point $\mathbf{x}$ is proposed as a replacement for a current one. The criterion that determines whether it is replaced or not is 
$$ \frac{1}{T} \sum_{t = 1}^{T} \frac{k_{\mathbf{X}}(\mathbf{x}_t)}{\tau^2} $$
where $\mathbf{x}_1, \dots, \mathbf{x}_T$ are the fixed grid points. Since the mean squared prediction error is only considered at the grid points $\mathbf{x}_t$ then naturally this method will tend to favor design points that are close to the grid points. When the number of grid points is large this might not be a big deal, but this error may become very noticeable when the number of grid points is small. Let's consider an example with a much smaller number of grid points. 
```{r}
X_grid <- as.matrix(expand.grid(seq(0, 1, length=3), seq(0, 1, length=3)))
X <- min_IMSPE(N = 25, D = 2, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design grid approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

Indeed, we observe that the design points are clustered around the grid points. Trying to approximate the IMSPE integral over a two-dimensional input space with 9 grid points unsurprisingly results in a poor approximation. Another interesting consequence 
of this grid approximation is that design points will tend to be pulled to the boundaries, whereas we noticed that the exact IMSPE results tended to shy away from the boundaries. This is due to the nature of selecting a grid. The ratio of the number of grid points on the boundaries vs. the number of interior points is much higher than the ratio of the "volume" of the boundaries vs. the volume of the interior. Thus, the boundary grid points will pull design points towards the boundaries moreso than the exact solution. 

We now return to the denser grid, but entertain increasing the lengthscale. Intuitively, we expect a longer lengthscale to yield even more spread out points. However, this will come into conflict with the tendency for design points to want to stay close to the grid points. We present results for the closed-form and grid approximated solutions below. We observe some odd behavior from the grid approximation. 

```{r}
X <- min_IMSPE(N = 25, D = 2, theta = 0.5, N_itr = 10000)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design closed-form; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

```{r}
X_grid <- as.matrix(expand.grid(seq(0, 1, length=10), seq(0, 1, length=10)))
X <- min_IMSPE(N = 25, D = 2, theta = 0.5, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design grid approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

Gramacy presents an interesting example illustrating the case where the approximate solution allows sampling desing points on irregularly shaped domains; in particular, he presents an example where the domain consists of two intersecting ellipsoids. 
In essence, his example boils down to assuming the density $w(\mathbf{x})$ weighting the input space is a mixture of Gaussians. 
$$ w(\mathbf{x}) = \frac{1}{2}\mathcal{N}_2(\mathbf{x}|\mathbf{\mu}_1, \mathbf{\Sigma}_1) + \frac{1}{2}\mathcal{N}_2(\mathbf{x}|\mathbf{\mu}_2, \mathbf{\Sigma}_2)$$
The below code more-or-less samples from $\mathbf{w}(\mathbf{x})$ (instead of randomly choosing which Gaussian to sample from, 
it simply chooses equal sample sizes from each) to construct the grid, and then proceeds as before. 
```{r}
X_grid_1 <- rmvnorm(100, mean=c(0.25, 0.25), sigma=0.005*rbind(c(2, 0.35), c(0.35, 0.1)))
X_grid_2 <- rmvnorm(100, mean=c(0.25, 0.25), sigma=0.005*rbind(c(0.1, -0.35), c(-0.35, 2)))
X_grid <- rbind(X_grid_1, X_grid_2)

X <- min_IMSPE(N = 25, D = 2, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design Monte Carlo approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

# Sequential Design/Active Learning
We have currently entertained the notion of *batch design*; that is, selecting the $N$ design points $\mathbf{X}$ in one fell swoop. An alternative, often very practical approach, is to choose that points one at a time - at each iteration selecting the next point based on the criterion 
$J(\mathbf{x})$. The general idea is as follows:

1. Start with a small design $\mathbf{X}_{N_0}$ consisting of $N_0$ inputs typically chosen via some sort of model-free, space-filling design 
(e.g. LHS). Run the forward model at these inputs and fit the GP on this initial dataset $\mathcal{D}_{N_0}$ (e.g. estimate the hyperparameters via MLE). 
Set $n := N_0$. 
2. Select the next design point via 
$$ \mathbf{x}_{n + 1} := \text{argmax}_{\mathbf{x}} J(\mathbf{x})|\mathcal{D}_n$$
Augment the design $\mathbf{X}_{n + 1} := \mathbf{X}_n \cup \mathbf{x}_{n + 1}$ and run the forward model at the new input point. Denote the 
new augmented dataset $\mathcal{D}_{n + 1}$. Re-fit the GP on the new dataset. 
3. Repeat step 2 until a desired number of design points $N$ is reached. 

There are many optimization tricks and pitfalls when considering such sequential schemes. For example, one might not update the MLE GP fit every iteration to save on computation, but this could potentially lead to unsatisfactory results. It is also common to *warm start* the MLE hyperparameter 
optimization but setting the initial values in the optimization to the values of the hyperparameters that were optimized in the previous step. 

Perhaps the simplest sequential design criterion is to choose the point that maximizes the predictive variance. The idea is that if the predictive variance is very high at a point, then the GP is very uncertain about the latent function at that point so we should run the model at that point to 
reduce that uncertainty. The objective thus looks like
$$ \mathbf{x}_{n + 1} := \text{argmax}_{\mathbf{x}} J(\mathbf{x})|\mathcal{D}_n = \text{argmax}_{\mathbf{x}} k_{\mathbf{X}_n}(\mathbf{x})$$
This method is known as *Active Learning MacKay (ALM)*, after David MacKay, who explored this notion of design for neural networks and argued that this sequential design strategy tends to approximate a maxent design. 

Let's consider a toy example with 2D input space. The below chode chunk defines the latent function we seek to learn. The function is a scaled 
and shifted version of 
$$ f(x_1, x_2) = x_1 \exp(-x_1^2 - x_2^2)$$
A heat map provides a visualization of this function below. 

```{r}
f <- function(X, sd=0.01) {
  X[,1] <- (X[,1] - 0.5)*6 + 1
  X[,2] <- (X[,2] - 0.5)*6 + 1
  f_vals <- X[,1] * exp(-X[,1]^2 - X[,2]^2)
  y_vals <- f_vals + rnorm(nrow(X), sd=sd)
  
  return(list(f = f_vals, y = y_vals))
}
```


```{r}
# Evaluate function at initial space-filling design.
N0 <- 12
X <- randomLHS(N0, 2)
f_list <- f(X)
f_vals <- f_list$f
y <- f_list$y

# Also evaluate at dense grid for plotting. 
X_grid <- as.matrix(expand.grid(seq(0, 1, length=51), seq(0, 1, length=51)))
f_grid <- f(X_grid)$f
```

```{r}
cols <- heat.colors(128)

# Latent function, noiseless evaluations. 
image(seq(0, 1, length=51), seq(0, 1, length=51), matrix(f_grid, ncol=51), 
      xlab="x1", ylab="x2", main = "Noiseless evaluations of f(), and initial design points.", col=cols)
points(X[,1], X[,2])
```

We observe that this is a tricky function, with a steep peak right next to a deep trough. Beyond these two modes, the function is quite flat. So there are separate regimes of the input space, corresponding to sub-domains on which the function is highly non-linear and the remaining sub-domain on which the function is flat. 

We now fit a GP using the *laGP* package on this initial design. 
```{r}
g <- garg(list(mle=TRUE, max=1), y)
d <- darg(list(mle=TRUE, max=0.25), X)
gp0 <- newGP(X, y, d=d$start, g=g$start, dK=TRUE)
mle <- jmleGP(gp0, c(d$min, d$max), c(g$min, g$max), d$ab, g$ab)
```



# Appendix 

## Information and Entropy of multivariate Gaussian
Here we derive the Shannon information content of a multivariate Gaussian 

$$ X \sim \mathcal{N}_D(\mu, \Sigma)$$

We have 
$$
\begin{align*}
I(X) &= \mathbb{E}\left[\log p_X(X) \right] \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mathbb{E}\left[(X - \mu)^T \Sigma^{-1} (X - \mu) \right] \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mu^T \Sigma^{-1}\mu - \frac{1}{2}\mathbb{E}\left[X^T \Sigma^{-1} X \right] + \mathbb{E}\left[X^T \Sigma^{-1} \mu \right] \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mu^T \Sigma^{-1}\mu - \frac{1}{2}\left[\text{tr}(\Sigma^{-1}\Sigma) + \mu^T \Sigma^{-1} \mu \right] + \mu^T \Sigma^{-1} \mu \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mu^T \Sigma^{-1}\mu - \frac{D}{2} - \frac{1}{2}\mu^T \Sigma^{-1} \mu + \mu^T \Sigma^{-1} \mu \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{D}{2} \\
     &= -\frac{1}{2}\log\det(2\pi e \Sigma)
\end{align*}
$$

The entropy is thus 
$$ H(X) = -I(X) = \frac{1}{2}\log\det(2\pi e \Sigma)$$

# References 
Binois, M, J Huang, RB Gramacy, and M Ludkovski. 2019. “Replication or Exploration? Sequential Design for Stochastic Simulation Experiments.” Technometrics 27 (4): 808–21. https://doi.org/10.1080/00401706.2018.1469433.






