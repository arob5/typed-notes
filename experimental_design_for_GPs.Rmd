---
title: "Experimental Design for GPs"
author: "Andrew Roberts"
date: '2023-05-15'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(lhs)
library(laGP)
library(plgp)
library(hetGP)
```

# Space-Filling Designs

## Latin Hypercube Sampling (LHS)

## Maximin Sampling
The idea of maximin sampling is to maximize the minimum distance between two points - quite an intuitive and reasonable criterion if all we want to do is spread points out. Specifically, for some distance metric $d(\cdot, \cdot)$ the maximin criterion is 
$$ \mathbf{X} := \text{argmax}_{\mathbf{x}_1, \dots, \mathbf{x}_N} \min_{1 \leq i < j \leq N} d(\mathbf{x}_i, \mathbf{x}_j)$$
Although conceptually simple, maximin procedures offer a challenging optimization problem. Below, I paste code implementing a stochastic exchange algorithm from Grmacy's *Surrogates*. This is a very simple algorithm that proceeds as follows: start by generating a design uniformly at random. 
Then randomly select a design point to consider updating. Generate a proposed replacement uniformly at random. If the proposal reduces the maximin 
criterion accept it, otherwise reject and stick with the existing design. This procedure is then iterated many times. Clearly, there is a lot of room for improving the efficiency of this algorithm, but for our purposes this is sufficient. 

```{r}
mymaximin <- function(n, m, T=100000, Xorig = NULL) 
 {  
  
  if(is.null(Xorig)) X <- matrix(runif(n*m), ncol=m)  ## initial design
  
  d <- plgp::distance(X)
  d <- d[upper.tri(d)]
  md <- min(d)

  for(t in 1:T) {
    row <- sample(1:n, 1)
    xold <- X[row,]                   ## random row selection
    X[row,] <- runif(m)               ## random new row
    d <- plgp::distance(X)
    d <- d[upper.tri(d)]
    mdprime <- min(d)
    if(mdprime > md) { md <- mdprime  ## accept
    } else { X[row,] <- xold }        ## reject
  }

  return(X)
}
```



# Model-Based Design for GPs 
While space-filling designs like latin hypercube or maximim are agnostic to the model being fit to the data, another approach is to develop design strategies that are tailored to the specific model being used - in our case, a GP. 

We begin with the following setup. Suppose we are interesting in fitting a GP 

$$ y(\cdot) \sim \mathcal{GP}(0, k(\cdot, \cdot))$$

over a $D$-dimensional input space. Let $\tilde{\mathbf{X}}$ denote the $M \times D$ matrix containing a set of input points $\tilde{\mathbf{x}}_1, \dots, \tilde{\mathbf{x}}_M$. From these $M$ inputs, we seek to choose an $N \times D$ design $\mathbf{X}$ consisting of inputs $\mathbf{x}_1, \dots, \mathbf{x}_N$ that is "optimal" in some sense.

It will be helpful to establish some notation here. First let $\overline{\mathbf{X}}$ be the $(M - N) \times D$ matrix contain the points not selected in the design $\mathbf{X}$. I will denote the kernel matrices by $\mathbf{K} := k(\mathbf{X})$, $\mathbf{\tilde{K}} := k(\mathbf{\tilde{X}})$, $\mathbf{\overline{K}} := k(\overline{\mathbf{X}})$ and the random vectors $\mathbf{y} := y(\mathbf{X})$, $\mathbf{\tilde{y}} := y(\mathbf{\tilde{X}})$, $\overline{\mathbf{y}} := y(\overline{\mathbf{X}})$. 

Note that once we have selected $\mathbf{X}$, then conditioning on the observed $\mathbf{y}$ yields the standard GP predictive distribution over the unobserved points. 

$$ \mathbf{\overline{y}}|\mathbf{y}, \mathbf{X}, \mathbf{\overline{X}} \sim \mathcal{N}_{M - N}\left(\mu_{\mathbf{X}}(\overline{\mathbf{X}}), k_{\mathbf{X}}(\overline{\mathbf{X}})  \right)$$

where 

$$
\begin{align*}
\mu_{\mathbf{X}}(\overline{\mathbf{X}}) &= k\left(\overline{\mathbf{X}}, \mathbf{X}\right) \mathbf{K}^{-1} \mathbf{y} \\
k_{\mathbf{X}}(\overline{\mathbf{X}}) &= k\left(\overline{\mathbf{X}}\right) - k\left(\overline{\mathbf{X}}, \mathbf{X}\right) \mathbf{K}^{-1} k\left(\mathbf{X}, \overline{\mathbf{X}}\right)
\end{align*}
$$

I will let $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$ denote the random vector with this predictive distribution. 

Throughout I will assume a squared exponential kernel, following the parameterization utilized in Gramacy's *Surrogates* book. 
$$ k(\mathbf{x}_i, \mathbf{x}_j) = \tau^2 \left(c(\mathbf{x_i}, \mathbf{x}_j) + g \mathbb{1}_{ij} \right)$$
where 
$$c(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\sum_{d = 1}^{D} \frac{(\mathbf{x}_{id} - \mathbf{x}_{jd})^2}{\theta_d} \right)$$
Here, $\tau^2$ is the *marginal variance*, $g$ the *nugget*, and $\theta_1, \dots, \theta_d$ the *lengthscales*.  


### Maximum Entropy Design
Intuitively, we seek to select $\mathbf{X}$ such that it yields the most information about the predictive distribution over the unobserved points; that is, the selection that makes $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$ as uncertain as possible. As our measure of uncertainty, we will first consider Shannon's entropy

$$ H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right) = -\mathbb{E}\left[\log p_{\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}}\left(\overline{\mathbf{y}}\right)\right]$$

where the expectation is with respect to the $(M - N)$-dimensional Gaussian distribution of $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$. The negative of entropy is called *information*, and hence minimizing entropy to equivalent to maximizing information 

$$ I_{\overline{\mathbf{X}}|\mathbf{X}} := -H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$$

 Since everything is Gaussian here, the formula for entropy of a Gaussian distribution will come up a lot. This is derived in the appendix for reference. Applying that formula to $\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}$, we find that the entropy of the predictive distribution over the outputs at the unobserved locations is given by 
 
$$
\begin{align*}
H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right) &= \frac{1}{2}\log\det\left(2\pi e k_{\mathbf{X}}(\overline{\mathbf{X}})\right) \\
&= \frac{1}{2}\log\det\left(2\pi e \left[k\left(\overline{\mathbf{X}}\right) - k\left(\overline{\mathbf{X}}, \mathbf{X}\right) \mathbf{K}^{-1} k\left(\mathbf{X}, \overline{\mathbf{X}}\right) \right] \right)
\end{align*}
$$

Now, as mentioned above, the goal is to select $\mathbf{X}$ to minimize $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$. Notice that the entropy of Gaussian distributions does not depend on the mean of the distributions. Since the GP predictive covariance $k_{\mathbf{X}}$ does not depend on $\mathbf{y}$ (the observed valued being conditioned on), this implies that 
$H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$ does not actually depend on $\mathbf{y}$. The relation to $\mathbf{y}$ is captured entirely through the kernel. In more general non-Gaussian settings this is not the case, and hence it is typical to instead consider minimizing $\mathbb{E} H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$, where the expectation is with respect to the prior on $\mathbf{y}$. Again, this is not required here but I state this in the interest of providing a more general picture. 
 
We will now show that minimizing $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$ is equivalent to maximizing $H(\mathbf{y})$; that is, the optimal design is to choose input points $\mathbf{X}$ corresponding to the locations that are *most uncertain* under the GP prior. To verify this claim, we show that the prior entropy $H(\tilde{\mathbf{y}})$ can be decomposed as a sum of the prior entropy at the selected inputs $H(\mathbf{y})$ and the entropy of the predictive distribution at the remaining inputs $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$. To this end, note that the entropy of the prior at locations $\tilde{\mathbf{X}}$ is 

$$ H(\tilde{\mathbf{y}}) = \frac{1}{2} \log\det\left(2\pi e \tilde{\mathbf{K}} \right) = \frac{N + M}{2} \log(2\pi e) + \frac{1}{2} \log\det(\tilde{\mathbf{K}})$$

where $\tilde{\mathbf{K}}$ can be written in block form as 

$$\tilde{\mathbf{K}} = \begin{pmatrix} \mathbf{K} & k(\mathbf{X}, \overline{\mathbf{X}}) \\ 
k(\overline{\mathbf{X}}, \mathbf{X}) & \overline{\mathbf{K}} \end{pmatrix} $$

We can re-write the determinant $\det(\tilde{\mathbf{K}})$ in terms of each block using the formula for determinants of block matrices; see e.g., $\href{https://math.stackexchange.com/questions/1905652/proofs-of-determinants-of-block-matrices}{this}$ StackExchange post. 

$$
\begin{align*}
\det(\tilde{\mathbf{K}}) &= \det(\mathbf{K}) \cdot \det\left(\overline{\mathbf{K}} - k(\overline{\mathbf{X}}, \mathbf{X}) \mathbf{K}^{-1} k(\mathbf{X}, \overline{\mathbf{X}})\right) \\
&= \det(\mathbf{K}) \cdot \det(k_{\mathbf{X}}(\overline{\mathbf{X}}))
\end{align*}
$$

Plugging this back into the expression for $H(\tilde{\mathbf{y}})$, we obtain
$$
\begin{align*}
H(\tilde{\mathbf{y}}) &= \left\{\frac{N}{2} \log(2\pi e) + \log\det(\mathbf{K}) \right\} + \left\{\frac{M}{2} \log(2\pi e) + \log\det(k_{\mathbf{X}}(\overline{\mathbf{X}})) \right\} \\
&= H(\mathbf{y}) + H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)
\end{align*}
$$

Since the lefthand side - the entropy of the prior at locations $\tilde{\mathbf{X}}$ - is fixed, then we see that minimizing $H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$ is equivalent to maximizing $H(\mathbf{y})$. This sort of entropy decomposition shows up in a variety of contexts; I emphasize that the decomposition is typically of the form 

$$ H(\tilde{\mathbf{y}}) = H(\mathbf{y}) + \mathbb{E}_{\overline{\mathbf{y}}} H\left(\overline{\mathbf{y}}_{\overline{\mathbf{X}}|\mathbf{X}}\right)$$ 

where the expectation is with respect to the prior, but that in this special Gaussian setting the equality holds without the expectation. The *maximum entropy (maxent) design* of size $N$ is thus the set of input locations satisfying

$$ 
\begin{align*}
\mathbf{X} &= \text{argmax} H(\mathbf{y}) \\
           &= \text{argmax} \log\det(\mathbf{K}) \\
           &= \text{argmax} \det(\mathbf{K})
\end{align*}
$$

This is good and all, but there is a glaring problem. The kernel matrix $\mathbf{K}$ depends on the kernel hyperparameters. The whole point of what we're doing here is to choose a design well-suited to learning the kernel hyperparameters, so we've run into a circular issue here. To address this, one reasonable solution is to first initialize a small space-filling design (e.g. LHS or maximin), learn the kernel hyperparameters from this design, then append to this design in a sequential fashion using the maxent strategy. 


The following function *maxent* assumes the hyperparameters have already been estimated, and performs the sequential maxent procedure to refine the design. The implementation below is as simple as possible, and certainly not efficient. The sequential procedure proceeds by randomly selecting a design point to update, proposing a new design point uniformly at random, and then accepting the proposal if the objective function (the determinant of the kernel matrix at the design points) has increased. This will naturally lead to many rejections.  
```{r}

maxent <- function(N, D, theta = 0.01, g = 0.01, N_itr = 100000) {
  
  if(length(theta) == 1) theta <- rep(theta, D)
  X <- matrix(runif(N*D), ncol = D) # Initial design is generated uniformly at random. 
  K <- covar.sep(X, d=theta, g=g) # Correlation matrix plus nugget. 
  ldetK <- determinant(K, logarithm = TRUE)$modulus # Current value of objective function. 
  
  for(itr in 1:N_itr) {
    row <- sample(1:N, 1) # Randomly sample design point to update. 
    x_curr <- X[row,]
    X[row,] <- runif(D) # Generate new proposed design point uniformly at random. 
    K_prop <- covar.sep(X, d=theta, g=g)
    ldetK_prop <- determinant(K_prop, logarithm = TRUE)$modulus
    if(ldetK_prop > ldetK) {
      ldetK <- ldetK_prop
    } else {
      X[row,] <- x_curr
    }
    
  }
  
  return(X)
  
}

```


We test the function below, simply consider the hyperparameters to be fixed at their default values, rather than estimating their values from an initial design. We run the function for differnt numbers of iterations. When running for fewer iterations, we still see the "clumpiness" resulting from the uniform random initialization, but for higher numbers of iterations the maxent procedure removes this clumpiness and spreads the points out. 

```{r}

N_itr_vec <- c(1, 100, 1000, 10000)

for(N_itr in N_itr_vec) {
  X <- maxent(N = 25, D = 2, N_itr = N_itr)
  plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("maxent design; N_itr = ", N_itr))
}
```

Notice that the results are much more reminiscent of a maximin, rather than LHS, design. In particular, the marginal distributions look quite clumpy; the one-dimensional projections tend to cluster at a handful of values, rather than being spread out. What if we instead initialized the kernel hyperparameters with anisitropic lengthscales? If a lengthscale is much longer in, say, dimension 1 then dimension 2, then this expresses a prior belief that the underlying function is much more wiggly in dimension 2 than dimension 1. We would therefore expect that the resulting design would place points closer together in dimension 2 to allow for a finer grain view of the function, while in dimension 1 we could get away with points more spaced out. This is indeed the behavior we see in the below example. 
```{r}
N_itr <- 10000
X <- maxent(N = 25, D = 2, N_itr = N_itr, theta = c(0.1, 0.8))
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("maxent design; N_itr = ", N_itr))
```

### Predictive Uncertainty Design
We now consider an alternative criterion; instead of minimizing the entropy of the predictive distribution at unobserved points, 
we consider minimizing the predictive uncertainty over these unobserved points. Recall that the predictive GP variance at at unobserved point $\mathbf{x}$ is given by
by 
$$ k_{\mathbf{X}}(\mathbf{x}) = k(\mathbf{x}) - k(\mathbf{x}, \mathbf{X})\mathbf{K}^{-1}k(\mathbf{X}, \mathbf{x}) = \tau^2\left(1 + g - c(\mathbf{x}, \mathbf{X})\mathbf{C}^{-1}c(\mathbf{X}, \mathbf{x}) \right)$$
By definition of the variance, this is equivalent to 
$$ k_{\mathbf{X}}(\mathbf{x}) = \mathbb{E}\left[\left(y(\mathbf{x}) - \mu_{\mathbf{X}}(\mathbf{x}) \right)^2 | \mathbf{X}, \mathbf{y} \right]$$
For this reason, the predictive variance is also often called the *mean-squared prediction error (MSPE)*. The MSPE is the average squared deviation between the GP prediction and its mean at the input point $\mathbf{x}$. We can then consider averaging the MSPE over the input space $\mathcal{X}$. We might consider a simple average that treats all input points $\mathbf{x}$ as equally important, or alternatively consider a density $w(\mathbf{x})$ on $\mathbf{X}$ that designates certain inputs as more important than others. An *integrated MSPE (IMSPE)* criterion (as a function of the design $\mathbf{X}$) might then look like
$$ J(\mathbf{X}) = \int_{\mathcal{X}} \frac{k_{\mathbf{X}}(\mathbf{x})}{\tau^2} w(\mathbf{x}) d\mathbf{x}$$
where $\tau^2$ is the prior marginal variance of the GP. The division by $\tau^2$ is not that important; it simply normalizes the predictive variance by the prior variance. While this integral typically requires numerical methods, it is available in closed-form for simple cases, where
$\mathcal{X}$ is a simple domain such as a hypercube and the covariance function is isotropic or separable Gaussian or Matern 
(see Binois et al, 2019 for derivations). Below we utilize the ISMPE function implemented in *hetGP*. This function is implemented more generally to work with heteroskedastic noise, so we define a wrapper that simplifies it to our setting. 

```{r}
imspe_criteria <- function(X, theta, g, ...) {
  IMSPE(X, theta=theta, Lambda=diag(g, nrow(X)), covtype="Gaussian", mult=rep(1, nrow(X)), nu=1) 
}
```

Given this, we can simply replace the log determinant objective (derived from the maxent criterion) with the IMSPE objection, noting that we must slightly adjust the code to minimize instead of maximize. 

```{r}

min_IMSPE <- function(N, D, theta = 0.1, g = 0.01, N_itr = 100000, ...) {
  
  if(length(theta) == 1) theta <- rep(theta, D)
  X <- matrix(runif(N*D), ncol = D) # Initial design is generated uniformly at random. 
  IMSPE_curr <- imspe_criteria(X = X, theta = theta, g = g, ...) # Current value of objective function.
  
  for(itr in 1:N_itr) {
    row <- sample(1:N, 1) # Randomly sample design point to update. 
    x_curr <- X[row,]
    X[row,] <- runif(D) # Generate new proposed design point uniformly at random. 
    IMSPE_prop <- imspe_criteria(X = X, theta = theta, g = g, ...)
    if(IMSPE_prop < IMSPE_curr) {
      IMSPE_curr <- IMSPE_prop
    } else {
      X[row,] <- x_curr
    }
    
  }
  
  return(X)
  
}

```

We now run this on the same examples as before, increasing the number of iterations each time. 

```{r}

N_itr_vec <- c(1, 100, 1000, 10000)

for(N_itr in N_itr_vec) {
  X <- min_IMSPE(N = 25, D = 2, N_itr = N_itr)
  plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design; N_itr = ", N_itr))
}
```
And the example with non-isotropic covariance. 
```{r}
N_itr <- 10000
X <- min_IMSPE(N = 25, D = 2, N_itr = N_itr, theta = c(0.1, 0.8))
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design; N_itr = ", N_itr))
```
The results look pretty similar to the maxent design; however, the min IMSPE design tends to shy away from the boundaries of the input space more. The effect is minimal in two dimensions but will become much more prominent as the dimension increases. This is due to the fact that the criterion for choosing a single point $\mathcal{x}$ takes into account the error averaged over the entire input space. Choosing more central points will tend to have a greater effect than boundary points in reducing error averaged over the whole space, so IMSPE tends to favor interior points.

#### Extending to cases where IMSPE is not analytically tractable 
There are many cases that might break analytic tractability; in particular, here we consider the case of non-uniform weights $w(x)$ or the case where $\mathcal{X}$ is non-rectangular. The obvious solution here is to simply consider a Monte Carlo estimate of the 
integral, 

$$ J(\mathbf{X}) \approx \frac{1}{T} \sum_{t = 1}^{T} \frac{k_{\mathbf{X}}(\mathbf{x}_t)}{\tau^2} w(\mathbf{x}_t), \text{ where } \mathbf{x}_t \sim \mathcal{U}(\mathcal{X})$$

or 

$$ J(\mathbf{X}) \approx \frac{1}{T} \sum_{t = 1}^{T} \frac{k_{\mathbf{X}}(\mathbf{x}_t)}{\tau^2}, \text{ where } \mathbf{x}_t \sim w $$

Alternatively, we might consider a numerical integration or quadrature-based scheme. For example, instead of uniformly sampling points from $\mathcal{X}$ we might instead fix a grid of points and then average the integrand over these fixed points to estimate the integral. Such an approach is suitable in low dimensions, as in the two-dimensional example we have been considering. Let's apply such a grid-based scheme to this example, and compare the result to the result produced using the closed-form expression for the integral.  


```{r}
# Grid approximation of IMSPE. 
imspe_criteria <- function(X, theta, g, X_grid) {
  
  # Closed-form method.
  if(missing(X_grid)) {
    return(IMSPE(X, theta=theta, Lambda=diag(g, nrow(X)), covtype="Gaussian", mult=rep(1, nrow(X)), nu=1))
  }
  
  # Grid approximation. 
  C <- covar.sep(X, d=theta, g=g) 
  C_inv <- solve(C)
  C_cross <- covar.sep(X_grid, X, d=theta, g=0)
  integrand_evaluations <- 1 + g - diag(C_cross %*% C_inv %*% t(C_cross))
  
  return(mean(integrand_evaluations))
}
```

We now test the grid approximation. In addition to the sampled design points, the fixed grid points are plotted as small gray dots. 

```{r}
X_grid <- as.matrix(expand.grid(seq(0, 1, length=10), seq(0, 1, length=10)))
X <- min_IMSPE(N = 25, D = 2, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design grid approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

The approximate result looks pretty good. Let's think about what kind of error the grid approximation might induce. At each iteration, a new design point $\mathbf{x}$ is proposed as a replacement for a current one. The criterion that determines whether it is replaced or not is 
$$ \frac{1}{T} \sum_{t = 1}^{T} \frac{k_{\mathbf{X}}(\mathbf{x}_t)}{\tau^2} $$
where $\mathbf{x}_1, \dots, \mathbf{x}_T$ are the fixed grid points. Since the mean squared prediction error is only considered at the grid points $\mathbf{x}_t$ then naturally this method will tend to favor design points that are close to the grid points. When the number of grid points is large this might not be a big deal, but this error may become very noticeable when the number of grid points is small. Let's consider an example with a much smaller number of grid points. 
```{r}
X_grid <- as.matrix(expand.grid(seq(0, 1, length=3), seq(0, 1, length=3)))
X <- min_IMSPE(N = 25, D = 2, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design grid approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

Indeed, we observe that the design points are clustered around the grid points. Trying to approximate the IMSPE integral over a two-dimensional input space with 9 grid points unsurprisingly results in a poor approximation. Another interesting consequence 
of this grid approximation is that design points will tend to be pulled to the boundaries, whereas we noticed that the exact IMSPE results tended to shy away from the boundaries. This is due to the nature of selecting a grid. The ratio of the number of grid points on the boundaries vs. the number of interior points is much higher than the ratio of the "volume" of the boundaries vs. the volume of the interior. Thus, the boundary grid points will pull design points towards the boundaries more so than the exact solution. 

We now return to the denser grid, but entertain increasing the lengthscale. Intuitively, we expect a longer lengthscale to yield even more spread out points. However, this will come into conflict with the tendency for design points to want to stay close to the grid points. We present results for the closed-form and grid approximated solutions below. We observe some odd behavior from the grid approximation. 

```{r}
X <- min_IMSPE(N = 25, D = 2, theta = 0.5, N_itr = 10000)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design closed-form; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

```{r}
X_grid <- as.matrix(expand.grid(seq(0, 1, length=10), seq(0, 1, length=10)))
X <- min_IMSPE(N = 25, D = 2, theta = 0.5, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design grid approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

Gramacy presents an interesting example illustrating the case where the approximate solution allows sampling design points on irregularly shaped domains; in particular, he presents an example where the domain consists of two intersecting ellipsoids. 
In essence, his example boils down to assuming the density $w(\mathbf{x})$ weighting the input space is a mixture of Gaussians. 
$$ w(\mathbf{x}) = \frac{1}{2}\mathcal{N}_2(\mathbf{x}|\mathbf{\mu}_1, \mathbf{\Sigma}_1) + \frac{1}{2}\mathcal{N}_2(\mathbf{x}|\mathbf{\mu}_2, \mathbf{\Sigma}_2)$$
The below code more-or-less samples from $\mathbf{w}(\mathbf{x})$ (instead of randomly choosing which Gaussian to sample from, 
it simply chooses equal sample sizes from each) to construct the grid, and then proceeds as before. 
```{r}
X_grid_1 <- rmvnorm(100, mean=c(0.25, 0.25), sigma=0.005*rbind(c(2, 0.35), c(0.35, 0.1)))
X_grid_2 <- rmvnorm(100, mean=c(0.25, 0.25), sigma=0.005*rbind(c(0.1, -0.35), c(-0.35, 2)))
X_grid <- rbind(X_grid_1, X_grid_2)

X <- min_IMSPE(N = 25, D = 2, N_itr = 10000, X_grid = X_grid)
plot(X[,1], X[,2], xlab = "X1", ylab = "X2", main = paste0("min_IMSPE design Monte Carlo approx; N_itr = ", N_itr))
points(X_grid, pch=20, cex=0.25, col="gray")
```

# Sequential Design/Active Learning
We have currently entertained the notion of *batch design*; that is, selecting the $N$ design points $\mathbf{X}$ in one fell swoop. An alternative, often very practical approach, is to choose the points one at a time - at each iteration selecting the next point based on the criterion 
$J(\mathbf{x})$. The general idea is as follows:

1. Start with a small design $\mathbf{X}_{N_0}$ consisting of $N_0$ inputs typically chosen via some sort of model-free, space-filling design 
(e.g. LHS). Run the forward model at these inputs and fit the GP on this initial dataset $\mathcal{D}_{N_0}$ (e.g. estimate the hyperparameters via MLE). 
Set $n := N_0$. 
2. Select the next design point via 
$$ \mathbf{x}_{n + 1} := \text{argmax}_{\mathbf{x}} J(\mathbf{x})|\mathcal{D}_n$$
Augment the design $\mathbf{X}_{n + 1} := \mathbf{X}_n \cup \mathbf{x}_{n + 1}$ and run the forward model at the new input point. Denote the 
new augmented dataset $\mathcal{D}_{n + 1}$. Re-fit the GP on the new dataset. 
3. Repeat step 2 until a desired number of design points $N$ is reached. 

There are many optimization tricks and pitfalls when considering such sequential schemes. For example, one might not update the MLE GP fit every iteration to save on computation, but this could potentially lead to unsatisfactory results. It is also common to *warm start* the MLE hyperparameter 
optimization by setting the initial values in the optimization to the values of the hyperparameters that were optimized in the previous step. 

## Active Learning MacKay 
Perhaps the simplest sequential design criterion is to choose the point that maximizes the predictive variance. The idea is that if the predictive variance is very high at a point, then the GP is very uncertain about the latent function at that point so we should run the model at that point to 
reduce that uncertainty. The objective thus looks like
$$ \mathbf{x}_{n + 1} := \text{argmax}_{\mathbf{x}} J(\mathbf{x})|\mathcal{D}_n = \text{argmax}_{\mathbf{x}} k_{\mathbf{X}_n}(\mathbf{x})$$
This method is known as *Active Learning MacKay (ALM)*, after David MacKay, who explored this notion of design for neural networks and argued that this sequential design strategy tends to approximate a maxent design. 

Let's consider a toy example with 2D input space. The below code chunk defines the latent function we seek to learn. The function is a scaled 
and shifted version of 
$$ f(x_1, x_2) = x_1 \exp(-x_1^2 - x_2^2)$$
A heat map provides a visualization of this function below. 

```{r}
f <- function(X, sd=0.01) {
  X[,1] <- (X[,1] - 0.5)*6 + 1
  X[,2] <- (X[,2] - 0.5)*6 + 1
  f_vals <- X[,1] * exp(-X[,1]^2 - X[,2]^2)
  y_vals <- f_vals + rnorm(nrow(X), sd=sd)
  
  return(list(f = f_vals, y = y_vals))
}
```


```{r}
# Evaluate function at initial space-filling design.
N0 <- 12
X <- randomLHS(N0, 2)
f_list <- f(X)
f_vals <- f_list$f
y <- f_list$y

# Also evaluate at dense grid for plotting. 
x1_grid <- seq(0, 1, length=51)
x2_grid <- seq(0, 1, length=51)
X_grid <- as.matrix(expand.grid(x1_grid, x2_grid))
f_grid <- f(X_grid)$f
```

```{r}
cols <- heat.colors(128)

# Latent function, noiseless evaluations. 
image(x1_grid, x2_grid, matrix(f_grid, ncol=51), 
      xlab="x1", ylab="x2", main = "Noiseless evaluations of f(), and initial design points.", col=cols)
points(X[,1], X[,2])
```

We observe that this is a tricky function, with a steep peak right next to a deep trough. Beyond these two modes, the function is quite flat. So there are separate regimes of the input space, corresponding to sub-domains on which the function is highly non-linear and the remaining sub-domain on which the function is flat. 

We now fit a GP using the *laGP* package on this initial design. 
```{r}
g <- garg(list(mle=TRUE, max=1), y)
d <- darg(list(mle=TRUE, max=0.25), X)
gp0 <- newGP(X, y, d=d$start, g=g$start, dK=TRUE)
mle <- jmleGP(gp0, c(d$min, d$max), c(g$min, g$max), d$ab, g$ab)
```

Now, let's turn to the sequential design problem with ALM objective (i.e. activation) function. At each iteration of this procedure, the selection of the next point requires solution of an optimization problem 
$$ \mathbf{x}_{n + 1} := \text{argmax}_{\mathbf{x}} k_{\mathbf{X}_n}(\mathbf{x})$$
This problem may be challenging in and of itself. Recall that predictive GP variances $k_{\mathbf{X}_n}(\mathbf{x})$ form a sort of "sausage" shape, shrinking at the observed locations and increasing as $\mathbf{x}$ moves away from observed locations. This means that the objective $k_{\mathbf{X}_n}(\mathbf{x})$ may have many local minima, in particular on the order of the number of observed locations. This presents a challenge for local optimization methods. To deal with this challenge, a common approach is to run a local optimizer multiple times initialized at different starting locations. The minimum of all the different runs can then be taken as $\mathbf{x}_{n + 1}$. The choice of starting locations for the optimizer is a whole design problem in its own right. Intuitively, we don't want 1.) starting locations to be close to each other and 2.) two starting locations to both be close to the same local minimum. In particular, imagine in a 1d problem that two starting locations are placed on the same "sausage"; we then expect the local minimizers both to converge to the same local minima. Thus, it seems that the ideal starting locations are those corresponding to large predictive variances; that is, the "fat" parts of the sausage. To this end, a space-filling design such as maximin seems a reasonable choice for the starting locations. 

The below code implements the ALM objective, as well as a solution to the ALM search for $\mathbf{x}_{n + 1}$. Note the negative sign, which means we seek to minimize, not maximize, `obj_alm`, to align with the R function `optim`. However, in `xnp1_2d_search` we negate the value of the objective, so that we can again view things as a maximization problem to align with the exposition above. 

```{r}
# Note that `optim()` minimizes by default, hence the negative. 
obj_alm <- function(x, gp) {-sqrt(predGP(gp, matrix(x, nrow=1), lite=TRUE)$s2)}
```


```{r}
xnp1_2d_search <- function(X, gp, obj=obj_alm, ...) {
  # X = current design (x_1, ..., x_n). 
  
  start_locs <- mymaximin(nrow(X), 2, T=100*nrow(X), Xorig=X) # Starting locations for optimization runs generated by maximin.
  local_optima <- matrix(NA, nrow=nrow(start_locs), ncol=ncol(X) + 1)

  for(i in 1:nrow(start_locs)) {
    optim_results <- optim(start_locs[i,], obj, method="L-BFGS-B", lower=0, upper=1, gp=gp, ...) # Run optimization at each starting location. 
    local_optima[i,] <- c(optim_results$par, -optim_results$value)
  }
  solns <- data.frame(cbind(start_locs, local_optima))
  names(solns) <- c("init1", "init2", "optim1", "optim2", "optim_val")
  return(solns)
  
}

```

We now feed the initial LHS design to the one-step search function, and observe how they are updated. To recap, here is what is happening: 

* We created an initial space-filling design $\mathbf{X}_n$ with $n$ inputs using LHS. 
* The GP kernel hyperparameters are estimated using this initial design. 
* We now seek to choose $\mathbf{x}_{n + 1}$ by maximizing the ALM criterion. To do so, we run an L-BFGS optimizer from multiple starting locations. 
* The starting locations for the optimizer are chosen by applying a *maximin* procedure ot the initial design $\mathbf{X}_n$. 
* In the plot below, the arrows are drawn from the initial locations fed to the optimizer to the optimized values returned by the optimizer. In practice, we would of course choose the point returned by the optimizer that resulted in the largest ALM criterion as the point $\mathbf{x}_{n+1}$. This point is plotted in red below. Arrows with near-zero length are omitted. The points on the plot compose the initial design $\mathbf{X}_n$. 

```{r}
solns <- xnp1_2d_search(X, gp0)
image(x1_grid, x2_grid, matrix(f_grid, ncol=51), xlab="x1", ylab="x2", col=cols)
points(X, xlab="x1", ylab="x2", xlim=c(0,1), ylim=c(0,1))
arrows(solns$init1, solns$init2, solns$optim1, solns$optim2, length=0.1)
m <- which.max(solns$optim_val)
prog <- solns$val[m]
points(solns$optim1[m], solns$optim2[m], col="red", pch=20)
```
We clearly see that there are many local optima; hence, the multiple initialization scheme is essential here. In high dimensions, the number of initialization points for the optimizer would have be very, very large to give us any sort of confidence that we are finding the global maximum. In, say, 100 dimensions, running with 100 initializations essentially gives us no guarantee that we have achieved the global minimum. We also note that, given that there are few points in the initial design and this is only the first point chosen sequentially, it is likely that the point will be close to a boundary. This is due to the fact that the initial design was chosen via LHS, which does not tend to clump at the boundaries. And the GP will typically be more uncertain near the boundaries, so the effect of the ALM criterion is to encourage points that fill out the boundaries. 

With $\mathbf{x}_{n+1}$ in hand we now run the forward model to obtain $y_{n+1} = y(\mathbf{x}_{n})$, augment the dataset to include 
$(\mathbf{x}_{n+1}, y_{n+1})$ to produce $\mathcal{D}_{n+1}$, and then re-fit the GP on the new dataset. If it seems wasteful to re-run the 
entire MLE (an $O(n^3)$ operation) when $\mathcal{D}_{n+1}$ only differs by $\mathcal{D}_{n}$ by a single point, that's because it is. Later on, 
we will show that this update can be performed in $O(n^2)$ time instead. For now, we will use the fast GP update provided by 
`laGP::updateGP`. We also keep track of root mean square error evaluated at the same set of grid points used to plot the heatmap. 

```{r}

# Size of initial design and final design. 
N0 <- 12
N <- 25

# Initial space-filling design. Run forward model on initial design. 
X <- randomLHS(N0, 2)
f_list <- f(X)
f_vals <- f_list$f
y <- f_list$y 

# Initial GP fit.
g <- garg(list(mle=TRUE, max=1), y)
d <- darg(list(mle=TRUE, max=0.25), X)
gp <- newGP(X, y, d=d$start, g=g$start, dK=TRUE)
mle <- jmleGP(gp0, c(d$min, d$max), c(g$min, g$max), d$ab, g$ab)

# Sequential design. 
objective_value <- vector(mode = "numeric", length = N-N0)
for(n in seq(1, N - N0)) {
  
  # Optimize ALM to get new point. 
  solns <- xnp1_2d_search(X, gp, obj = obj_alm)
  m <- which.max(solns$optim_val)
  objective_value[n] <- solns$optim_val[m]
  x_new <- as.matrix(solns[m, c("optim1", "optim2")], nrow = 1)
  
  # Run forward model at new point and augment dataset. 
  y_new <- f(x_new)$y
  X <- rbind(X, x_new)
  y <- c(y, y_new)
  
  # GP predictions over grid, for plotting below. 
  gp_pred <- predGP(gp, X_grid, lite = TRUE)
  
  # Update GP hyperparameter fit. 
  updateGP(gp, x_new, y_new)
  mle <- rbind(mle, jmleGP(gp, c(d$min, d$max), c(g$min, g$max), d$ab, g$ab))
  
}

```


The below plot shows the true response surface and overlays the design points obtained from the above procedure. The points constitute the initial space-filling design, while the numbers mark the points selected during the sequential design phase, numbered in the order that they were chosen. 
```{r}
image(x1_grid, x2_grid, matrix(f_grid, ncol=51), xlab="x1", ylab="x2", col=cols)
points(X[1:N0,1], X[1:N0,2])
text(X[(N0+1):nrow(X),1], X[(N0+1):nrow(X),2], cex = 0.75)
```

We also plot the GP predictive mean and predictive standard deviation surfaces to compare ot the ground-truth above.
```{r}
par(mfrow=c(1,2))

# Plot 1: predictive mean. 
image(x1_grid, x2_grid, matrix(gp_pred$mean, ncol=length(x1_grid)), xlab="x1", ylab="x2", col=cols, main = "Predictive Mean")
points(X[1:N0,1], X[1:N0,2])
text(X[(N0+1):nrow(X),1], X[(N0+1):nrow(X),2], cex = 0.75)

# Plot 2: predictive standard deviation. 
image(x1_grid, x2_grid, matrix(sqrt(gp_pred$s2), ncol=length(x1_grid)), xlab="x1", ylab="x2", col=cols, main = "Predictive SD")
points(X[1:N0,1], X[1:N0,2])
text(X[(N0+1):nrow(X),1], X[(N0+1):nrow(X),2], cex = 0.75)
```

The sequential design procedure also provides a measure of progress. In particular, we can monitor the optimal value of the objective function found during each iteration. In the case of ALM, this means monitoring the maximum predictive variance. 
```{r}
plot((N0+1):N, objective_value, xlab = "n", ylab = "Maximum Predictive Variance", main = "Measure of Progress")
```

It would initially seem that the predictive variance could only decrease at iteration. This would be true if the GP hyperparameters were fixed the whole time. However, since the hyperparameters are re-fit at each iteration, it is possible to have jumps in maximum predictive 
variance. In particular, the predictive variance can be very sensitive to the marginal variance estimate $\hat{\tau}_n$. We could avoid 
jumps in the plot by considering the predictive variance normalized by $\hat{\tau}_n$. 

## Active Learning Cohn 
One downside of the ALM criterion is that it only targets locations where predictive variance is large at a single point. It ignores the fact that observing a new data point will have an effect on predictive variance over the entire input domain. We might want a criterion 
that can take into account regions of the input space (not just individual points) that cumulatively have high uncertainty. We have seen ideas like this when discussing the IMSPE criterion in the one-shot design setting. 

The idea here is to pick $x_{n+1}$ that would cause the largest reduction in the predictive variance, averaged over the input space. 
Recall that the predictive variance at point $x$ is given by 
$$ 
k_{X_n}(x) = k_n(x) - k_n(x, X_n)K_n(X_n)^{-1}k_n(X_n, x)
$$
where I utilize the subscript $n$ to indicate that the kernel hyperparameters are fit using the design points $x_1, \dots, x_n$. 
We define $\tilde{k}_{X_{n+1}}(x)$ to be the predictive variance at $x$ for the GP that has been run at the additional input 
$x_{n + 1}$ and conditioned on the new observation $(x_{n + 1}, y_{n + 1})$, but still using the hyperparameter estimates 
based on $x_1, \dots, x_n$. That is, 
$$ 
\tilde{k}_{X_{n+1}}(x) = k_n(x) - k_n(x, X_{n+1})K_n(X_{n+1})^{-1}k_n(X_{n+1}, x)
$$
I use a subscript $n$ on the kernel $k$ to emphasize that the hyperparameter estimates are based on the first $n$ design points. 
Now we can consider the average reduction in uncertainty:
$$
\Delta k_n(x_{n+1}) := \int_{\mathcal{X}} \left[k_n(x) - \tilde{k}_{n+1}(x) \right] dx
$$
Since $k_n(x)$ does not depend on $x_{n+1}$, maximizing this quantity is the same as minimizing 
$$
J(x_{n + 1}) := \int_{\mathcal{X}} \tilde{k}_{n+1}(x) dx
$$
Essentially this is the average one-step-look-ahead uncertainty, since it considers what the predictive variance would be were we to 
add the new design point $x_{n+1}$. This is the ALC objective. It is the sequential version of the IMSPE. I will not implement this here, but just make a few notes. In general, 
ALC favors candidate points in the interior of the domain moreso than ALM. This is due to the fact that it takes into account the 
effect of the design points on uncertainty over the entire domain, and points in the interior have more of a global impact that points 
on the boundaries. For a rectangular
input space $\mathcal{X}$ this can be evaluated in closed form. However, in general it is common to approximate this integral via a Monte Carlo approach. In particular, a space-filling reference design (e.g. LHS) is utilized to approximate $J(x_{n+1})$. When this is done we observe similar behavior as when investigating IMSPE approximated by a reference grid of points above. ALC will favor candidate points 
that are far from the current design $X_{n+1}$ but close to the reference grid. If a LHS is used to generate the reference points, then 
few of the reference points will be near the boundaries. This will do even more to favor points in the interior of the domain. 

## Fisher Information Objective
**TODO**

# Sequential Design for Bayesian Inverse Problems
The model-based design strategies discussed above seek to select a point $\mathbf{x}_{n+1}$ that is optimal in some sense for improving the GP fit. These strategies have presumed that we care about the GP fit uniformly across the input space. However, in the setting of Bayesian inverse problems it is common that 
the posterior distribution is highly concentrated in the input space. Thus, in most of the input space the posterior will effectively be zero, so we care much less about GP predictive performance in these regions. Instead, we would like a design strategy that produces design points in the region of actual importance; that is, the region of significant posterior mass. This is yet another chicken-and-egg type problem, since the ultimate goal of solving a Bayesian inverse problem is to characterize the posterior distribution, which is unknown in advance. However, each time we run the forward model at a new design points $\mathbf{x}_n$ we can also evaluate (up to a normalizing constant) the posterior density; thus, as sequential design proceeds we get more and more information about the posterior. We can thus try for an active learning approach, which chooses $\mathbf{x}_{n + 1}$ based on an approximation to the posterior constructed using 
$\mathbf{x}_{1}, \dots, \mathbf{x}_{n}$. Once $\mathbf{x}_{n + 1}$ is selected and the forward model run at this input point, the posterior approximation can be refined and then used to select the subsequent point. To my knowledge, this type of strategy has not been deeply explored in the literature. We start by outlining an approach proposed by Micheal Sinsbeck and Wolfgang Nowak in 2017. 

First, we establish some notation. The posterior of interest is 
$$ \pi(\mathbf{x}) \propto \mathcal{L}(\mathbf{x})\pi_0(\mathbf{x})$$
Evaluating the likelihood $\mathcal{L}(\mathbf{x})$ requires running the expensive forward model, so we opt to approximate the likelihood with a surrogate 
$\hat{\mathcal{L}}(\mathbf{x})$. We assume a GP surrogate has been used, either to directly emulate the likelihood itself, or to emulate the forward model, or to emulate some sufficient statistic of the likelihood.  
We will entertain a sequential design strategy, so let $\hat{\mathcal{L}}_n$ denote the likelihood approximation resulting from conditioning the GP on the data observed at the first $n$ design points $\mathbf{x}_1, \dots, \mathbf{x}_n$. Let $y^*_n$ denote the GP conditioned on these same points, which may or may not be the same as $\hat{\mathcal{L}}_n$, depending on what specifically the GP is emulating. 


## Sinsbeck and Nowak Approach
The Sinsbeck and Nowak (SN) approach seeks to select design points based on an adaptively-improved likelihood approximation. They comment that this is easier than constructing a posterior approximation, as the posterior has the constraint that it must integrate to 1, among other difficulties. 

Supposing the likelihood is given by a probability density, they consider an $L_2$, prior-weighted error between likelihoods:
$$\ell(f, g) = \mathbb{E}_{\pi_0}\left[(f(\mathbf{x}) - g(\mathbf{x}))^2 \right]$$
Note that the likelihood is the Radon-Nikodym derivative of the posterior with respect to the prior, so assessing the error in the likelihood approximation with respect to the prior measure seems a resonable thing to do. Of course, $\ell(\mathcal{L}, \cdot)$ cannot be computed without knowledge of the true likelihood $\mathcal{L}$. The idea is thus to replace $\mathcal{L}$ with the random approximation $\hat{\mathcal{L}}_n$ and average the whole error over
$y^*_n$.


## My personal notes:
As motivation, first recall the IMSPE activation function from above: 
$$ J(\mathbf{X}) = \int k_{\mathbf{X}}(\mathbf{x}) d\mathbf{x}$$
In the definition provided above, we also normalized by the marginal prior variance and multiplied by weighting function $w(\mathbf{x})$ but the idea is the same. The idea with this activation function is to choose the design $\mathbf{X}$ that minimizes the average predictive uncertainty in the GP. We also explored the 
ALM activation function for sequential design
$$ J(\mathbf{x}) = k_{\mathbf{X}_n}(\mathbf{x})$$
where similarly, the next input is chosen such that it corresponds to the location where the GP is currently most uncertain. Both of these have intuitive appeal, but may not be ideal in the Bayesian inverse problem setting. The IMSPE averages over the entire input space, but the posterior may only be concentrated in a small subset of the input space. Similarly, ALM chooses a point where the GP is most uncertain, but maybe that point corresponds to a location where the posterior is essentially 0. The SN approach addresses this by acknowledging that it is not really the GP itself we care about, but really the approximation of the posterior induced by the GP. As mentioned above, they actually focus on the approximation to the likelihood, but we should keep in mind that the ultimate focus is the posterior. Letting $\mathcal{L}_n$ denote the GP-induced likelihood approximation constructed from a set of inputs $\mathbf{x}_1, \dots, \mathbf{x}_n$ a reasonable modification of the ALM criterion is 
$$ J(\mathbf{x}) = \text{Var}_{y_n}(\mathcal{L}_n(\mathbf{x}))$$
where now the predictive variance of $\mathcal{L}_n$ is considered instead of the predictive variance of $y_n$. This directly targets uncertainty in the likelihood approximation but still doesn't address the issue of favoring regions of the input space of high posterior mass. 


# Appendix 

## Determinant and Inverse of Block Matrix
In the sequential design setting, we are continually adding data to the design $X_n$ and then having to re-compute quantities such as 
$K(X_{n+1})^{-1}$ and $\det\left(K(X_{n+1}) \right)$. Formulas for the inverse and determinant of block matrices will come in handy 
when deriving fast updates for these quantities. 

### Determinant
Let $A$, $B$, $C$, $D$ be square matrices with $A$ invertible. Then, 
$$ 
\det\begin{pmatrix} A & B \\ C & D \end{pmatrix} = \det \begin{pmatrix} A & B \\ 0 & D - CA^{-1}B \end{pmatrix} = \det(A) \cdot 
\det\left(D - CA^{-1}B \right)
$$

### Inverse
James E. Pustejovsky provides a nice post on the inversion of partitioned matrices $\href{https://www.jepusto.com/inverting-partitioned-matrices/}{here}$. Suppose $A$ and $D$ are invertible, not necessarily of 
equal dimension. Letting $S = \left(D - CA^{-1}B\right)^{-1}$ we have 
$$
\begin{pmatrix} A & B \\ C & D \end{pmatrix}^{-1} = \begin{pmatrix} A^{-1} + A^{-1}BSCA^{-1} & -A^{-1}BS \\
-SCA^{-1} & S \end{pmatrix}
$$

## Information and Entropy of multivariate Gaussian
Here we derive the Shannon information content of a multivariate Gaussian 

$$ X \sim \mathcal{N}_D(\mu, \Sigma)$$

We have 
$$
\begin{align*}
I(X) &= \mathbb{E}\left[\log p_X(X) \right] \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mathbb{E}\left[(X - \mu)^T \Sigma^{-1} (X - \mu) \right] \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mu^T \Sigma^{-1}\mu - \frac{1}{2}\mathbb{E}\left[X^T \Sigma^{-1} X \right] + \mathbb{E}\left[X^T \Sigma^{-1} \mu \right] \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mu^T \Sigma^{-1}\mu - \frac{1}{2}\left[\text{tr}(\Sigma^{-1}\Sigma) + \mu^T \Sigma^{-1} \mu \right] + \mu^T \Sigma^{-1} \mu \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{1}{2} \mu^T \Sigma^{-1}\mu - \frac{D}{2} - \frac{1}{2}\mu^T \Sigma^{-1} \mu + \mu^T \Sigma^{-1} \mu \\
     &= -\frac{1}{2}\log\det(2\pi \Sigma) - \frac{D}{2} \\
     &= -\frac{1}{2}\log\det(2\pi e \Sigma)
\end{align*}
$$

The entropy is thus 
$$ H(X) = -I(X) = \frac{1}{2}\log\det(2\pi e \Sigma)$$

## Fast GP Updates
At first glance, the sequential design GP update at step $n$ appears to incur $O(n^3)$ runtime. If the scheme is run for $N$ steps 
then the whole procedure is a costly $O(N^4)$ scheme. Fortunately, this need not be the case. Most updates to an existing 
GP based on the addition of a single data point $(x_{n+1}, y_{n+1})$ can be accomplished in $O(n^2)$ time, meaning the whole procedure 
is now $O(N^3)$. To be more clear, let us consider what actually needs to be updated. There are two general regimes here:
1.) Updating GP predictive quantities with fixed hyperparameterization. 
2.) Update the GP hyperparameterization (i.e. re-fit the hyperparameters). 
The second is a more costly operation which will be dicussed later. We begin by focusing on the first item. As far as these notes are concerned, the main goal is to be able to reduce the computation in evaluating the acquisition function/design criterion. These 
criteria typically depend on the GP predictive quantities, so we will consider fast updates for: 1.) the inverse kernel matrix; 
i.e. updating $K(X_n)^{-1}$ to $K(X_{n+1})^{-1}$; 2.) the predictive mean and variance at fixed input locations; e.g. updating $k_{X_n}(x)$ to $k_{X_{n+1}}(x)$; 3.) the log determinant of the kernel matrix; i.e. updating $\log \text{det}(K(X_n))$ to 
$\log \text{det}(K(X_{n+1}))$. Throughout this section I will let $K_n$ denote the kernel matrix constructed from 
$x_1, \dots, x_n$ and $K_{n+1}$ the kernel matrix constructed from $x_1, \dots, x_n, x_{n+1}$ each using the same GP 
hyperparameterization. Note that this contrasts with the notation sometimes used in the main text, where the kernel subscript indexed
the hyperparameterization. I will also let $k_n := k(X_n, x_{n+1})$ and $k_{n+1} := k(x_{n+1})$. 

### Updating the inverse kernel matrix 
The addition of a new design point $x_{n+1}$ increases the dimension of the kernel matrix by one. The matrix can be written in block 
form as 
$$
K_{n+1} = \begin{pmatrix} K_n & k_n \\ k_n^T & k_{n+1} \end{pmatrix}
$$
To find the inverse we can apply the inverse formula for block matrices presented earlier in the appendix. First we compute the matrix $S$ from 
that section to be
$$ S = \left(k_{n+1} - k_n^T K_n^{-1} k_n\right)^{-1} = \frac{1}{k_{X_n}(x_{n+1})}$$
The second equality follows from the fact that we recognize the expression as the predictive variance at $x_{n+1}$ given the design $X_n$. Thus, in this 
case $S$ is just a scalar. Applying the partitioned inverse formula now yields 
$$
K_{n+1}^{-1} = \begin{pmatrix} K_n^{-1} + k^{-1}_{X_n}(x_{n+1})K_n^{-1} k_n k_n^T K_n^{-1} & -k^{-1}_{X_n}(x_{n+1}) K_n^{-1}k_n \\
-k^{-1}_{X_n}(x_{n+1}) k_n^T K_n^{-1} & k^{-1}_{X_n}(x_{n+1}) \end{pmatrix}
$$
Noticing that many of the same terms come up in each entry, we can clean this up. Define $\nu := k_{X_n}(x_{n+1})$ and 
$z := -\nu K_n^{-1} k_n$. Plugging these in, we obtain 
$$ 
K_{n+1}^{-1} = \begin{pmatrix} K_n^{-1} + \nu zz^\top & z \\ z^\top & \nu^{-1} \end{pmatrix}
$$
Given that $K_n^{-1}$ is already available, we observe that all of the computations here involve either matrix-vector products, or an outer 
product between vectors. Thus, updating $K_n^{-1}$ to $K_{n+1}^{-1}$ can be done in $O(n^2)$ operations. 


## Integrating GP Predictive Variance 
Many sequential design criteria, such as the integrated mean squared prediction error (IMSPE) and Active Learning Cohn (ALC) require 
computing an integral of the GP predictive variance of the input domain. In simple settings, such as when the input domain is 
rectangular, this can be done in closed-form. The integral we seek to compute is 
$$
\begin{align*}
I_{n+1}(x_{n+1}) &= \int_{\mathcal{X}} k_{X_n}(x) \rho(x) dx
\end{align*}
$$
where $\rho$ is a density on $\mathcal{X}$. We take the input domain to be the unit hypercube $\mathcal{X} = [0, 1]^D$. The beginning of this 
derivation will be applicable to any covariance family. However, eventually we must specify a specific family. We consider the squared exponential 
kernel with the following parameterization. 
$$
k(x, x^\prime) = \tau^2 \exp\left\{-\frac{1}{2} \sum_{d = 1}^{D} \left(\frac{x_d - x_d^\prime}{\theta_d}\right)^2  \right\} = \tau^2 \exp\left\{-\frac{1}{2} (x - x^\prime)^\top \Sigma_\theta^{-1}(x - x^\prime) \right\}
$$
where $\Sigma_\theta = \text{diag}\left(\theta_1^2, \dots, \theta_D^2 \right$. 


### The General Case 

We now consider evaluation of the integral $I_{n+1}(x_{n+1}) $. It is helpful to re-write this as an expectation. 
$$
\begin{align*}
I_{n+1}(x_{n+1}) &= \mathbb{E}_{x \sim \rho}\left[k_{X_n}(x) \right] \\
                 &= \mathbb{E}_{x \sim \rho}\left[k(x) - k_{n+1}^\top K_{n+1}^{-1} k_{n+1} \right] \\
                 &= \mathbb{E}_{x \sim \rho}[k(x)] - \mathbb{E}_{x \sim \rho}\left[k_{n+1}^\top K_{n+1}^{-1} k_{n+1} \right]
\end{align*}
$$
Note that $K^{-1}_{n+1}$ does not depend on $x$ so is fixed with respect to the expectation; $k_{n+1}$ does depend on $x$ since 
$k_{n+1} = k(X_n, x) = \left(k(x_1, x), \dots, k(x_n,x) \right)^\top$. The second term is thus an expectation of a quadratic form with 
fixed matrix and a random vector. Such expectations are well-known to be equal to the trace of the produce of the matrix times the 
covariance matrix of the vector: 
$$
\mathbb{E}_{x \sim \rho}\left[k_{n+1}^\top K_{n+1}^{-1} k_{n+1} \right] = \text{tr}\left(K_n^{-1}\text{Cov}\left[k_{n+1}\right]\right)
$$
Let $\Sigma := \text{Cov}\left[k_{n+1}\right]$. In general this matrix has entries 
$$
\begin{align*}
\Sigma_{ij} &= \text{Cov}_{x \sim \rho}\left[k(x_i, x), k(x_j, x) \right] \\
            &= \mathbb{E}_{x \sim \rho}\left[k(x_i, x)k(x_j, x) \right] - \mathbb{E}_{x \sim \rho}\left[k(x_i, x)\right]\mathbb{E}_{x \sim \rho}\left[k(x_j, x) \right]
\end{align*}
$$
In the case that the GP has a zero-mean prior the second terms vanishes. Indeed, 
$$
\begin{align*}
\mathbb{E}_{x \sim \rho}\left[k(x_i, x)\right] &= \mathbb{E}_{x \sim \rho} \text{Cov}\left[y(x_i), y(x) \right] \\
                                               &= \mathbb{E}_{x \sim \rho} \mathbb{E}\left[y(x_i)y(x) \right] \\
                                               &= \mathbb{E} \mathbb{E}_{x \sim \rho} \left[y(x_i)y(x) \right] \\
                                               &= \mathbb{E}\left[y(x_i)\right] \mathbb{E}_{x \sim \rho} \left[y(x) \right] \\
                                               &= 0 \cdot \mathbb{E}_{x \sim \rho} \left[y(x) \right] \\
                                               &= 0
\end{align*}
$$
where the second line uses the assumption of zero-mean GP prior. This is about as far as we can go without making additional assumptions 
about $\mathcal{X}$, $\rho$, or $k(\cdot, \cdot)$. 

### Special Cases 
In certain simple cases, $\Sigma$ can be computed in closed-form. We focus here on the case where the kernel is of the exponentiated quadratic family. 
Recall from above that $\Sigma$ has entries of the form 
$$\Sigma_{ij} = \mathbb{E}_{x \sim \rho}\left[k(x_i, x)k(x_j, x) \right] - \mathbb{E}_{x \sim \rho}\left[k(x_i, x)\right]\mathbb{E}_{x \sim \rho}\left[k(x_j, x) \right]$$
Plugging in the expression for the exponentiated quadratic kernel yields the following results for the two expectations in the above expression. 
$$
\begin{align*}
\mathbb{E}_{x \sim \rho}\left[k(x, x^\prime)\right] &= \tau^2 \mathbb{E}_{x \sim \rho}\left[\exp\left\{-\frac{1}{2}(x - x^\prime)^{\top}\Sigma_\theta^{-1}(x - x^\prime)\right\} \right] \\
&= \tau^2 \int_{\mathcal{X}} \exp\left\{-\frac{1}{2}(x - x^\prime)^{\top}\Sigma_\theta^{-1}(x - x^\prime)\right\} \rho(x) dx
\end{align*}
$$

#### EQ Kernel, No weights, Unbounded Domain 
First we consider the form $\Sigma$ takes in the specific case of the exponentiated quadratic kernel, $\rho(x) \equiv 1$ and $\mathcal{X} = \mathbb{R}^D$. 
In this case, the required expectations simplify to 
$$
\begin{align*}
\mathbb{E}_{x \sim \rho}\left[k(x, x^\prime)\right] &= \tau^2 \int_{\mathcal{X}} \exp\left\{-\frac{1}{2}(x - x^\prime)^{\top}\Sigma_\theta^{-1}(x - x^\prime)\right\} \rho(x) dx \\
&= \tau^2 \int_{\mathbb{R}^D} \exp\left\{-\frac{1}{2}(x - x^\prime)^{\top}\Sigma_\theta^{-1}(x - x^\prime)\right\} dx \\
&= \tau^2 \int_{\mathbb{R}^D} \text{det}\left(2\pi \Sigma_\theta \right)^{1/2} \mathcal{N}_D\left(x|x^\prime, \Sigma_\theta \right)  dx \\
&= \tau^2 \text{det}\left(2\pi \Sigma_\theta \right)^{1/2}
\end{align*}
$$
Now for the second expectation. 
$$
\begin{align*}
\mathbb{E}_{x \sim \rho}\left[k(x_i, x)k(x_j, x) \right] &= \int_{\mathcal{X}} k(x_i, x)k(x_j, x) \rho(x) dx \\
&= \int_{\mathbb{R}^D} k(x_i, x)k(x_j, x) dx \\
&= \tau^4 \int_{\mathbb{R}^D} \exp\left\{-\frac{1}{2} (x - x_i)^\top \Sigma_\theta^{-1}(x - x_i) -\frac{1}{2} (x - x_j)^\top \Sigma_\theta^{-1}(x - x_j) \right\} dx \\
&= \tau^4 \exp\left\{-\frac{1}{2} x_i^\top \Sigma_\theta^{-1} x_i - \frac{1}{2} x_j^\top \Sigma_\theta^{-1} x_j\right\} 
\int_{\mathbb{R}^D} \exp\left\{-x^\top \Sigma_\theta^{-1}x + x^\top \Sigma_\theta^{-1}(x_i + x_j) \right\} dx \\
&= \tau^4 \exp\left\{-\frac{1}{2} x_i^\top \Sigma_\theta^{-1} x_i - \frac{1}{2} x_j^\top \Sigma_\theta^{-1} x_j + \frac{1}{2} \mu^\top A^{-1}\mu \right\} 
\int_{\mathbb{R}^D} \exp\left\{-\frac{1}{2}(x - \mu)^\top A^{-1} (x - \mu) \right\} dx \\
&= \tau^4 \exp\left\{-\frac{1}{2} x_i^\top \Sigma_\theta^{-1} x_i - \frac{1}{2} x_j^\top \Sigma_\theta^{-1} x_j + \frac{1}{2} \mu^\top A^{-1}\mu \right\} 
\text{det}\left(2\pi A \right)^{1/2}
\end{align*}
$$
where 
$$
\begin{align*}
A^{-1} &= 2\Sigma_\theta^{-1}, && \mu = \frac{x_i + x_j}{2}
\end{align*}
$$
Plugging in these values to the above expression, we obtain 
$$
\mathbb{E}_{x \sim \rho}\left[k(x_i, x)k(x_j, x) \right] = \tau^4 \exp\left\{-\frac{1}{4}(x_i - x_j)^\top \Sigma_\theta^{-1}(x_i - x_j) \right\}
\text{det}\left(\pi \Sigma_\theta \right)^{1/2}
$$

# References 
Binois, M, J Huang, RB Gramacy, and M Ludkovski. 2019. “Replication or Exploration? Sequential Design for Stochastic Simulation Experiments.” Technometrics 27 (4): 808–21. https://doi.org/10.1080/00401706.2018.1469433.






