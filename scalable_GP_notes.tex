\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{local-defs-GPs}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

\begin{document}

\begin{center}
\Large
Notes on Scalable Gaussian Processes
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Overview of Approximate GPs
\section{Overview of Approximate GPs}

% Pseudo Data
\subsection{Pseudo Data}
The use of pseudo data is a feature utilized in many different approximate GP schemes, including both the approximate generative model and approximate inference approaches. Hence, we choose to introduce 
this concept here rather than with a specific scalable GP method.

The primary computational challenge of working with GPs is performing operations on the dense $N \times N$ kernel matrix $K$, where $N$ is the number of training observations. When thinking about 
reducing the computational cost, a pretty natural thought is that maybe we can capture the essence of the training data with fewer than $N$ points. The idea is to introduce 
$M << N$ so-called pseudo data points (which may or may not be a subset of the original training dataset) and utilize these points in such a way that hopefully the computation time now scales as some function 
of $M$, rather than as $N^3$. When $M$ is much smaller than $N$ this can result in large computational gains, at the cost of some approximation error.

% Background
\section{Background}

% Factor Graphs
\subsection{Factor Graphs} 
It is useful to have visual representation of GP models, which clearly displays the conditional dependencies between all of the variables. \textit{Factor graphs} are such a tool. To begin, consider a density $p$
that can be factored as 
\[p(x_1, x_2, x_3) = p_1(x_1, x_2)p_2(x_2, x_3)\]
for some other densities $p_1$ and $p_2$. If we consider $x_2$ as fixed, then we see that $p(x_1, x_2, x_3)$ can be interpreted as the conditional density of $x_1$ and $x_3$ given $x_2$, up to a 
normalizing constant. The two terms on the righthand side can be interpreted similarly, so we see that this factorization is telling us that $x_1$ is independent of $x_3$, given $x_2$ (since their conditional 
joint density factors as a product of their marginal conditional densities). This conditional independence can equivalently be expressed as the following factor graph. 
\[\text{\textbf{TODO}: add factor graph}\] 
This context can help us discover a very nice interpretation of the precision matrix (i.e., the inverse covariance matrix) of a multivariate Gaussian distribution; namely, that the precision matrix encodes 
conditional independencies. This is in contrast to the covariance matrix of a Gaussian, which encodes independence between variables, which represents a stronger condition. To see this, consider the 
density for the gaussian $\mathcal{N}(0, \Sigma)$. 
\begin{align*}
\mathcal{N}(x|0, \Sigma) &\propto \exp\left\{-\frac{1}{2}x^T \Sigma^{-1}x \right\} \\
				       &= \exp\left\{-\frac{1}{2} \sum_{i, j} x_i x_j \left(\Sigma^{-1}\right)_{ij} \right\} \\
				       &= \prod_{i,j} \exp\left\{-\frac{1}{2} x_i x_j \left(\Sigma^{-1}\right)_{ij} \right\} \\
				       &\propto \prod_{i,j}  p_{ij}(x_i, x_j)
\end{align*}
Thus, we observe that Gaussian densities factor into a product of densities that each only depend on two of the variables. Notice that if $\left(\Sigma^{-1}\right)_{ij} = 0$ then 
$p_{ij}(x_i, x_j) = 1$ so the factor associated with the variables $i$ and $j$ drops out of the product and we conclude that $x_i$ and $x_j$ are conditionally independent, given all 
the other variables. Let's be a bit more rigorous in proving this result. 

Consider what this looks like for a three-dimensional Gaussian. Notice that the factor for each $i \neq j$ appears twice in the above product, so the density has the form
\[p(x_1, x_2, x_3) = p(x_1) p^2(x_1, x_2)p^2(x_1, x_3)p(x_2) p^2(x_2, x_3)p(x_3)\]
so if, say, $\left(\Sigma^{-1}\right)_{23} = 0$ then this becomes 
\[p(x_1, x_2, x_3) = p(x_1) p^2(x_1, x_2)p^2(x_1, x_3)p(x_2) p(x_3)\]
Now, notice that if we fix $x_1$ then $p(x_1)$ is just a constant, and the $p^2(x_1, x_3)$ is now viewed as a function of only $x_3$ so what's left is a product of terms that depend on 
$x_2$ times a product of terms that depend on $x_3$. Thus, $x_2$ is conditionally independent of $x_3$, given $x_1$. 

% The Nystrom Method
\subsection{The Nystrom Method}
This section largely follows the paper \cite{Sun}, which provides a nice review of Nystrom methods in machine learning. 

% Approximately Solving Integral Equations using Quadrature
\subsubsection{Approximately Solving Integral Equations using Quadrature}
The classical Nystrom method is concerned with numerically solving the integral equation 
\begin{align}
\int_{\inputSpace} \GPKer(\bx, \bx^\prime) \phi_i(\bx) d\bx = \lambda_i \phi_i(\bx^\prime) \label{integral_eqn}
\end{align}
where the $\phi_i$ are the eigenfunctions of the kernel $\GPKer(\cdot, \cdot)$ with associated eigenvalues $\lambda_i$. Here $\GPKer(\bx, \bx^\prime)$ is a kernel with 
Mercer representation \footnote{Mercer's theorem guarantees such a representation exists as long as the kernel is continuous and $\inputSpace$ is compact. We will suppose these assumptions hold.} 
\begin{align}
\GPKer(\bx, \bx^\prime) &= \sum_{i = 1}^{\infty} \lambda_i \phi_i(\bx) \phi_i(\bx^\prime) \label{Mercer_kernel}
\end{align}
Mercer's theorem implies the eigenvalues $\{\lambda_i\}$ are non-negative, and the eigenfunctions $\{\phi_i\}$ form an orthonormal basis for $L_2(\inputSpace)$. We will consider numerical 
approximation of the $\Ndesign$ dominant eigenvalues and eigenfunctions in \ref{Mercer_kernel}.

The Nystrom method essentially applies 
a quadrature rule to discretize, and thus approximately solve, \ref{integral_eqn}. Indeed, consider a Gauss quadrature rule 
$\{\bx_{\designIdx}, w_{\designIdx}\}_{\designIdx = 1}^{\Ndesign}$ consisting of $\Ndesign$ input points $\bx_{\designIdx}$ with associated weights $w_{\designIdx}$.
Applying this rule to \ref{integral_eqn} yields the approximation 
\begin{align}
\sum_{\designIdx = 1}^{\Ndesign} w_{\designIdx} \GPKer(\bx_{\designIdx}, \bx^\prime) \phi_i(\bx_{\designIdx}) = \lambda_i \phi_i(\bx^\prime) \label{integral_eqn_discretized}
\end{align}
for each $\bx^\prime \in \inputSpace$. We refer to the set $\{\bx_{\designIdx}\}_{\designIdx = 1}^{\Ndesign}$ as the set of \textit{samples} used to form the approximation. 
To discretize the continuum of $\bx^\prime \in \inputSpace$ values, the Nystrom method utilizes the same set of samples. In effect, we evaluate \ref{integral_eqn_discretized}
(as a function of $\bx^\prime$) at each of the samples $\bx_{\designIdx}$. This yields, 
\begin{align}
\sum_{\designIdx = 1}^{\Ndesign} w_{\designIdx}  \GPKer(\bx_{\designIdx}, \bx_m) \phi_i(\bx_{\designIdx}) = \lambda_i \phi_i(\bx_m), \text{ for } m = 1, \dots, \Ndesign \label{Nystrom_discretization}
\end{align}
The original integral equation \ref{integral_eqn} has now been fully discretized, approximated by the finite linear system \ref{Nystrom_discretization}. To write this linear system in 
matrix notation, define $\bu_i := \begin{pmatrix} \phi_i(\bx_1), \dots, \phi_i(\bx_N) \end{pmatrix}^{\top}$, $\bW := \text{diag}\left\{w_1, \dots, w_{\Ndesign} \right\}$, and 
$\kerMat$ the $\Ndesign \times \Ndesign$ matrix with entries $\kerMat[\designIdx, m] = \GPKer(\bx_{\designIdx}, \bx_m)$. We can now re-write \ref{Nystrom_discretization} as 
\begin{align}
\kerMat \bW \bu_i = \lambda_i \bu_i \label{Nystrom_eigenvalue_prob}
\end{align}
We observe that \ref{Nystrom_eigenvalue_prob} is a familiar eigenvalue problem from linear algebra. However, $\kerMat \bW$ is in general not symmetric, which renders the 
eigenvalue problem more difficult than the symmetric case. However, it is easy to symmetrize the problem by defining left-multiplying by $\bW^{1/2}$,
\[\bW^{1/2} \kerMat \bW \bu_i = \bW^{1/2} \kerMat \bW^{1/2} \bW^{1/2} \bu_i= \lambda_i \bW^{1/2} \bu_i\]
and defining $\tilde{\kerMat} := \bW^{1/2} \kerMat \bW^{1/2}$ (which is symmetric) and $\bv_i := \bW^{1/2} \bu_i$. This yields the equivalent symmetric eigenvalue problem 
\begin{align}
\tilde{\kerMat} \bv_i = \lambda_i \bv_i
\end{align}

Since $\tilde{\kerMat}$ is symmetric positive-definite, solving this eigenvalue problem yields an $\Ndesign$-dimensional orthonormal basis of 
eigenvectors $\bV$ (with columns $\bv_i$) and positive eigenvalues $\hat{\lambda}_1, \dots, \hat{\lambda}_{\Ndesign}$, sorted in descending 
order. That is, we have $\tilde{\kerMat} = \bV \Sig \bV^T$, where $\Sig := \text{diag}\left\{\hat{\lambda}_1, \dots, \hat{\lambda}_{\Ndesign} \right\}$. 
To obtain the eigenvectors of interest $\bu_i$, we can simply map back via $\bu_i = \bW^{-1/2}\bv_i$; i.e., $\bU = \bW^{-1/2}\bV$. Now, 
the $j^{\text{th}}$ entry of $\bu_i$ is our approximation of the $i^{\text{th}}$ eigenfunction evaluated at the $j^{\text{th}}$ quadrature
point (i.e. $\phi_i(\bx_j)$). Moreover, $\hat{\lambda}_i$ is our approximation to $\lambda_i$. But what if we want an approximation to 
$\phi_i(\bx^\prime)$, where $\bx^\prime \in \inputSpace$ is not one of the sample points? The Nystrom approximation provides us the formula 
\ref{integral_eqn_discretized}, which can readily be used to approximate $\phi_i(\bx^\prime)$. Indeed, dividing both sides by $\lambda_i$ and then substituting in the 
approximations $\hat{\lambda}_i \approx \lambda_i$ and $ \bU_{\designIdx i} \approx \phi_i(\bx_\designIdx)$, we obtain the estimate
\begin{align}
\phi_i(\bx^\prime) \approx \frac{1}{\hat{\lambda}_i } \sum_{\designIdx = 1}^{\Ndesign} w_{\designIdx} \GPKer(\bx_{\designIdx}, \bx^\prime)  \bU_{\designIdx i} \label{Nystrom_estimate}
\end{align}
Note that \ref{Nystrom_estimate} again utilizes an approximation based on the sampling points $\{\bx_{\designIdx}\}$.

We make one final note on the baseline Nystrom method before proceeding to generalizations. Recall that the Mercer representation of the kernel implied 
orthonormal eigenfunctions $\{\phi_i\}$. Therefore, when solving the equation \ref{integral_eqn}, we also seek to enforce the 
orthogonality constraint 
\begin{align}
\int_{\inputSpace} \phi_i(\bx) \phi_j(\bx) d\bx = \delta_{ij} \label{orthogonality_constraint}
\end{align}
Has this discretization procedure ensured a good approximation of this condition? Indeed, discretizing the integral \ref{orthogonality_constraint} (using the 
same quadrature rule as before) we see that
\begin{align*}
\sum_{\designIdx = 1}^{\Ndesign} w_{\designIdx} \phi_i(\bx_{\designIdx}) \phi_j(\bx_{\designIdx}) &\approx \sum_{\designIdx = 1}^{\Ndesign} \sqrt{w_{\designIdx}} \bU_{\designIdx i}\cdot \sqrt{w_k} \bU_{\designIdx j}\\
								   &=  \sum_{\designIdx = 1}^{\Ndesign} \bV_{\designIdx i} \bV_{\designIdx j} \\
								   &= \bv_i^{\top} \bv_j
\end{align*}
In words, the discretization tells us that to approximate the orthogonality of the $\phi_i$ we should impose orthogonality on the $\bv_i$. This
is convenient as standard software packages will return the $\bv_i$ already normalized, and then we do not need to worry about scaling the 
$\bu_i$ after executing the mapping $\bu_i = \bW^{-1/2}\bv_i$.   

% Approximately Solving Integral Equations using Monte Carlo
\subsubsection{Approximately Solving Integral Equations using Monte Carlo}
We now consider a generalization of the integral equation \ref{integral_eqn}, whereby instead of integrating uniformly over the compact set $\inputSpace$ we consider 
integrating over $\R^{\inputDim}$ with respect to a probability density $\rho$. 
\begin{align}
\int_{\R^{\inputDim}} \GPKer(\bx, \bx^\prime) \phi_i(\bx) \rho(\bx) d\bx = \lambda_i \phi_i(\bx^\prime) \label{integral_eqn_weighted}
\end{align}
Since the lefthand side is of the form $\E_{\bx \sim \rho}\left[\GPKer(\bx, \bx^\prime) \phi_i(\bx) \right]$ we consider discretization via a Monte Carlo approximation in place of the 
quadrature rule used above. For $\bx_1, \dots, \bx_{\Ndesign} \overset{iid}{\sim} \rho$ we have the Monte Carlo estimate
\begin{align}
\int_{\R^{\inputDim}} \GPKer(\bx, \bx^\prime) \phi_i(\bx) \rho(\bx) d\bx \approx \frac{1}{\Ndesign} \sum_{\designIdx = 1}^{\Ndesign} \GPKer(\bx_{\designIdx}, \bx^\prime) \phi_i(\bx_{\designIdx}).
\end{align} 
We now proceed exactly as before to obtain the discretized system of linear equations
\begin{align}
\frac{1}{\Ndesign} \sum_{\designIdx = 1}^{\Ndesign} \GPKer(\bx_{\designIdx}, \bx_m) \phi_i(\bx_{\designIdx}) &= \hat{\lambda}_i \phi_i(\bx_{m}), \text{ for } m = 1, \dots, M
\end{align}
and (using the same notation as the un-weighted case), we obtain the eigenvalue problem 
\begin{align}
\frac{1}{\Ndesign} \kerMat \bu_i &= \hat{\lambda}_i \bu_i
\end{align}
To write this more clearly, let $\tilde{\Sig} := \text{diag}\{\tilde{\lambda}_1, \dots, \tilde{\lambda}_{\Ndesign}\} := \text{diag}\{\Ndesign \hat{\lambda}_1, \dots, \Ndesign \hat{\lambda}_{\Ndesign}\}$. We then first solve the eigenvalue problem 
\begin{align}
\kerMat \tilde{\bU} &= \tilde{\Sig} \tilde{\bU}
\end{align}
To obtain estimates for the $\lambda_i$, we transform back via 
\begin{align}
\Sig &:= \text{diag}\{\hat{\lambda}_1, \dots, \hat{\lambda}_{\Ndesign}\} = \frac{1}{\Ndesign} \tilde{\Sig} 
\end{align}
The direction of the eigenvectors $\tilde{\bU}$ are unaffected by the scaling by $N^{-1}$, but we must ensure that this discretization has respected the weighted analog of the orthonormality constraint 
\ref{orthogonality_constraint}. We have 
\begin{align}
\int_{\inputSpace} \phi_i(\bx) \phi_j(\bx) d\bx &\approx \frac{1}{\Ndesign} \sum_{\designIdx = 1}^{\Ndesign} \tilde{\bU}_{\designIdx i} \tilde{\bU}_{\designIdx j} \\
								   &= \frac{1}{\Ndesign} \bu_i^\top \bu_j \\
								   &= \frac{1}{\Ndesign} \delta_{ij}
\end{align}
so while orthogonality is satisfied, the scaling is off. To correct for this we define $\bU := \sqrt{\Ndesign} \tilde{\bU}$. To summarize, the final estimates for the eigenvalues and 
eigenfunctions (evaluated at the sampling points) are given by 
\begin{align}
&\Sig = \frac{1}{\Ndesign} \tilde{\Sig}  &&\bU = \sqrt{\Ndesign} \tilde{\bU} \label{Nystrom_eigs_weighted}
\end{align}
The associated estimate for $\phi_i(\bx^\prime)$ at new input $\bx^\prime$ (the analog of \ref{Nystrom_estimate}) is 
\begin{align}
\phi_i(\bx^\prime) \approx \frac{1}{\hat{\lambda}_i \Ndesign} \sum_{\designIdx = 1}^{\Ndesign} w_{\designIdx} \GPKer(\bx_{\designIdx}, \bx^\prime)  \bU_{\designIdx i} = \frac{\sqrt{\Ndesign}}{\tilde{\lambda}_i} \sum_{\designIdx = 1}^{\Ndesign} w_{\designIdx} \GPKer(\bx_{\designIdx}, \bx^\prime)  \tilde{\bU}_{\designIdx i} \label{Nystrom_estimate_weighted}
\end{align}

% Low-Rank Matrix Approximations
\subsubsection{Low-Rank Matrix Approximations}
We next consider the problem of producing a low-rank approximation to a postive semi-definite matrix $\kerMat \in \R^{\Ndesign \times \Ndesign}$. 
We know that the optimal rank-$R$ approximation in the 
$L_2$ sense is given by the truncated eigendecomposition 
\begin{align}
\kerMat \approx \bU_{R} \Sig_R \bU_R^\top \label{eigendecomposition}
\end{align}
where $\bU_R$ is the matrix consisting of the first $R$ columns of $\bU$ and $\Sig_R$ is the upper-left $R \times R$ sub-matrix of $\Sig$. However, computing the 
eigendecomposition requires $O(\Ndesign^3)$ operations, which is computationally intractable when $\Ndesign$ is large. Therefore, the task here is to provide an 
approximation to \ref{eigendecomposition} at lower computational cost. 

In the two previous sections, the Nystrom method has provided approximations to the eigenvalues and eigenfunctions of a kernel $\GPKer(\cdot, \cdot)$ through a discretization 
based on a finite number of sample points $\{\bx_{\designIdx}\}_{\designIdx = 1}^{\Ndesign}$. On the other hand, the problem of producing a low-rank approximation to $\kerMat$ is already 
a finite-dimensional problem, but we can adopt a similar approach. By analogy to the sample points, the Nystrom approximation in this setting samples a subset of the columns of $\kerMat$. 
While much work has gone into smart sampling schemes, here we assume the sample of $\NpseudoData < \Ndesign$ columns $\bC \in \R^{\Ndesign \times \NpseudoData}$ has already been 
selected. Moreover, let $\bW \in \R^{\NpseudoData \times \NpseudoData}$ denote the first $\NpseudoData$ rows of $\bC$. Without loss of generality, assume the 
rows and columns of $\kerMat$ are arranged so that 
\begin{align}
\kerMat &= \begin{pmatrix} \bW & \kerMat[12] \\ \kerMat[12]^\top & \kerMat[22] \end{pmatrix}, &&\bC = \begin{pmatrix} \bW \\ \kerMat[12]^\top \end{pmatrix}
\end{align}
Since $\kerMat$ is PSD, then so much be the sub-matrix $\bW$. The exact eigendecomposition of interest here is 
\begin{align}
\kerMat \bU = \Sig \bU
\end{align}
which is the analog to the exact integral equation of interest \ref{integral_eqn} discussed above. Just as in that case, we produce an approximation by only operating at the 
sample points, which in this case correspond to $\bW$. Assuming we want a rank $R < \NpseudoData < \Ndesign$ approximation to $\kerMat$, we consider the optimal rank $R$
approximation to $\bW$ provided by the truncated eigendecomposition 
\begin{align}
\bW_R &:= \bU_{\bW, R} \Sig_{\bW, R} \bU_{\bW, R}^\top 
\end{align}
Now we apply equations \ref{Nystrom_eigs_weighted} and \ref{Nystrom_estimate_weighted}. Applying the former yields the approximate eigenvalues and the approximate eigenvectors 
only at the sampling locations (not the desired complete approximate eigenvectors for $\kerMat$). 
\begin{align}
&\hat{\Sig} = \frac{\Ndesign}{\NpseudoData} \Sig_{\bW, R}  &&\hat{\bU}_{\bW, R} = \sqrt{\NpseudoData} \bU_{\bW, R} \label{Nystrom_eigs_weighted}
\end{align}
(TODO: check the scaling on the second one). 


\bigskip
\textbf{TODOs:}
\begin{enumerate}
\item I may want to change notation in the integral section to use M instead of N for the number of sampling points. 
\item Need to change notation so that it is clear which eigenproblem is being solved.  
\item Follow \cite{Williams} for the derivation of the low-rank matrix approximations; I think the trick is to view the problem as a special case of the integral equations. 
\item The wikipedia page \href{https://en.wikipedia.org/wiki/Low-rank_matrix_approximations is also good.}{Low rank approx}
\item \href{https://stats.stackexchange.com/questions/261149/nystroem-method-for-kernel-approximation}{Nice post}; add this as an alternative derivation. 
\item First consider case that $\kerMat$ is actually rank $R$. 
\end{enumerate}


% Local Approximate GPs
\section{Local Approximate GPs}

% Bibliography
\begin{thebibliography}{20}
\bibitem{Sun} Sun, Shiliang \& Zhao, Jing \& Zhu, Jiang. (2015). A Review of Nyström Methods for Large-Scale Machine Learning. Information Fusion. 26. 10.1016/j.inffus.2015.03.001. 
\bibitem{Williams} Christopher K. I. Williams and Matthias Seeger. 2000. Using the Nyström method to speed up kernel machines. In Proceedings of the 13th International Conference on Neural Information Processing Systems (NIPS'00). MIT Press, Cambridge, MA, USA, 661–667.
\bibitem{Drineas} Petros Drineas and Michael W. Mahoney. 2005. On the Nyström Method for Approximating a Gram Matrix for Improved Kernel-Based Learning. J. Mach. Learn. Res. 6 (12/1/2005), 2153–2175.
\bibitem{Schölkopf} B. Schölkopf, A. Smola and K. Müller, "Nonlinear Component Analysis as a Kernel Eigenvalue Problem," in Neural Computation, vol. 10, no. 5, pp. 1299-1319, 1 July 1998, doi: 10.1162/089976698300017467.
\bibitem{Schwaighofer} Anton Schwaighofer and Volker Tresp. 2002. Transductive and inductive methods for approximate Gaussian process regression. In Proceedings of the 15th International Conference on Neural Information Processing Systems (NIPS'02). MIT Press, Cambridge, MA, USA, 977–984.
\bibitem{Snelson} Snelson, Edward and Ghahramani, Zoubin. Sparse Gaussian Processes using Pseudo-inputs. Advances in Neural Information Processing Systems. % This is essentially coresets for GPs. 

\end{thebibliography}

\end{document}


