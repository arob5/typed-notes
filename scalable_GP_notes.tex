\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{local-defs-GPs}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

\begin{document}

\begin{center}
\Large
Notes on Scalable Gaussian Processes
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Overview of Approximate GPs
\section{Overview of Approximate GPs}

% Pseudo Data
\subsection{Pseudo Data}
The use of pseudo data is a feature utilized in many different approximate GP schemes, including both the approximate generative model and approximate inference approaches. Hence, we choose to introduce 
this concept here rather than with a specific scalable GP method.

The primary computational challenge of working with GPs is performing operations on the dense $N \times N$ kernel matrix $K$, where $N$ is the number of training observations. When thinking about 
reducing the computational cost, a pretty natural thought is that maybe we can capture the essence of the training data with fewer than $N$ points. The idea is to introduce 
$M << N$ so-called pseudo data points (which may or may not be a subset of the original training dataset) and utilize these points in such a way that hopefully the computation time now scales as some function 
of $M$, rather than as $N^3$. When $M$ is much smaller than $N$ this can result in large computational gains, at the cost of some approximation error.

% Background
\section{Background}

% Factor Graphs
\subsection{Factor Graphs} 
It is useful to have visual representation of GP models, which clearly displays the conditional dependencies between all of the variables. \textit{Factor graphs} are such a tool. To begin, consider a density $p$
that can be factored as 
\[p(x_1, x_2, x_3) = p_1(x_1, x_2)p_2(x_2, x_3)\]
for some other densities $p_1$ and $p_2$. If we consider $x_2$ as fixed, then we see that $p(x_1, x_2, x_3)$ can be interpreted as the conditional density of $x_1$ and $x_3$ given $x_2$, up to a 
normalizing constant. The two terms on the righthand side can be interpreted similarly, so we see that this factorization is telling us that $x_1$ is independent of $x_3$, given $x_2$ (since their conditional 
joint density factors as a product of their marginal conditional densities). This conditional independence can equivalently be expressed as the following factor graph. 
\[\text{\textbf{TODO}: add factor graph}\] 
This context can help us discover a very nice interpretation of the precision matrix (i.e., the inverse covariance matrix) of a multivariate Gaussian distribution; namely, that the precision matrix encodes 
conditional independencies. This is in contrast to the covariance matrix of a Gaussian, which encodes independence between variables, which represents a stronger condition. To see this, consider the 
density for the gaussian $\mathcal{N}(0, \Sigma)$. 
\begin{align*}
\mathcal{N}(x|0, \Sigma) &\propto \exp\left\{-\frac{1}{2}x^T \Sigma^{-1}x \right\} \\
				       &= \exp\left\{-\frac{1}{2} \sum_{i, j} x_i x_j \left(\Sigma^{-1}\right)_{ij} \right\} \\
				       &= \prod_{i,j} \exp\left\{-\frac{1}{2} x_i x_j \left(\Sigma^{-1}\right)_{ij} \right\} \\
				       &\propto \prod_{i,j}  p_{ij}(x_i, x_j)
\end{align*}
Thus, we observe that Gaussian densities factor into a product of densities that each only depend on two of the variables. Notice that if $\left(\Sigma^{-1}\right)_{ij} = 0$ then 
$p_{ij}(x_i, x_j) = 1$ so the factor associated with the variables $i$ and $j$ drops out of the product and we conclude that $x_i$ and $x_j$ are conditionally independent, given all 
the other variables. Let's be a bit more rigorous in proving this result. 

Consider what this looks like for a three-dimensional Gaussian. Notice that the factor for each $i \neq j$ appears twice in the above product, so the density has the form
\[p(x_1, x_2, x_3) = p(x_1) p^2(x_1, x_2)p^2(x_1, x_3)p(x_2) p^2(x_2, x_3)p(x_3)\]
so if, say, $\left(\Sigma^{-1}\right)_{23} = 0$ then this becomes 
\[p(x_1, x_2, x_3) = p(x_1) p^2(x_1, x_2)p^2(x_1, x_3)p(x_2) p(x_3)\]
Now, notice that if we fix $x_1$ then $p(x_1)$ is just a constant, and the $p^2(x_1, x_3)$ is now viewed as a function of only $x_3$ so what's left is a product of terms that depend on 
$x_2$ times a product of terms that depend on $x_3$. Thus, $x_2$ is conditionally independent of $x_3$, given $x_1$. 

% The Nystrom Method
\subsection{The Nystrom Method}

\subsubsection{Solving Integral Equations}
The classical Nystrom method is concerned with numerically solving the integral equation 
\begin{align}
\int_{\inputSpace} \GPKer(\bx, \bx^\prime) \phi_i(\bx) d\bx = \lambda_i \phi_i(\bx^\prime) \label{integral_eqn}
\end{align}
where the $\phi_i$ are the eigenfunctions of the kernel $\GPKer(\cdot, \cdot)$ with associated eigenvalues $\lambda_i$. The method essentially applies 
a quadrature rule to discretize, and thus approximately solve, \ref{integral_eqn}. Indeed, consider a Gauss quadrature rule 
$\{\bx_{\designIdx}, w_{\designIdx}\}_{\designIdx = 1}^{\Ndesign}$ consisting of $\Ndesign$ input points $\bx_{\designIdx}$ with associated weights $w_{\designIdx}$.
Applying this rule to \ref{integral_eqn} yields the approximation 
\begin{align}
\sum_{\designIdx = 1}^{\Ndesign} w_{\designIdx} \GPKer(\bx_{\designIdx}, \bx^\prime) \phi_i(\bx_{\designIdx}) = \lambda_i \phi_i(\bx^\prime) \label{integral_eqn_discretized}
\end{align}
for each $\bx^\prime \in \inputSpace$. We refer to the set $\{\bx_{\designIdx}\}_{\designIdx = 1}^{\Ndesign}$ as the set of \textit{samples} used to form the approximation. 
To discretize the continuum of $\bx^\prime \in \inputSpace$ values, the Nystrom method utilizes the same set of samples. In effect, we evaluate \ref{integral_eqn_discretized}
(as a function of $\bx^\prime$) at each of the samples $\bx_{\designIdx}$. This yields, 
\begin{align}
\sum_{\designIdx = 1}^{\Ndesign} w_{\designIdx}  \GPKer(\bx_{\designIdx}, \bx_m) \phi_i(\bx_{\designIdx}) = \lambda_i \phi_i(\bx_m), \text{ for } m = 1, \dots, \Ndesign \label{Nystrom_discretization}
\end{align}
The original integral equation \ref{integral_eqn} has now been fully discretized, approximated by the finite linear system \ref{Nystrom_discretization}. To write this linear system in 
matrix notation, define $\bu_i := \begin{pmatrix} \phi_i(\bx_1), \dots, \phi_i(\bx_N) \end{pmatrix}^{\top}$, $\bW := \text{diag}\left\{w_1, \dots, w_{\Ndesign} \right\}$, and 
$\kerMat$ the $\Ndesign \times \Ndesign$ matrix with entries $\kerMat[\designIdx, m] = \GPKer(\bx_{\designIdx}, \bx_m)$. We can now re-write \ref{Nystrom_discretization} as 
\begin{align}
\kerMat \bW \bu_i = \lambda_i \bu_i \label{Nystrom_eigenvalue_prob}
\end{align}
We observe that \ref{Nystrom_eigenvalue_prob} is a familiar eigenvalue problem from linear algebra. However, $\kerMat \bW$ is in general not symmetric, which renders the 
eigenvalue problem more difficult than the symmetric case. However, it is easy to symmetrize the problem by defining left-multiplying by $\bW^{1/2}$,
\[\bW^{1/2} \kerMat \bW \bu_i = \bW^{1/2} \kerMat \bW^{1/2} \bW^{1/2} \bu_i= \lambda_i \bW^{1/2} \bu_i\]
and defining $\tilde{\kerMat} := \bW^{1/2} \kerMat \bW^{1/2}$ (which is symmetric) and $\bv_i := \bW^{1/2} \bu_i$. This yields the equivalent symmetric eigenvalue problem 
\begin{align}
\tilde{\kerMat} \bv_i = \lambda_i \bv_i
\end{align}

Since $\tilde{\kerMat}$ is symmetric positive-definite, solving this eigenvalue problem yields an $\Ndesign$-dimensional orthonormal basis of 
eigenvectors $\bV$ (with columns $\bv_i$) and positive eigenvalues $\hat{\lambda}_1, \dots, \hat{\lambda}_{\Ndesign}$, sorted in descending 
order. That is, we have $\tilde{\kerMat} = \bV \Sig \bV^T$, where $\Sig := \text{diag}\left\{\hat{\lambda}_1, \dots, \hat{\lambda}_{\Ndesign} \right\}$. 
To obtain the eigenvectors of interest $\bu_i$, we can simply map back via $\bu_i = \bW^{-1/2}\bv_i$; i.e., $\bU = \bW^{-1/2}\bV$. Now, 
the $j^{\text{th}}$ entry of $\bu_i$ is our approximation of the $i^{\text{th}}$ eigenfunction evaluated at the $j^{\text{th}}$ quadrature
point (i.e. $\phi_i(\bx_j)$). Moreover, $\hat{\lambda}_i$ is our approximation to $\lambda_i$. But what if we want an approximation to 
$\phi_i(\bx^\prime)$, where $\bx^\prime \in \inputSpace$ is not one of the sample points? The Nystrom approximation provides us the formula 
\ref{integral_eqn_discretized}, which can readily be used to approximate $\phi_i(\bx^\prime)$. Indeed, dividing both sides by $\lambda_i$ and then substituting in the 
approximations $\hat{\lambda}_i \approx \lambda_i$ and $ \bU_{\designIdx i} \approx \phi_i(\bx_\designIdx)$, we obtain the estimate
\begin{align}
\phi_i(\bx^\prime) \approx \frac{1}{\hat{\lambda}_i } \sum_{\designIdx = 1}^{\Ndesign} w_{\designIdx} \GPKer(\bx_{\designIdx}, \bx^\prime)  \bU_{\designIdx i} \label{Nystrom_estimate}
\end{align}
Note that \ref{Nystrom_estimate} again utilizes an approximation based on the sampling points $\{\bx_{\designIdx}\}$. 

We make one final note on the baseline Nystrom method before proceeding to generalizations. When solving the equation \ref{integral_eqn}, we often also seek to enforce the 
orthogonality constraint 
\begin{align}
\int_{\inputSpace} \phi_i(\bx) \phi_j(\bx) d\bx = \delta_{ij} \label{orthogonality_constraint}
\end{align}
Has this discretization procedure ensured a good approximation of this condition? Indeed, discretizing the integral \ref{orthogonality_constraint} (using the 
same quadrature rule as before) we see that
\begin{align*}
\sum_{\designIdx = 1}^{\Ndesign} w_{\designIdx} \phi_i(\bx_{\designIdx}) \phi_j(\bx_{\designIdx}) &\approx \sum_{\designIdx = 1}^{\Ndesign} \sqrt{w_{\designIdx}} \bU_{\designIdx i}\cdot \sqrt{w_k} \bU_{\designIdx j}\\
								   &=  \sum_{\designIdx = 1}^{\Ndesign} \bV_{\designIdx i} \bV_{\designIdx j} \\
								   &= \bv_i^{\top} \bv_j
\end{align*}
In words, the discretization tells us that to approximate the orthogonality of the $\phi_i$ we should impose orthogonality on the $\bv_i$. This
is convenient as standard software packages will return the $\bv_i$ already normalized, and then we do not need to worry about scaling the 
$\bu_i$ after executing the mapping $\bu_i = \bW^{-1/2}\bv_i$.   




% Local Approximate GPs
\section{Local Approximate GPs}


\end{document}


