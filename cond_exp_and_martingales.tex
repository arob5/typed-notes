\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Conditional Expectation and Martingales}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Defining Conditional Expectation}
Here we slowly build up to the general definition of conditional expectation in order to gain intuition. 

\subsection{A Simple Example}

\subsubsection{Constructing the Probability Space for a Two-Coin Flip Experiment}
We begin with the simple example of independently flipping two coins, with respective probabilities $p$ and $q$ of landing heads. Coding $0$ as tails and $1$ as heads, the probability space associated with 
this experiment is
\[\Omega = \{(0, 0), (0, 1), (1, 0), (1, 1)\}\]
The typical $\sigma$-algebra to consider here is the full power set 
\[\mathcal{F} = 2^{\Omega} = \{\text{All subsets of } \Omega\}\]
Writing out all of the subsets would be inconvenient, but note that there are $2^4 = 16$ sets in the collection $2^{\Omega}$. Now that we have the measurable space $(\Omega, \mathcal{F})$, we can 
define a probability measure $\mathbb{P}: \mathcal{F} \to [0, 1]$, which assigns weights to each set in the $\sigma$-algebra. The probability measure corresponding to the two-coin experiment is 
\begin{align*}
\Prob(\{(1, 1)\}) &= pq \\
\Prob(\{(0, 0)\}) &= (1 - p)(1 - q) \\
\Prob(\{(0, 1)\}) &= (1 - p)q \\
\Prob(\{(1, 0)\}) &= p(1 - q)
\end{align*}
This follows from the fact that any sequence of two independent flips has probability equal to the product of the probabilities of the individual flips. 
For example, the probability of flipping two heads in a row should be the product of each independently turning up heads. 
In this case, defining $\Prob(\{\omega\})$ for all singleton sets $\{\omega\} \in \mathcal{F}$ is sufficient to define $\Prob$ on all of $\mathcal{F}$, since the probabilities of disjoint events must add. 
For example, consider 
\[A = \{(0, 0), (0, 1)\} = \{\text{First flip is tails and second flip is heads or tails}\}\]
The probability of $A$ is then sum of the probabilities of the disjoint singleton sets $\{(0, 0)\}$ and 
$\{0, 1\}$. 
\[\Prob(A) = \Prob(\{\omega \in \Omega: \omega \in \{(0, 0), (0, 1)\}\}) = \Prob(\{(0, 0)\}) + \Prob(\{(0, 1)\}) = (1 - p)(1 - q) + (1 - p)q = 1 - p\] 
We have thus constructed a valid probability space $(\Omega, \mathcal{F}, \Prob)$. 

\subsubsection{A Smaller $\sigma$-algebra}
Although the power set $\sigma$-algebra is the natural choice here, for the purposes of illustration let's consider an alternative 
$\sigma$-algebra. Since $\sigma$-algebras are supposed to model \textit{knowledge} or \textit{information}, let's try to define the $\sigma$-algebra $\mathcal{G}$ that models the knowledge of the the total number 
of heads in the experiment. Intuitively, this is less information than is captured by $\mathcal{F}$. For example., the power set $\sigma$-algebra allowed us to make statements about the probability of sets $\{0, 1\}$ and 
$\{1, 0\}$. However, each of these sets have the same number of heads (one), so according to $\mathcal{G}$ we should not be able to distinguish between these two events. $\mathcal{G}$ should allow us to make statements 
about the probability of events like ``the experiment produced one head'' but it should not let us say anything about which coin produced that head. Therefore, the sets that have the same number of heads
\[\{(0, 0)\}, \{(0, 1), (1, 0)\}, \{(1, 1)\}\] 
should be in $\mathcal{G}$. The $\sigma$-algebra can now be completed by considering unions, intersections, and complements of these sets.
\begin{align*}
\mathcal{G} = &\{\{(0, 0)\}, \{(0, 1), (1, 0)\}, \{(1, 1)\}, \\
		       &\{(0, 0), (0, 1), (1, 0)\}, \{(1, 1), (0, 1), (1, 0)\}, \{(0, 0), (1, 1)\} \}
\end{align*}
If should make intuitive sense that these are the sets we end up with; e.g. $\{(0, 0), (0, 1), (1, 0)\}$ is the set corresponding to the event ``the experiment produced zero or one heads''. None of these sets will allow us to make 
probabilistic statements about the specific sequence in which the heads appeared in the coin flips. We can again define a probability measure $\Prob: \mathcal{G} \to [0, 1]$ (note that this is the restriction of the previous 
probability measure to the smaller domain $\mathcal{G}$) on the measurable space $(\Omega, \mathcal{G})$. 
\begin{align*}
\Prob(\{(1, 1)\}) &= pq \\
\Prob(\{(0, 0)\}) &= (1 - p)(1 - q) \\
\Prob(\{(0, 1), (1, 0)\}) &= p(1 - q) + (1 - p)q
\end{align*}
The probability $p(1 - q) + (1 - p)q$ is simply the probability of flipping a single head (either the first is heads and the second is tails, or vice versa). 
The main point about $\mathcal{G}$ is that the sets $\{(0, 1)\}$ and $\{1, 0\}$ are indistinguishable. Notice that the probability measure assigns a probability to $\{(0, 1), (1, 0)\}$ by averaging over the probabilities 
of $\{(0, 1)\}$ and $\{(0, 1)\}$, but it cannot assign probabilities to these latter two sets! 

\subsubsection{Smaller $\sigma$-algebra generated by a random variable}
In the previous section, we stated the goal of defining a $\sigma$-algebra $\mathcal{G}$ that captured knowledge only of the number of heads that resulted from the experiment. We can arrive at the same $\sigma$-algebra by 
considering the random variable $Y: \Omega \to \{0, 1, 2\}$ that returns the number of heads in the experiment; e.g. $Y((1, 1)) = 2$ and $Y(0, 1) = 1$. My claim is that $\mathcal{G}$ is the smallest $\sigma$-algebra to which $Y$
is measurable. Intuitively, this should be the case since being able to make probabilistic statements about the values $Y$ can assume only requires knowledge about how many heads occurred in the experiment. Let's show this more formally. $Y$ can assume three values, and hence we need to consider the inverse image of these three values. 
\begin{align*}
Y^{-1}(0) &= \{(0, 0)\} \\
Y^{-1}(1) &= \{(1, 0), (0, 1)\} \\
Y^{-1}(2) &= \{(1, 1)\}
\end{align*}
So the three sets on the righthand side are the three subsets of $\Omega$ to which we must assign probabilities in order to define a distribution on $Y$. This is the minimal collection of sets with this property, so the minimal 
$\sigma$-algebra follows from taking unions, intersections, and complements of these sets. But this is the same exact thing we did previously to arrive at $\mathcal{G}$! Thus, we have shown that 
\[\mathcal{G} = \sigma(Y)\]
or in words: $\mathcal{G}$ is the $\sigma$-algebra generated by $Y$. This just means that $\mathcal{G}$ is the smallest possible $\sigma$-algebra to which $Y$ is measurable. The smallest $\sigma$-algebra can also be 
thought of as the ``coarsest'' in that it contains less fine-grained information compared to, for example, $\mathcal{F}$. $Y$ is also measurable with respect to $\mathcal{F}$, but $\mathcal{F}$ contains more sets than are needed 
to define a distribution on $Y$. Note also that \textit{any} random variable $X: \mathcal{\Omega} \to \R$ that is measurable with respect to $\mathcal{G}$ must have the property $X((1, 0)) = X((0, 1))$. This is because $\mathcal{G}$
does not contain enough information to differentiate between the two outcomes $(1, 0)$ and $(0, 1)$. 

\subsubsection{Defining conditional expectation with respect to the smaller $\sigma$-algebra}
We are now ready to give a definition of conditional expectation in this simple setting. In this setting and the more general one, conditional expectation is defined as conditional with respect to a $\sigma$-algebra. This makes sense given the fact that $\sigma$-algebra models knowledge, so we seek to define an expectation with respect to knowledge of certain events. Let $X: \Omega \to \R$ be any random variable that is $\mathcal{F}$ measurable. 
The conditional expectation of $X$ given $\mathcal{G}$ will itself be a random variable, let's call it $Z$. Intuitively, if we condition on $\mathcal{G}$ then we are assuming knowledge of only the events in $\mathcal{G}$. Given 
these events, the conditional expectation $Z$ will give the ``best guess'' of $X$ for each event in $\mathcal{G}$. Thus, $Z$ must be $\mathcal{G}$ measurable but does not in general need to be $\mathcal{F}$ measurable. 
We can think of $Z(\omega)$ in the following way: 
\begin{itemize}
\item The random experiment produces an outcome $\omega \in \Omega$. 
\item We only have knowledge of which set in $\mathcal{G}$ that $\omega$ falls into. Suppose $\omega \in G$ for some $G \in \mathcal{G}$. 
\item $Z(\omega)$ averages over the $\mathcal{F}$-information in $G$ to give the best guess of $X$. 
\end{itemize}
To make this more clear, we first present an example. After I will give the definition of the conditional expectation of $X$ given $\mathcal{G}$ in this setting. For now, consider 
$X = \mathbbm{1}\{\text{Second flip is heads}\}$. So we want to condition the random variable representing knowledge that the second flip is heads on knowledge of the total number of heads.
If we know there are zero heads, then our best guess (and the correct one) at the value of $X$ is $0$. Similarly, if we know there are two heads then we can correctly infer that $X = 1$. However, 
if we know there is a single head, then we are not able to pin down the exact value of $X$. So instead $Z$ averages over the two possibilities: that either the first flip or the second flip contributed the 
single head. Thus, we define $Z(\omega) := $
\begin{align*}
&0, &&\text{ if } \omega \in \{(0, 0)\} \\
&1, &&\text{ if } \omega \in \{(1, 1)\} \\
&\frac{(1 - p)q}{(1 - p)q + p(1 - q)} \cdot 0 + \frac{p(1 - q)}{(1 - p)q + p(1 - q)} \cdot 1, &&\text{ if } \omega \in \{(1, 0), (0, 1)\}
\end{align*}
The third entry looks a bit complicated, but it is simply a weighted average of the values $X$ can assume on the set $\{(1, 0), (0, 1)\}$. We have to re-weight the probabilities to sum to $1$, as is usual when defining 
conditional probability. Note that in the case where both coins are fair $p = q = \frac{1}{2}$ then each of the weights in the above expression reduce to $\frac{1}{2}$. 

Now that I have made the point that the conditional expectation is a random variable, I will replace $Z(\omega)$ the more common notation $\E[X|\mathcal{G}](\omega)$. This latter notation is clearly must better, 
but to those accustomed to undergraduate-level probability, it may initially be weird to think of something denoted $\E[X|\mathcal{G}]$ as a random variable. 

We now consider generalizing the definition of $\E[X|\mathcal{G}]: \Omega \to \R$ to any $\mathcal{F}$-measurable random variable $X$. We define $\E[X|\mathcal{G}](\omega) := $
\begin{align*}
&X(0, 0), &&\text{ if } \omega \in \{(0, 0)\} \\
&X(1, 1), &&\text{ if } \omega \in \{(1, 1)\} \\
&\frac{(1 - p)q}{(1 - p)q + p(1 - q)} \cdot X(0, 1) + \frac{p(1 - q)}{(1 - p)q + p(1 - q)} \cdot X(1, 0), &&\text{ if } \omega \in \{(1, 0), (0, 1)\}
\end{align*}
Notice that the weighted sum can also be written as 
\[\frac{(1 - p)q}{(1 - p)q + p(1 - q)} \cdot X(0, 1) + \frac{p(1 - q)}{(1 - p)q + p(1 - q)} \cdot X(1, 0) = \frac{\E[X \mathbbm{1}_G]}{\Prob(G)}\]
where $G := \{(1, 0), (0, 1)\}$, as can be verified by considering the calculations for the expectation and probability in the expression. Thus, in general we can think of the conditional expectation as averaging 
over the set $G \in \mathcal{G}$, re-weighting as necessary by its probability $\Prob(G)$. This expression will be utilized when we consider the next generalization of the conditional expectation definition, so 
we give it a name
\[\E[X|G] := \frac{\E[X \mathbbm{1}_G]}{\Prob(G)},\]
Note that although the notation is similar, $G$ is a set, so this is very different from conditioning on a $\sigma$-algebra as in $\E[X|\mathcal{G}]$. 

\subsubsection{What does it mean to condition on a random variable?}
We have given a rigorous (though currently very limited) definition of conditional expectation with respect to a $\sigma$-algebra $\mathcal{G}$. How does this relate to the usual convention of writing conditional 
expectations with respect to random variables? For example, consider $Y$, the number of heads in the experiment. What does $\E[X|Y]$ mean? When we use this notation, the true rigorous meaning underlying it 
is 
\[\E[X|Y] := \E[X|\sigma(Y)],\]
namely, that we condition with respect to the $\sigma$-algebra generated by the random variable. In the case of this example, we know that $\mathcal{G} = \sigma(Y)$ so $\E[X|Y]$ is in fact the conditional expectation 
we have been working with throughout this example!

Note that in undergraduate-level probability we often think of the conditional expectation as a number $\E[X|Y = y]$. Given a different $y$ this will be a different number, but it  is still just a number. By instead defining the 
random variable $\E[X|Y]$ we have essentially have a formula as a function of $Y$ that outputs the same numbers once the experiment occurs. This simple change in thinking allows the conditional expectation to be a much 
more powerful concept. 

\subsection{A More General Definition}
In the previous section, we considered defining the conditional expectation $\E[X|\mathcal{G}]$ in a simple case where the $\sigma$-algebra $\mathcal{G}$ was generated by a finite collection of sets. Naturally, the next 
generalization will be the case where $\mathcal{G}$ is generated by a countable collection of sets. In particular, assume that $\{G_i\}_{i = 1}^{\infty}$ form a partition of the sample space $\Omega$; i.e. they are 
pairwise disjoint and their union is $\Omega$. We suppose $\mathcal{G} = \sigma\left(\{G_i\}_{i = 1}^{\infty}\right)$. Recalling the notation 
\[\E[X|G] := \frac{\E[X\mathbbm{1}_G]}{\Prob(G)}, \text{ for } G \in \mathcal{G}\]
we simply define $\E[X|\mathcal{G}]: \Omega \to \R$ by $\E[X|\mathcal{G}](\omega) := \E[X|G_i]$ if $\omega \in G_i$. This is essentially the same definition as in the previous case, we now simply have a countable 
number of $G_i$. Once again, if all we know is that $\omega \in \mathcal{G}$ then the idea is to average $X$ over $G_i$ to produce a best guess for $X$, re-weighting by $\Prob(G_i)$ as is necessary.

\subsubsection{An Example: Infinite Coin Flips}
\textbf{TODO}: this section is incomplete.
As an example of this generalization, consider the experiment that consists of an infinite number of coin tosses. The probability space $\Omega$ consists of all sequences $\omega: \mathbb{N} \to \{0, 1\}$ of the form 
\[\omega = (\omega_1, \omega_2, \omega_3, \dots), \text{ where } \omega_i \in \{0, 1\}, \text{ for all } i,\]
that is, all infinite sequences of zeros and ones. Constructing a $\sigma$-algebra that allows us to make probabilistic statements about all of the coin tosses is is a bit tricky. For example, we might want to be able to talk about 
the probability that every coin flip is heads. However, it is not hard to construct smaller $\sigma$-algebras that allow us to make statements about finitely many of the coin tosses. Intuitively, we want to define a $\sigma$-algebra
like 
\[\mathcal{F}_n = \{\text{Knowledge of the first $n$ coin tosses}\}\]
for each $n \in \mathbb{N}$. This $\sigma$-algebra should contain all events that are determined entirely by the first $n$ coin tosses. Formally, we can describe the sets in $\mathcal{F}_n$ by:
\[A \in \mathcal{F}_n \iff \exists A_n \subset \{0, 1\}^n \text{ such that } A = \{\omega \in \Omega: (\omega_1, \dots, \omega_n) \in A_n\}\] 
As we increase $n$ then $\mathcal{F}_n$ will contain knowledge of more coin tosses, and should therefore be a larger $\sigma$-algebra. Indeed, it is not difficult to show that 
\[\mathcal{F}_n \subset \mathcal{F}_{n + 1}\]
A natural candidate for a larger $\sigma$-algebra $\mathcal{F}_0$ is the collection resulting from tossing all of the sets in each $\mathcal{F}_n$ into one large collection; that is, 
\[\mathcal{F}_0 := \bigcup_{n \in \mathbb{N}} \mathcal{F}_n\]
Unfortunately, it turns out that $\mathcal{F}_0$ is not actually a $\sigma$-algebra; it is missing some sets that should be included. Thus, the next natural step is to consider the smallest $\sigma$-algebra containing $\mathcal{F}_0$. 
\[\mathcal{F} := \sigma(\mathcal{F}_0)\]
Given the measurable space $(\Omega, \mathcal{F})$ it is now possible to define a probability measure $\mathcal{P}$ that models the concept of ``fair'' coin tosses. I will not give specifics on this but see 
\href{https://www.ee.iitm.ac.in/~krishnaj/EE5110_files/notes/lecture8_Infinite\%20Coin\%20Toss.pdf}{here} for details. 

\subsubsection{Next Steps}
In the interest of taking the next step in generalizing to arbitrary $\sigma$-algebras, not necessarily generated by a countable collection of sets, let's consider the salient property of our current definition of conditional expectation 
that makes it a reasonable definition. I will again denote the conditional expectation as $Z = \E[X|\mathcal{G}]$ as I believe it makes the following explanation easier to understand. Recall that the conditional expectation $Z$ 
takes the knowledge that an outcome $\omega$ is in some $G_i \in \mathcal{G}$ and then produces a guess for the value of $X$ by averaging over the finer-grain $\mathcal{F}$-information in $G_i$ (since this information is not 
available; all that is known is that $\omega \in G_i$). The key thing to note here is that this ``averaging over'' is done with respect to $\mathcal{F}$. Recall the definition 
\[Z(\omega) = \frac{\E[X \mathbbm{1}_{G_i}]}{\Prob(G_i)}, \text{ if } \omega \in G_i\]
The expectation in the numerator is defined with respect to $\mathcal{F}$, since $X$ is $\mathcal{F}$-measurable. 

\subsection{General Definition of Conditional Expectation}

\end{document}


