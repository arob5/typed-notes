\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Conditional Expectation and Martingales}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Defining Conditional Expectation}
Here we slowly build up to the general definition of conditional expectation in order to gain intuition. 

\subsection{A Simple Example}

\subsubsection{Constructing the Probability Space for a Two-Coin Flip Experiment}
We begin with the simple example of independently flipping two coins, with respective probabilities $p$ and $q$ of landing heads. Coding $0$ as tails and $1$ as heads, the probability space associated with 
this experiment is
\[\Omega = \{(0, 0), (0, 1), (1, 0), (1, 1)\}\]
The typical $\sigma$-algebra to consider here is the full power set 
\[\mathcal{F} = 2^{\Omega} = \{\text{All subsets of } \Omega\}\]
Writing out all of the subsets would be inconvenient, but note that there are $2^4 = 16$ sets in the collection $2^{\Omega}$. Now that we have the measurable space $(\Omega, \mathcal{F})$, we can 
define a probability measure $\mathbb{P}: \mathcal{F} \to [0, 1]$, which assigns weights to each set in the $\sigma$-algebra. The probability measure corresponding to the two-coin experiment is 
\begin{align*}
\Prob(\{(1, 1)\}) &= pq \\
\Prob(\{(0, 0)\}) &= (1 - p)(1 - q) \\
\Prob(\{(0, 1)\}) &= (1 - p)q \\
\Prob(\{(1, 0)\}) &= p(1 - q)
\end{align*}
This follows from the fact that any sequence of two independent flips has probability equal to the product of the probabilities of the individual flips. 
For example, the probability of flipping two heads in a row should be the product of each independently turning up heads. 
In this case, defining $\Prob(\{\omega\})$ for all singleton sets $\{\omega\} \in \mathcal{F}$ is sufficient to define $\Prob$ on all of $\mathcal{F}$, since the probabilities of disjoint events must add. 
For example, consider 
\[A = \{(0, 0), (0, 1)\} = \{\text{First flip is tails and second flip is heads or tails}\}\]
The probability of $A$ is then sum of the probabilities of the disjoint singleton sets $\{(0, 0)\}$ and 
$\{0, 1\}$. 
\[\Prob(A) = \Prob(\{\omega \in \Omega: \omega \in \{(0, 0), (0, 1)\}\}) = \Prob(\{(0, 0)\}) + \Prob(\{(0, 1)\}) = (1 - p)(1 - q) + (1 - p)q = 1 - p\] 
We have thus constructed a valid probability space $(\Omega, \mathcal{F}, \Prob)$. 

\subsubsection{A Smaller $\sigma$-algebra}
Although the power set $\sigma$-algebra is the natural choice here, for the purposes of illustration let's consider an alternative 
$\sigma$-algebra. Since $\sigma$-algebras are supposed to model \textit{knowledge} or \textit{information}, let's try to define the $\sigma$-algebra $\mathcal{G}$ that models the knowledge of the the total number 
of heads in the experiment. Intuitively, this is less information than is captured by $\mathcal{F}$. For example., the power set $\sigma$-algebra allowed us to make statements about the probability of sets $\{0, 1\}$ and 
$\{1, 0\}$. However, each of these sets have the same number of heads (one), so according to $\mathcal{G}$ we should not be able to distinguish between these two events. $\mathcal{G}$ should allow us to make statements 
about the probability of events like ``the experiment produced one head'' but it should not let us say anything about which coin produced that head. Therefore, the sets that have the same number of heads
\[\{(0, 0)\}, \{(0, 1), (1, 0)\}, \{(1, 1)\}\] 
should be in $\mathcal{G}$. The $\sigma$-algebra can now be completed by considering unions, intersections, and complements of these sets.
\begin{align*}
\mathcal{G} = &\{\{(0, 0)\}, \{(0, 1), (1, 0)\}, \{(1, 1)\}, \\
		       &\{(0, 0), (0, 1), (1, 0)\}, \{(1, 1), (0, 1), (1, 0)\}, \{(0, 0), (1, 1)\} \}
\end{align*}
If should make intuitive sense that these are the sets we end up with; e.g. $\{(0, 0), (0, 1), (1, 0)\}$ is the set corresponding to the event ``the experiment produced zero or one heads''. None of these sets will allow us to make 
probabilistic statements about the specific sequence in which the heads appeared in the coin flips. We can again define a probability measure $\Prob: \mathcal{G} \to [0, 1]$ (note that this is the restriction of the previous 
probability measure to the smaller domain $\mathcal{G}$) on the measurable space $(\Omega, \mathcal{G})$. 
\begin{align*}
\Prob(\{(1, 1)\}) &= pq \\
\Prob(\{(0, 0)\}) &= (1 - p)(1 - q) \\
\Prob(\{(0, 1), (1, 0)\}) &= p(1 - q) + (1 - p)q
\end{align*}
The probability $p(1 - q) + (1 - p)q$ is simply the probability of flipping a single head (either the first is heads and the second is tails, or vice versa). 
The main point about $\mathcal{G}$ is that the sets $\{(0, 1)\}$ and $\{1, 0\}$ are indistinguishable. Notice that the probability measure assigns a probability to $\{(0, 1), (1, 0)\}$ by averaging over the probabilities 
of $\{(0, 1)\}$ and $\{(0, 1)\}$, but it cannot assign probabilities to these latter two sets! 

\subsubsection{Smaller $\sigma$-algebra generated by a random variable}
In the previous section, we stated the goal of defining a $\sigma$-algebra $\mathcal{G}$ that captured knowledge only of the number of heads that resulted from the experiment. We can arrive at the same $\sigma$-algebra by 
considering the random variable $Y: \Omega \to \{0, 1, 2\}$ that returns the number of heads in the experiment; e.g. $Y((1, 1)) = 2$ and $Y(0, 1) = 1$. My claim is that $\mathcal{G}$ is the smallest $\sigma$-algebra to which $Y$
is measurable. Intuitively, this should be the case since being able to make probabilistic statements about the values $Y$ can assume only requires knowledge about how many heads occurred in the experiment. Let's show this more formally. $Y$ can assume three values, and hence we need to consider the inverse image of these three values. 
\begin{align*}
Y^{-1}(0) &= \{(0, 0)\} \\
Y^{-1}(1) &= \{(1, 0), (0, 1)\} \\
Y^{-1}(2) &= \{(1, 1)\}
\end{align*}
So the three sets on the righthand side are the three subsets of $\Omega$ to which we must assign probabilities in order to define a distribution on $Y$. This is the minimal collection of sets with this property, so the minimal 
$\sigma$-algebra follows from taking unions, intersections, and complements of these sets. But this is the same exact thing we did previously to arrive at $\mathcal{G}$! Thus, we have shown that 
\[\mathcal{G} = \sigma(Y)\]
or in words: $\mathcal{G}$ is the $\sigma$-algebra generated by $Y$. This just means that $\mathcal{G}$ is the smallest possible $\sigma$-algebra to which $Y$ is measurable. The smallest $\sigma$-algebra can also be 
thought of as the ``coarsest'' in that it contains less fine-grained information compared to, for example, $\mathcal{F}$. $Y$ is also measurable with respect to $\mathcal{F}$, but $\mathcal{F}$ contains more sets than are needed 
to define a distribution on $Y$. Note also that \textit{any} random variable $X: \mathcal{\Omega} \to \R$ that is measurable with respect to $\mathcal{G}$ must have the property $X((1, 0)) = X((0, 1))$. This is because $\mathcal{G}$
does not contain enough information to differentiate between the two outcomes $(1, 0)$ and $(0, 1)$. 

\subsubsection{Defining conditional expectation with respect to the smaller $\sigma$-algebra}
We are now ready to give a definition of conditional expectation in this simple setting. In this setting and the more general one, conditional expectation is defined as conditional with respect to a $\sigma$-algebra. This makes sense given the fact that $\sigma$-algebra models knowledge, so we seek to define an expectation with respect to knowledge of certain events. Let $X: \Omega \to \R$ be any random variable that is $\mathcal{F}$ measurable. 
The conditional expectation of $X$ given $\mathcal{G}$ will itself be a random variable, let's call it $Z$. Intuitively, if we condition on $\mathcal{G}$ then we are assuming knowledge of only the events in $\mathcal{G}$. Given 
these events, the conditional expectation $Z$ will give the ``best guess'' of $X$ for each event in $\mathcal{G}$. Thus, $Z$ must be $\mathcal{G}$ measurable but does not in general need to be $\mathcal{F}$ measurable. 
We can think of $Z(\omega)$ in the following way: 
\begin{itemize}
\item The random experiment produces an outcome $\omega \in \Omega$. 
\item We only have knowledge of which set in $\mathcal{G}$ that $\omega$ falls into. Suppose $\omega \in G$ for some $G \in \mathcal{G}$. 
\item $Z(\omega)$ averages over the $\mathcal{F}$-information in $G$ to give the best guess of $X$. 
\end{itemize}
To make this more clear, we first present an example. After I will give the definition of the conditional expectation of $X$ given $\mathcal{G}$ in this setting. For now, consider 
$X = \mathbbm{1}\{\text{Second flip is heads}\}$. So we want to condition the random variable representing knowledge that the second flip is heads on knowledge of the total number of heads.
If we know there are zero heads, then our best guess (and the correct one) at the value of $X$ is $0$. Similarly, if we know there are two heads then we can correctly infer that $X = 1$. However, 
if we know there is a single head, then we are not able to pin down the exact value of $X$. So instead $Z$ averages over the two possibilities: that either the first flip or the second flip contributed the 
single head. Thus, we define $Z(\omega) := $
\begin{align*}
&0, &&\text{ if } \omega \in \{(0, 0)\} \\
&1, &&\text{ if } \omega \in \{(1, 1)\} \\
&\frac{(1 - p)q}{(1 - p)q + p(1 - q)} \cdot 0 + \frac{p(1 - q)}{(1 - p)q + p(1 - q)} \cdot 1, &&\text{ if } \omega \in \{(1, 0), (0, 1)\}
\end{align*}
The third entry looks a bit complicated, but it is simply a weighted average of the values $X$ can assume on the set $\{(1, 0), (0, 1)\}$. We have to re-weight the probabilities to sum to $1$, as is usual when defining 
conditional probability. Note that in the case where both coins are fair $p = q = \frac{1}{2}$ then each of the weights in the above expression reduce to $\frac{1}{2}$. 

Now that I have made the point that the conditional expectation is a random variable, I will replace $Z(\omega)$ the more common notation $\E[X|\mathcal{G}](\omega)$. This latter notation is clearly must better, 
but to those accustomed to undergraduate-level probability, it may initially be weird to think of something denoted $\E[X|\mathcal{G}]$ as a random variable. 

We now consider generalizing the definition of $\E[X|\mathcal{G}]: \Omega \to \R$ to any $\mathcal{F}$-measurable random variable $X$. We define $\E[X|\mathcal{G}](\omega) := $
\begin{align*}
&X(0, 0), &&\text{ if } \omega \in \{(0, 0)\} \\
&X(1, 1), &&\text{ if } \omega \in \{(1, 1)\} \\
&\frac{(1 - p)q}{(1 - p)q + p(1 - q)} \cdot X(0, 1) + \frac{p(1 - q)}{(1 - p)q + p(1 - q)} \cdot X(1, 0), &&\text{ if } \omega \in \{(1, 0), (0, 1)\}
\end{align*}
Notice that the weighted sum can also be written as 
\[\frac{(1 - p)q}{(1 - p)q + p(1 - q)} \cdot X(0, 1) + \frac{p(1 - q)}{(1 - p)q + p(1 - q)} \cdot X(1, 0) = \frac{\E[X \mathbbm{1}_G]}{\Prob(G)}\]
where $G := \{(1, 0), (0, 1)\}$, as can be verified by considering the calculations for the expectation and probability in the expression. Thus, in general we can think of the conditional expectation as averaging 
over the set $G \in \mathcal{G}$, re-weighting as necessary by its probability $\Prob(G)$. This expression will be utilized when we consider the next generalization of the conditional expectation definition, so 
we give it a name
\[\E[X|G] := \frac{\E[X \mathbbm{1}_G]}{\Prob(G)},\]
Note that although the notation is similar, $G$ is a set, so this is very different from conditioning on a $\sigma$-algebra as in $\E[X|\mathcal{G}]$. 

\subsubsection{What does it mean to condition on a random variable?}
We have given a rigorous (though currently very limited) definition of conditional expectation with respect to a $\sigma$-algebra $\mathcal{G}$. How does this relate to the usual convention of writing conditional 
expectations with respect to random variables? For example, consider $Y$, the number of heads in the experiment. What does $\E[X|Y]$ mean? When we use this notation, the true rigorous meaning underlying it 
is 
\[\E[X|Y] := \E[X|\sigma(Y)],\]
namely, that we condition with respect to the $\sigma$-algebra generated by the random variable. In the case of this example, we know that $\mathcal{G} = \sigma(Y)$ so $\E[X|Y]$ is in fact the conditional expectation 
we have been working with throughout this example!

Note that in undergraduate-level probability we often think of the conditional expectation as a number $\E[X|Y = y]$. Given a different $y$ this will be a different number, but it  is still just a number. By instead defining the 
random variable $\E[X|Y]$ we have essentially have a formula as a function of $Y$ that outputs the same numbers once the experiment occurs. This simple change in thinking allows the conditional expectation to be a much 
more powerful concept. 

\subsection{A More General Definition}
In the previous section, we considered defining the conditional expectation $\E[X|\mathcal{G}]$ in a simple case where the $\sigma$-algebra $\mathcal{G}$ was generated by a finite collection of sets. Naturally, the next 
generalization will be the case where $\mathcal{G}$ is generated by a countable collection of sets. In particular, assume that $\{G_i\}_{i = 1}^{\infty}$ form a partition of the sample space $\Omega$; i.e. they are 
pairwise disjoint and their union is $\Omega$. We suppose $\mathcal{G} = \sigma\left(\{G_i\}_{i = 1}^{\infty}\right)$. Recalling the notation 
\[\E[X|G] := \frac{\E[X\mathbbm{1}_G]}{\Prob(G)}, \text{ for } G \in \mathcal{G}\]
we simply define $\E[X|\mathcal{G}]: \Omega \to \R$ by $\E[X|\mathcal{G}](\omega) := \E[X|G_i]$ if $\omega \in G_i$. This is essentially the same definition as in the previous case, we now simply have a countable 
number of $G_i$. Once again, if all we know is that $\omega \in \mathcal{G}$ then the idea is to average $X$ over $G_i$ to produce a best guess for $X$, re-weighting by $\Prob(G_i)$ as is necessary.

\subsubsection{An Example: Infinite Coin Flips}
\textbf{TODO}: this section is incomplete.
As an example of this generalization, consider the experiment that consists of an infinite number of coin tosses. The probability space $\Omega$ consists of all sequences $\omega: \mathbb{N} \to \{0, 1\}$ of the form 
\[\omega = (\omega_1, \omega_2, \omega_3, \dots), \text{ where } \omega_i \in \{0, 1\}, \text{ for all } i,\]
that is, all infinite sequences of zeros and ones. Constructing a $\sigma$-algebra that allows us to make probabilistic statements about all of the coin tosses is is a bit tricky. For example, we might want to be able to talk about 
the probability that every coin flip is heads. However, it is not hard to construct smaller $\sigma$-algebras that allow us to make statements about finitely many of the coin tosses. Intuitively, we want to define a $\sigma$-algebra
like 
\[\mathcal{F}_n = \{\text{Knowledge of the first $n$ coin tosses}\}\]
for each $n \in \mathbb{N}$. This $\sigma$-algebra should contain all events that are determined entirely by the first $n$ coin tosses. Formally, we can describe the sets in $\mathcal{F}_n$ by:
\[A \in \mathcal{F}_n \iff \exists A_n \subset \{0, 1\}^n \text{ such that } A = \{\omega \in \Omega: (\omega_1, \dots, \omega_n) \in A_n\}\] 
As we increase $n$ then $\mathcal{F}_n$ will contain knowledge of more coin tosses, and should therefore be a larger $\sigma$-algebra. Indeed, it is not difficult to show that 
\[\mathcal{F}_n \subset \mathcal{F}_{n + 1}\]
A natural candidate for a larger $\sigma$-algebra $\mathcal{F}_0$ is the collection resulting from tossing all of the sets in each $\mathcal{F}_n$ into one large collection; that is, 
\[\mathcal{F}_0 := \bigcup_{n \in \mathbb{N}} \mathcal{F}_n\]
Unfortunately, it turns out that $\mathcal{F}_0$ is not actually a $\sigma$-algebra; it is missing some sets that should be included. Thus, the next natural step is to consider the smallest $\sigma$-algebra containing $\mathcal{F}_0$. 
\[\mathcal{F} := \sigma(\mathcal{F}_0)\]
Given the measurable space $(\Omega, \mathcal{F})$ it is now possible to define a probability measure $\mathcal{P}$ that models the concept of ``fair'' coin tosses. I will not give specifics on this but see 
\href{https://www.ee.iitm.ac.in/~krishnaj/EE5110_files/notes/lecture8_Infinite\%20Coin\%20Toss.pdf}{here} for details. 

\subsubsection{Next Steps}
In the interest of taking the next step in generalizing to arbitrary $\sigma$-algebras, not necessarily generated by a countable collection of sets, let's consider the salient property of our current definition of conditional expectation 
that makes it a reasonable definition. I will again denote the conditional expectation as $Z = \E[X|\mathcal{G}]$ as I believe it makes the following explanation easier to understand. Recall that the conditional expectation $Z$ 
takes the knowledge that an outcome $\omega$ is in some $G_i \in \mathcal{G}$ and then produces a guess for the value of $X$ by averaging over the finer-grain $\mathcal{F}$-information in $G_i$ (since this information is not 
available; all that is known is that $\omega \in G_i$). The key thing to note here is that this ``averaging over'' is done with respect to $\mathcal{F}$. Recall the definition 
\[Z(\omega) = \frac{\E[X \mathbbm{1}_{G_i}]}{\Prob(G_i)}, \text{ if } \omega \in G_i\]
The expectation in the numerator is defined with respect to $\mathcal{F}$, since $X$ is $\mathcal{F}$-measurable. 

\subsection{General Definition of Conditional Expectation}

\section{Review for Final Exam}

\subsection{Conditional Expectation}
Throughout this section let $(\Omega, \mathcal{F}, \Prob)$ be a probability space and $\mathcal{G}$ a sub-$\sigma$-algebra of $\mathcal{F}$. 

\begin{definition}
Let $X: \Omega \to \R$ be an integrable $\mathcal{F}$-measurable random variable (denoted $X \in \mathcal{F}$). The conditional expectation of $X$ with respect to $\mathcal{G}$ is a random variable 
$Z \in \mathcal{G}$ satisfying $\E[X \mathbbm{1}_{G}] = \E[Z \mathbbm{1}_{G}]$ for all $G \in \mathcal{G}$. The conditional expectation is typically denoted
as $\E[X|\mathcal{G}] \equiv Z$. 
\end{definition}

The conditional expectation satisfies some properties that we would intuitively hope are true: linearity, monotonicity, conditional expectation of a constant is a.s. equal to that constant. Here are some perhaps less-obvious 
properties: 
\begin{itemize}
\item Law of iterated expectations: $\E \E[X|\mathcal{G}] = \E X$. Note that the outer expectation of the left is wrt $\mathcal{G}$, while the one on the right is wrt $\mathcal{F}$. 
\item If $X \in \mathcal{G}$ then $\E[X|\mathcal{G}] \overset{a.s.}{=} X$. This just says that, given knowledge of $\mathcal{G}$ completely determines $X$, since $X \in \mathcal{G}$. Essentially, $X$ can be treated like a constant. 
\item If $\mathcal{G}$ is independent of $X$ then $\E[X|\mathcal{G}] = \E[X]$. So $\mathcal{G}$ provides no additional information about $X$. 
\item $\abs{\E[X|\mathcal{G}]} \leq \E[\abs{X}|\mathcal{G}]$. More generally, the generalization of Jensen's inequality for conditional expectation holds as we would expect. 
\item If $X$ and $XY$ are integrable, $Y \in \mathcal{G}$ then $\E[XY|\mathcal{G}] = Y\E[X|\mathcal{G}]$ ($Y$ can be treated as a constant). 
\item The tower property: If $\mathcal{F}_1 \subset \mathcal{F}_2$ then 
\[\E[\E[X|\mathcal{F}_1]|\mathcal{F}_2] = \E[X|\mathcal{F}_1] = \E[\E[X|\mathcal{F}_2]|\mathcal{F}_1] \]
In other words, not matter the order of conditioning the best best guess of $X$ is limited by the less granular $\sigma$-algebra, since once you condition on $\mathcal{F}_1$ the information in 
the more granular $\mathcal{F}_2$ is lost. 
\end{itemize}

\subsection{Martingale Basics}

\begin{definition}
A collection of increasing sub-$\sigma$-algebras of $\mathcal{F}$, 
\[\mathcal{F}_1 \subset \mathcal{F}_2 \subset \dots \subset \mathcal{F}\]
 is called a \textit{filtration}. A sequence of random variables $\{X_n\}$ is said to be \textit{adapted} to the filtration $\{\mathcal{F}_n\}$ provided that $X_n \in \mathcal{F}_n$ for all $n$. 
 If instead $X_n \in \mathcal{F}_{n-1}$ for all $n$, then we say that $\{X_n\}$ is $\{\mathcal{F}_n\}$-predictable. 
\end{definition}
We can think of a filtration as capturing knowledge through time, where $\mathcal{F}_n$ captures the knowledge up through time $n$. A sequence of random variables being adapted to the filtration encodes the 
notion that $X_n$ only depends on information up through time $n$ (it does not require looking into the future). A sequence of random variables being predictable means that $X_n$ is completely determined by 
information up through the previous time step $n - 1$. A good example is a betting strategy, where $X_n$ is how much money you are going to bet in round $n$. It is reasonable to think that how much you are going to 
bet is completely determined by how you have fared in the previous rounds. 

A few more notes on filtrations and $\sigma$-algebras. First, define the $\sigma$-algebra $\sigma_\infty = \sigma \left\{\bigcup_{n} \mathcal{F}_n \right\}$, 
which includes all information as time ``goes to infinity''. Next, we typically are considering a sequence $\{X_n\}$ and do not explicitly mention a filtration, which means that we are implicitly assuming the natural filtration
\[\mathcal{F}_n := \sigma\left\{X_0, X_1, \dots, X_n \right\} := \sigma\left(\bigcup_{k = 0}^{n} \sigma(X_k) \right)\]
Recall that the union of $\sigma$-algebras is not necessarily a $\sigma$-algebra, hence the need for the $\sigma$ on the outside of the union. 

\begin{definition}
Let $\{X_n\}$ be an $\{\mathcal{F}_n\}$-adapted sequence of integrable random variables. Then $\{X_n, \mathcal{F}_n\}$ is called a \textit{martingale} if 
\[\E[X_{n + 1}|\mathcal{F}_n] = X_n \text{, for all } n\]
If is called a sub-martingale if $\E[X_{n + 1}|\mathcal{F}_n] \geq X_n$ and a super-martingale if $\E[X_{n + 1}|\mathcal{F}_n] \leq X_n$. We call it an $L^p$-martingale (or sub/super) if, in addition, $\E[\abs{X_n}^p] < \infty$ for all $n$. 
\end{definition}
A martingale encodes the notion of a ``fair game'' in which, given information up through round $n$, the expected payout of future rounds is $0$ (i.e. your expected earnings is whatever your current earnings are). It can also be seen 
as a generalization of a random walk (where in a random walk $X_n$ is simply a sum of $n$ mean-zero independent random variables). One key initial observation is that martingales have constant mean (they are ``mean stationary''). 
Indeed, applying the defining relation of martingales and the law of iterated expectations
\[\E X_{n+1} = \E \E[X_{n + 1}|\mathcal{F}_n] = \E X_n \text{ for all } n\]


To explore the idea of martingales generalizing random walks, consider a random walk
\[X_n = \sum_{k = 0}^{n} Y_k\]
where the $Y_k$ are independent, mean-zero. The key property of a random walk is that it has independent increments $X_{n + 1} - X_n$ (clearly, since this difference equals $Y_k$ and the $Y_k$ are independent). We can thus 
view a random walk as some starting condition plus its increments up to the current time: 
\[X_n = Y_0 + \sum_{k = 1}^{n} Y_k\]
where we note that $X_0 = Y_0$. The initial condition is in general a mean-zero random variable, independent of the increments, though it is often simply set to zero, $X_0 \equiv 0$ so that the process starts at the origin. Martingales 
generalize this notion by allowing processes which look like 
\[X_n = \sum_{k = 0}^{n} U_k\]
where the $U_k$ are mean-zero, but not necessarily independent. In the case of martingales, the $U_k$ are uncorrelated (a weaker condition), which can also be thought of as an orthogonality condition. So while random walks have 
independent increments, martingales of this form have orthogonal increments. I should emphasize at this point that martingales are more general than sums of mean zero random variables; for example, we will see product forms of 
martingales in addition to the sum forms discussed here. That being said, martingales can always be decomposed as a sum of its increments in this way. Indeed, let $\{X_n\}$ be a generic martingale. Then 
\[X_n = X_0 + \sum_{k = 1}^{n} (X_k - X_{k-1})\]
which simply follows from the terms cancelling in the telescoping sum. Now, we can define $U_0 := X_0$ and $U_n := X_n - X_{n - 1}$, $n \geq 1$ so that this can be re-written as 
\[X_n = \sum_{k = 0}^{n} U_k\]
Note that since martingales have constant expectation then 
\[\E U_n = \E[X_n - X_{n-1}] = 0\]
so the increments are indeed mean zero. Also note that $U_n = X_n - X_{n-1} \in \mathcal{F}_n$ so that $\{U_n\}$ is $\{\mathcal{F}_n\}$-adapted. Finally note that the increments satisfy a property analogous to the defining relation for 
martingales. 
\[\E[U_{n + 1}|\mathcal{F}_n] = \E[X_{n + 1} - X_n|\mathcal{F}_n] = \E[X_{n + 1}] - X_n = \E[X_{n + 1} - X_n] = 0\]
so that the conditional expectations of the increments, in addition to the unconditional ones, are also $0$. The above argument is a very common one; it uses linearity of conditional expectation, the fact that $X_n \in \mathcal{F}_n$ and hence 
gets pulled out like a constant, and $X_{n + 1}$ is independent of $\mathcal{F}_n$ so that its conditional expectation equals the unconditional expectation. We have thus shown that martingales can be decomposed as partial sums of 
a sequence $\{U_n\}$ satisfying the properties just outlined. It makes sense to give this sequence a name. 

\begin{definition}
An $\{F_n\}$-adapted sequence $\{U_n\}$ is called a \textit{martingale difference sequence} provided that $\E[U_{n+1}|\mathcal{F}_n] = 0$. The standard sub/super terminology is also used when the equality is replaced with 
$\geq$ or $\leq$, respectively. 
\end{definition}

Let's now consider some martingales other than random walks. It turns out that the square of an $L_2$ martingale is not quite a martingale, but by subtracting off the mean of the process we can obtain a martingale. 
To show this, let $Y_k$ be independent mean zero random variables with variances $\sigma_k^2$, $S_n = \sum_{k = 1}^{n} Y_k$ their partial sum, and denote $s_n^2 := \sum_{k = 1}^{n} \sigma_k^2$. Note that the mean zero restriction on the $Y_k$ is not actually a restriction; these random variables could have 
non-zero mean and then we would just center them to show the analogous result. The claim 
is that $X_n := S_n^2$ is a sub-martingale but $X_n^\prime := S_n^2 - s_n^2$ is a martingale.  To show this, consider 
\begin{align*}
\E[X_{n + 1}|\mathcal{F}_n] &= \E\left[\left(Y_{n + 1} + S_n \right)^2 \bigg| \mathcal{F}_n \right] \\
					   &= \E[Y_{n + 1}^2|\mathcal{F}_n] + \E[S_n^2|\mathcal{F}_n] + 2\E[Y_{n+1}S_n|\mathcal{F}_n] \\
					   &= \E[Y_{n + 1}^2] + S_n^2 + 2 S_n \E[Y_{n+1}|\mathcal{F}_n] \\
					   &= \sigma_{n + 1}^2 + S_n^2 + 2S_n \E[Y_{n+1}] \\
					   &= X_n + \sigma_{n + 1}^2 \geq S_n^2
\end{align*}
so this is indeed a sub-martingale. Notice that the last three steps use the standard measurability and independence arguments discussed above (note that $S_n^2 \in \mathcal{F}_n$). 
An almost identical calculation shows that the correction $X_n^\prime := S_n^2 - s_n^2$ results in a martingale. 

It turns out that many martingales also take a product form. If $Y_k$ are independent with mean $1$, then 
\[X_n = \prod_{k = 1}^{n} Y_k\]
can easily be shown to be a martingale. The argument is very similar as the above, where the final term is split off of the product, rather than the sum and then measurability and independence 
arguments are applied. We finish this introduction with a couple more examples of special types of martingales. First, let $\{\mathcal{F}_n\}$ be a filtration, $Z$ a random variable with finite 
expectation Then define 
\[X_n := \E[Z|\mathcal{F}_n]\]
I claim this defines a martingale. We can think of the random process $\{X_n\}$ as representing the acquisition of more and more information about $Z$ over time. Before proving that it actually holds, 
consider the defining relation $\E[X_{n+1} |\mathcal{F}_n] = X_n$. This is a bit odd to interpret given that $X_n := \E[Z|\mathcal{F}_n]$. Loosely speaking, I like to think of $X_n$ as the ``best guess'' of 
$Z$ given knowledge up to time $n$. The defining relation thus says that considering a ``better guess'' $\E[Z|\mathcal{F}_{n+1}]$ does not gain us any thing if we are conditioning on $\mathcal{F}_n$; i.e.
conditioning on $\mathcal{F}_n$ drops all additional knowledge gained from $\mathcal{F}_{n+1}$ (this is essentially just the tower law, and indeed the proof relies on the tower law as I show below).
\[\E[X_{n + 1}|\mathcal{F}_n] = \E[\E(Z|\mathcal{F}_{n+1})|\mathcal{F}_n] = \E[Z|\mathcal{F}_n] = X_n\] 
This martingale seems pretty odd, but it turns out to have some very nice and important properties. 

Using similar arguments, we can actually devise a way to adjust any integrable, adapted sequence $\{Y_k\}$ into a martingale. Indeed, set $X_0 = Y_0$ and define 
\[X_n = \sum_{k = 1}^{n} \left(Y_k - \E[Y_k|\mathcal{F}_{k-1}] \right), n \geq 1\]
$\{X_n, \mathcal{F}_n\}$ is a martingale. This makes sense; we are just adjusting the increments (i.e. martingale differences) to have conditional mean zero so that the defining relation 
will hold. This is basically the analog of centering random walks. 

\subsection{Martingale Algebra}
Whenever defining a new mathematical object, it is typical to study classes of transformations that preserve the structure of the object. In this vein, one can show the following. 
Let $\{X_n\}$, $\{Q_n\}$ be martingales. 
\begin{enumerate}
\item linear combinations of martingales are martingales; i.e. a $\{a X_n + b Q_n\}$. If $X_n$ and $Q_n$ are instead submartingales, then \textit{positive} linear combinations 
$(a, b > 0)$ preserve the submartingale structure. The same is true of supermartingales.  
\item $\max\{X_n, Q_n\}$ is a submartingales. this makes sense since submartingales are increasing random processes, in the sense that expectations of future states given the 
past are higher than the current state. 
\item Similarly, $\min\{X_n, Q_n\}$ is a supermartingale. 
\item Changing the sign of a supermartingale (i.e. $a \cdot X_n$, where $a < 0$) results in submartingale, and vice versa. 
\item A Jensen-type result: if $\{X_n\}$ is a martingale and $g$ a convex function then $\{g(X_n)\}$ is a sub-martingale. 
\item If $\{X_n\}$ is a martingale then so is $\{\abs{X_n}\}$ as well as the positive and negative parts $\{X_n^+\}$ and $\{X_n^-\}$. 
\item If $X_n \in L_p(\Omega)$ and $p > 1$ then $\{\abs{X_n}^p\}$ (this is a consequence of the Jensen-type result due to convexity). 
\end{enumerate}

\subsection{Orthogonality}
We have already noted that martingales have orthogonal (i.e. uncorrelated) increments. We make this more precise here. To even talk about orthogonality, we must 
have $X_n \in L_2(\Omega)$, hence throughout this section we assume $\{X_n\}$ is an $L_2$ martingale. We showed before that any martingale can be represented 
as the partial sums of a martingale difference sequence $\{U_k\}$, such that 
\[X_n = \sum_{k = 0}^{n} U_k\]
Since $\E U_k = 0$ then the claim of orthogonal increments is a claim that $\E U_k U_j = 0$ for $j > k$. Conditioning on the less informative (``coarser'') $\sigma$-algebra, 
\[\E[U_k U_j | \mathcal{F}_k] = U_k \E[U_j | \mathcal{F}_k] = U_k \E[U_j] = U_k \cdot 0 = 0\]
where the second step uses independence. Now taking expectations of both sides and applying the law of iterated expectations yields the result 
\[\E \E[U_k U_j|\mathcal{F}_k] = \E U_k U_j = 0\]
This orthogonality implies 
\[\E \left(\sum_{i = k + 1}^{j} U_i \right)^2 = \sum_{i = k + 1}^{j} \E U_i^2\]
Perhaps a little more surprisingly, we have 
\[\E[X_k X_j] = \E[X_k^2]\]
This follows from an identical argument as before, since
\[\E[X_k X_j | \mathcal{F}_k] = X_k \E[X_j | \mathcal{F}_j] = X_k \E[X_j]\]
Now taking expectations and applying tower law, 
\[\E \E[X_k X_j | \mathcal{F}_k]  = \E[X_k X_j] = \E[X_k] \E[X_j] = \E[X_k]^2\]
where the last step follows from the fact that martingales have constant mean. 

\subsection{Decompositions}
We now discuss a couple useful martingale decompositions. 

\subsubsection{The Doob Decomposition}
Recall that we already discussed that any $\{\mathcal{F}_n\}$-adapted, integrable sequence $\{Y_k\}$ can be turned into a martingale by 
setting $X_0 = Y_0$ and defining 
\[X_n = \sum_{k = 1}^{n} \left(Y_k - \E[Y_k|\mathcal{F}_{k-1}] \right), n \geq 1\]
We will use this below. Also, before proceeding we need one more definition. We call $\{A_n, \mathcal{F}_n\}$ an \textit{increasing process} provided that 1.) $A_0 = 0$, 2.) $A_n$ increases to 
$+\infty$, and 3.) $A_n$ is $\{\mathcal{F}_n\}$-predictable. The result known as \textit{Doob's decomposition} claims that any submartingale $\{X_n\}$ can be uniquely decomposed as 
\[X_n = M_n + A_n\]
where $\{M_n\}$ is a martingale and $\{A_n\}$ an increasing process. While I will not prove this in full, it uses the fact that 
\[M_n := \sum_{k = 1}^{n} \left(X_k - \E[X_k|\mathcal{F}_{k-1}] \right), n \geq 1\]
is a martingale. Given this, the only choice for $A_n$ is to set it equal to the remainder $X_n - M_n$:
\[A_n := X_n - M_n\]
$\{A_n\}$ can be shown to be an increasing process and that this decomposition is unique. 

\subsubsection{The Krickenberg Decomposition}
It is often convenient to avoid working with potentially negative objects $x$ by instead decomposing as a difference of two positive quantities $x = x^+ - x^-$. For example, this is used 
when extending the definition of the Lebesgue integral from non-negative functions to arbitrary functions. This idea turns out to be useful for martingales as well. In particular, we seek 
to understand the conditions under which a martingale $\{X_n\}$ can be decomposed as 
\[X_n = M_n^+ - M_n^-\]
where $M_n^+$ and $M_n^-$ are non-negative martingales. It turns out that the condition we need is a uniform bound on $\E X_n^+$, 
\[\sup_n \E(X_n^+) < \infty\]
Note that this is very similar to assuming a uniform bound on $\E \abs{X_n}$, and that there is nothing special about the positive part; we could also have assumed the same for the negative 
part. The reason is that martingales have constant expectation $\mu$, so that 
\[\mu = \E X_n = \E X_n^+ - \E X_n^-\]
implies that either $\E X_n^+$,  $\E X_n^-$ must both be finite or must both be infinite. Since these two terms differ only by $\mu$ then uniformly bounding one of them implies a uniform bound 
on the other. And since 
\[\E\abs{X_n} =  \E X_n^+ + \E X_n^-,\]
this also implies a uniform bound on $\E\abs{X_n}$. Without proving this result, note that the obvious decomposition $X_n = X_n^+ - X_n^-$ does not get the job done since 
$X_n^+$ and $X_n^-$ are only guaranteed to be \textit{submartingales}. $M_n^+$ and $M_n^-$ require a more sophisticated construction that is not detailed here. 

\subsection{Stopping Times}
Stopping times are a fundamental concept in the theory of martingales. Discrete time martingales are random processes indexed by integer-valued time, and there are many questions we want 
to ask about the value of the process $X_n$ at a specific time $n$, or perhaps as $n \to \infty$, etc. However, we will also find it useful to consider the behavior of the process at a \textit{random time}
$\tau: \Omega \to \mathbb{N}_0 = \{0, 1, 2, \dots\}$. The value of process at this time is $X_\tau$, and there is now uncertainty both in the value of the process at any specific time, as well as 
in the time $\tau$ itself. As an example of why we might care about this, consider a ``first entrance time'' or ``hitting time'',
\[\tau = \min_{n} \{\abs{X_n} \geq a\}\]
so that $\tau$ is the first time that the process escapes the interval of radius $a$ centered at the origin. Since the process is random, this time is clearly random. If we simulate different trajectories 
of the process, some may exceed a radius of $a$ very quickly, while others may take much longer. Studying $X_\tau$ will then allow us to characterize the behavior of the process at this random 
hitting time.

We now define a concept that generalizes the notion of the random hitting time. The key idea we seek to capture in the following definition is that if we condition on $\mathcal{F}_n$ (knowledge up 
through time $n$) for any $n$ then we ought to be able to pin down the value of the random time. That is, we will only consider random times that are knowable from the past, excluding those that 
require knowledge of the future. In the example of the hitting time, at any time $n$ knowledge of the past is sufficient to know whether the process has exceeded the radius $a$ or not. On the other 
hand, what if we considered a ``last exit time''
\[\tau = \max_{n} \{\abs{X_n} \leq a\}\]
which means that $\tau$ is the last time the process is within the interval $(-a, a)$. Any any time $n$, given knowledge of the past we cannot pin down the value of $\tau$ since in the future the process 
may wander back to this interval. The following definition excludes such ``last exit times''. 
\begin{definition}
A random variable $\tau: \Omega \to \mathbb{N}_0 \cup \{+\infty\}$ is called a stopping time with respect to $\{\mathcal{F}_n\}$ provided that 
\[\{\tau = n\} \in \mathcal{F}_n\]
for all $n$. 
\end{definition}
Note that we allow the stopping time to assume the value infinity. This is convenient; for example, for a hitting time $\tau = \min_{n} \{\abs{X_n} \geq a\}$ the process may almost surely never 
leave the interval $(-a, a)$ in which case $\Prob(\tau = +\infty) = 1$. Using the defining properties of $\sigma$-algebras it is straightforward to show that equivalent definitions of stopping times 
could sub out $\{\tau = n\}$ for $\{\tau < n\}$, $\{\tau \leq n\}$, $\{\tau > n\}$, or $\{\tau \geq n\}$. 

Given that $\mathcal{F}_n$ encodes all knowledge through time $n$ it is natural to wonder if we can define a $\sigma$-algebra $\mathcal{F}_\tau$ that encodes all knowledge up through the 
random time $\tau$. Indeed we can. As a motivating example, consider the first entrance time we have been discussing. Recall that we discussed that probabilistic statements about $X_\tau$ 
require accounting for the uncertainty in the random time $\tau$ as well as uncertainty in the process itself, conditional on a fixed time. Events such as 
\[A = \{X_\tau = x\}\]
should be in $\mathcal{F}_\tau$ as this event is a piece of information that only requires knowledge up through time $\tau$. To decompose the randomness in the time versus the randomness 
in the process, consider re-writing this event as, 
\[A = \bigcup_{\tau = 0}^{\infty} \left(\{X_\tau = x\} \cap \{\tau = n\}\right) = \bigcup_{\tau = 0}^{\infty} \left(A \cap \{\tau = n\}\right) \]
Events of the form $A \cap \{\tau = n\}$ are really the key here, as they allow us to make statements about arbitrary event $A$ that also account for the uncertainty in the value of 
$\tau$. In particular, we must be able to make statements about events of this form given knowledge up to time $n$, not requiring knowledge of the future. Formally, this means that 
$A \cap \{\tau = n\}$ must be $\mathcal{F}_n$-measurable. Thus, $\mathcal{F}_\tau$ is defined as 
\[\mathcal{F}_\tau := \{A \in \mathcal{F}_\infty : A \cap \{\tau = n\} \in \mathcal{F}_n \text{ for all } n\}\]
Recall that $\mathcal{F}_\infty$ is the $\sigma$-algebra generating by all of the $\mathcal{F}_n$ and is hence is the largest (most granular) $\sigma$-algebra we work with in this context. 
It can be shown that $\mathcal{F}_\tau$ is indeed a well-defined $\sigma$-algebra. Having defined this concept, we state some properties that help to demonstrate that this definition is 
reasonable; really any notion of ``knowledge up to time $\tau$'' should satisfy these properties. 
\begin{enumerate}
\item The trivial random variable $\tau(\omega) \equiv k$ for some $k \in \mathbb{N}_0$ is a stopping time. 
\item If $\tau(\omega) \equiv k$ then $\mathcal{F}_\tau = \mathcal{F}_k$. 
\item $\tau \in \mathcal{F}_\tau$
\item $\tau \in \mathcal{F}_\infty$
\item $\mathcal{F}_\tau \subset \mathcal{F}_\infty$
\item $\{\tau = +\infty\} \in \mathcal{F}_\infty$
\item If $\tau_1 \leq \tau_2$ then $\mathcal{F}_{\tau_1} \subset \mathcal{F}_{\tau_2}$
\end{enumerate}
I'll prove the first three. For the trivial stopping time $\tau(\omega) \equiv k$ note that $\{\tau = n\} = \Omega$ if $n = k$ and $\varnothing$. Since both of these events are in $\mathcal{F}_n$
then $\tau$ is indeed a stopping time, which proves number 1. Also for any $A \in \mathcal{F}_\infty$ this means either $A \cap \{\tau = n\}$ equals either $A \cap \Omega = A$ or 
$A \cap \varnothing = \varnothing$ so the definition $A \cap \{\tau = n\}$ simply reproduces the events in $\mathcal{F}_k$. Thus, $\mathcal{F}_\tau = \mathcal{F}_k$, which proves property 2. 
For property 3, consider an arbitrary stopping time $\tau$. We seek to show that for any $n$, $\tau^{-1}(n) = \{\tau = n\} \in \mathcal{F}_\tau$. By definition 
$A \cap \{\tau = n\} \in \mathcal{F}_\tau$ so setting $A = \Omega$ proves this. 

\subsubsection{Stopping Time Arithmetic}
Just as we studied which operations preserve martingales, we seek to do the same with stopping times. As far as basic arithmetic we have that 
sums of stopping times are stopping times, but differences are not in general (they can assume negative values, and stopping times cannot). Minima and maxima also 
preserve stopping times. Minima and maxima of stopping times will be important in subsequent theory, so we introduce special lightweight notation for them:
\begin{align*}
\tau_1 \wedge \tau_2 &:= \min\{\tau_1, \tau_2\} \\
\tau_1 \vee \tau_2 &:= \max\{\tau_1, \tau_2\}
\end{align*}
These results can all be proved by checking the measurability condition and using typical properties of $\sigma$-algebras. 

Next, do limits preserve stopping times? We consider limits of monotone sequences: $\tau_k \uparrow \tau$ and $\tau_k \downarrow \tau$. Now it is important to remember that stopping times 
are integer-valued (or infinity) so the limit of the sequence $\{\tau_k(\omega)\}$ either does not exist, equals a non-negative integer, or is $+\infty$. Since we only consider monotone sequences, 
this guarantees the first option does not occur, so that the sequences have (potentially infinite) limits. I will show that in either case the limit $\tau$ is indeed a stopping time. To this end, we must 
show that for any $n$, $\{\tau = n\} \in \mathcal{F}_n$. In the case $\tau_k \uparrow \tau$ and for $\omega \in \Omega$ $\tau(\omega) < \infty$, the 
once the sequence $\tau_k(\omega)$ hits the value $\tau(\omega)$ it must stay at that value. If $\tau(\omega) = +\infty$ then the sequence $\tau_k(\omega)$ will keep growing larger. Thus, 
\[\{\tau = n\} = \bigcup_k \{\tau_k = n\} \in \mathcal{F}_n\]
The first equality captures the above logic that the sequence must eventually hit the value $n$, and the set inclusion uses the fact that unions of sets in $\sigma$-algebras also belong to 
the $\sigma$-algebra. The case $\tau_k \downarrow \tau$ is similar. In this case the limit must be finite and moreover every element of the sequence $\tau_k(\omega)$ must equal $n$ in order for 
the limit to be $n$. This is due to the fact that the sequence is decreasing; for example, if $\tau_k < n$ then the limit would have to be less than $n$ so this is not possible. If $\tau_k > n$ then the 
process is stopped at a value greater than $n$ and the limit is greater than $n$ (it seems that the sequence could still decrease to $n$, but perhaps I am misunderstanding what we mean by 
monotone decreasing here). Thus, 
\[\{\tau = n\} = \bigcap_k \{\tau_k = n\} \in \mathcal{F}_n\]

\subsubsection{Optional Sampling/Stopping}
We setup out of the way, we now arrive at some important results for stopping times and martingales. First of all, recall the defining relation for martingales 
\[\E[X_{n + 1}|\mathcal{F}_n] = X_n\]
which we also saw implies that the process has constant (unconditional) expectation. Some natural questions are to wonder if these results also hold for 
stopping times; i.e. for stopping times $\tau$ and $\tau_1 < \tau_2$,
\begin{align*}
&\E[X_{\tau_2}|\mathcal{F}_{\tau_1}] = X_{\tau_1}? &&\E X_{\tau} = \E X_0?
\end{align*}
If these statements are both true then in essence we can think of $X_\tau$ as ``part of the martingale'', in that it behaves just like $X_n$ for a non-random time $n$. 
It turns out these results do not hold in general, so we are interested in conditions under which they do hold. But first we consider an example where they do not hold. 

\bigskip
\textbf{Example.} Let $\{Y_k\}$ be iid $\Prob(Y_k = 1) = \Prob(Y_k = -1) = \frac{1}{2}$ and define the process 
$X_n = \sum_{k = 1}^{n} Y_k$ and stopping time $\tau := \min\{n: X_n = 1\}$. First note that $\{X_n\}$ is a centered random walk and hence is a martingale with respect 
to the natural filtration. 
Next note that since $X_\tau = X_{\min\{n: X_n = 1\}}$ then $X_\tau$ is almost surely equal to $1$; that is,
\[\Prob(X_\tau = 1) = 1\]
Thus, $\E X_\tau = 1$, but $\E X_n = 0$ for all $n$. Thus the process $\{X_1, X_\tau\}$ is \textit{not} a martingale.  
Thus, any sort of theorems that guarantee that $X_\tau$ is part of the martingale must assume conditions that exclude situations such as this example. It turns out that 
the problem here is that $\tau$ is ``too large'' in some sense; in fact, it can be shown that $\E \tau = +\infty$. 

It turns out that assuming $\Prob(\tau \leq M) = 1$ (i.e. that $\tau$ is almost surely bounded) is enough to guarantee $X_\tau$ is part of the martingale. There are a few different 
related results that are often called \textit{Doob's optional sampling} or \textit{Doob's optional stopping} theorem. We present one such result below. 
\begin{thm}
Suppose that $\{X_n \mathcal{F}_n\}$ is a martingale and $\tau$ a bounded stopping time; that is, $\Prob(\tau \leq M) = 1$ for some $M$. Then $\{X_\tau, X_M\}$ is also 
a martingale (with respect to $\{\mathcal{F}_M, \mathcal{F}_\tau\}$); in particular,
\[\E X_\tau = \E X_M\]
and 
\[\E[X_M|\mathcal{F}_\tau] = X_\tau\]
\end{thm}
Thinking about $\{X_M, X_\tau\}$ is a bit confusing; it is a random process indexed by random time, but the time can never exceed $M$ (almost surely). And the expectation 
of the process is constant. Also, the expectation of the process at the ``final time'' $M$, given information up to random time $\tau$ is equal to $X_\tau$, the value of the 
process at that random time. It really is just a generalization of martingales where the time index $\{0, 1, 2, \dots\}$ has been replaced by a random time $\tau$. 

As a corollary, note that we can construct bounding stopping times from potentially unbounded ones. Indeed, if $\tau$ is a stopping time then 
$\tau \wedge n$ is bounded; $\Prob(\tau \wedge n \leq n) = 1$. Then by the previous theorem we know that $\{X_{\tau \wedge n}, X_n\}$ is a martingale, and in particular, 
\[\E X_{\tau \wedge n} = \E X_n\]

Next, I present a more general version of Doob's optional stopping theorem that implies the version stated above. Recall that any integrable random variable 
$Z \in L_1(\Omega)$ can be used to define a martingale with respect to filtration $\{\mathcal{F}_n\}$ by defining the process
\[X_n = \E[Z|\mathcal{F}_n]\]
It turns out that $\{(X_\tau, \mathcal{F}_\tau), (Z, \mathcal{F}_\infty)\}$ is a martingale and in particular, 
\[\E X_\tau = \E Z\]
Thus, $Z$ provides a natural ``right endpoint'' for martingales of this sort. 


\end{document}


