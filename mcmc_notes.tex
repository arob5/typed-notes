\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Notes on Markov Chains, Stochastic Stability, and MCMC
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Section: Beyond Independent Sampling
\section{Beyond Independent Sampling}
Discuss some of the shortfalls of Accept/Reject and other independent sampling methods here. 

% Section: Markov Chains
\section{Markov Chains}
\subsection{Defining Markov Chains}
\subsubsection{Measure Theoretic Setup}
Let $(\Omega, \mathcal{A}, \Prob)$ be a measure space. Although we will rarely mention this space going forward, it it important to remember that it is always
underlying all of the theory we develop. Now, consider a measurable space $(\mathcal{X}, \mathcal{F})$ which will call a \textit{state space}. $\mathcal{X}$ is a set of states that the Markov 
Chain can assume, and $\mathcal{F}$ contains all of the subsets of the state space to which we will be able to assign probabilities. We will typically restrict ourselves to 
$(\mathcal{X}, \mathcal{F}) = (\R^n, \mathcal{B})$.
We consider a discrete-time stochastic process $\mathbb{X} = \{X_k\}_{k = 1}^{\infty}$ defined on the state space $(\mathcal{X}, \mathcal{F})$. That is, each $X_k$ is a random variable
$X: \Omega \to \mathcal{X}$. So for $A \in \mathcal{F}$, the event $\{X_k \in A\}$ means ``at time step k, the state of the chain is somewhere in A''. Since the system is random, we 
can only talk about the probability of this event occurring, and the probability measure $\Prob$ assigns such a probability. It is important to emphasis that all of the $X_k$ are defined on 
the \textit{same} underlying probability space; that is, the source of randomness is the same for the system as a whole.

\subsubsection{The Markov Property}
\textbf{TODO}

\subsubsection{Homogenous Chains}
\textbf{TODO}

\subsubsection{Transition Kernels}
Under the assumption of homogeneity, a Markov chain is completely defined by its initial distribution $\nu_0 = \mathcal{L}(X_0)$ (this notation means the distribution or 
\textit{law} of $X_0$) and its \textbf{transition probability kernel} $P$. (include definition)

\subsection{Stationary Distributions}
The basic idea of MCMC is to construct a Markov Chain with stationary distribution equal to the target distribution, and then sample using this chain. 
Thus, it is first necessary to state some basic results regarding stationary distributions of Markov chains. 
\begin{definition}
A probability distribution $\nu$ is said to be \textbf{stationary} (equivalently, \textbf{invariant}) with respect to a homogenous Markov chain with transition kernel $P$ provided
that $\nu P = \nu$. 
\end{definition}
In general, invariant distributions are not unique. Thus, when we're designing Markov chains with an intended invariant distribution in mind, we will have to prove uniqueness
(otherwise we might be sampling from the wrong distribution). Since invariant distributions will be central in our design of sampling algorithms, it will be essential to check whether 
a given distribution is invariant or not. Unfortunately, checking $\nu P = \nu$ involves evaluating a potentially difficult integral. Fortunately, there is a very convenient sufficient condition 
that's much easier to check. To define this condition, we'll first need to define one more concept. But first, let's try to intuitively motivate this condition. Invariance says that if we make one 
time step forward then the distribution of the chain does not change. Now, consider a chain that runs the same forward as it does in reverse. Intuitively, this seems to be a stronger condition than invariance and will indeed see that this reversibility condition is the sufficient condition we seek. 

\begin{definition}
Define the product measure $\mu \otimes P$ on $(\mathcal{X} \times \mathcal{X}, \mathcal{F} \times \mathcal{F})$ by: 
\[(\mu \otimes P)(A \times B) := \int_{A \times B} \mu(dx) P(x, dy)\]
for $A, B \in \mathcal{F}$
\end{definition}
To get a better sense of this, first consider fixing $x$. Then integrating over $y$ gives the probability of transitioning from $x$ to $B$, weighted by 
the probability of being at $x$. Do this 
for all $x \in A$ and you have the probability of transitioning from $A$ to $B$ (again, weighted appropriately). So we can interpret 
$(\mu \otimes P)(A \times B)$ as ``the probability of observing a chain that is currently in A and then one step later is in B'' (note that 
``currently'' means with respect to the distribution $\mu$). Or put more simply, we're just considering the joint distribution between 
a state and the subsequent state. Now we arrive at the sufficient condition hinted at before. 

\begin{definition}
We say that a chain satisfies the \textbf{detailed balance} condition with respect to distribution $\mu$ provided 
\[(\mu \otimes P)(A \times B) = (\mu \otimes P)(B \times A)\]
for all $A, B \in \mathcal{F}$. In this case, we say that the chain is \textbf{reversible} with respect to $\mu$. 
\end{definition}

The following proposition simply verifies our previous intuition that reversibility should be a sufficient condition for invariance. 
\begin{prop}
If a chain is reversible with respect to $\nu$ (that is, satisfies detailed balance), then it is invariant with respect to $\nu$. 
\end{prop}

% Section: MCMC
\section{Markov Chain Monte Carlo (MCMC)}

\subsection{Motivation: Bayesian Inference}
Although MCMC has its origins in physics and finds applications in various domains, it has become known largely for its use in 
Bayesian statistics. In Bayesian inference, we seek to summarize the distribution of the parameter of interest $x$ conditional on observed 
data: 
\[\pi(x) := \pi(x|D) = \frac{p(D|x)\pi_0(x)}{\pi(D)}\]
$\pi(x|D)$ is known as the posterior distribution, $p(D|x)$ the likelihood (this is the model for our data; for example, a regression model), 
$\pi_0(x)$ the prior density, and $\pi(D)$ the marginal likelihood. For complicated distributions, our best bet is to use sampling. The typical 
situation is that $\pi(D)$ is intractable and thus can't be calculated analytically so we only have access to the unnormalized posterior 
through pointless evaluation of $p(D|x)\pi_0(x)$. In this case, the posterior will be the target distribution for our MCMC algorithms. 
Throughout these notes, I will always denote the target distribution as $\pi$. 

\subsection{Simple Monte Carlo}
\textbf{TODO}: approximating expectations, LLN, CLT, Monte Carlo error on order of $\frac{1}{\sqrt{k}}$

\subsubsection{The Big Picture}
MCMC tries to address some of the shortfalls of simple Monte Carlo by ditching independent samples and exploiting dependence 
to produce a more efficient sampling scheme. In particular, MCMC constructs a Markov chain $\mathbb{X} = \{X_k\}$ with unique invariant 
distribution equal to the target distribution $\pi$. The chain also of course must be initialized with some starting distribution $X_0 \sim \pi_0$.
Here is a high-level overview of how we will proceed in analyzing this approach: 
\begin{enumerate}
\item Construct a Markov chain with \textit{unique} stationary distribution $\pi$. This is a necessary first step, but it leaves many open questions: 
will the chain actually converge to the stationary distribution? (that is, is the limiting distribution equal to the stationary distribution?) How fast will 
it converge? Can we quantify the fluctuations around the limit? 
\item To first answer the most basic questions of convergence, we will develop a Law of Large Numbers (LLN) for Markov chains, analogous to the 
standard LLN for independent samples that guarantees convergence for simple Monte Carlo. 
\item Next we will want to develop more quantitative results; that is, to quantify the fluctuations in the limiting process. This will come in the form of a 
type of Central Limit Theorem (CLT) for Markov chains, which will allow us to calculate confidence intervals. 
\end{enumerate} 

\subsection{Step 1: Constructing a Markov Chain with Stationary Distribution $\pi$}
When we assume that the target distribution $\pi$ has a density with respect to a measure $\mu$ (typically the Lebesgue or counting measure).
I will denote this density by $h_\pi$. 

\subsubsection{Metropolis-Hastings}
We begin with one of the most well-known and widely used MCMC algorithms, Metropolis-Hastings (MH). Instead of just stating the algorithm, I will 
try to build up to it to hopefully give more of a sense of discovery. First, let's recall the basic structure of one of the fundamental independent sampling
techniques: the Accept-Reject algorithm. In this method, a sample is \textit{proposed} by sampling from some distribution from which we already know how to 
sample. Next, the sample is either accepted or rejected depending on some criteria. Let's adopt the same basic structure for our new method.

For now, let's assume densities exist to keep things simple - we will generalize this after. Let $q(x, \cdot)$ denote the density of the \textit{proposal distribution}; 
that is $q(x, y)$ is proportional to the probability of proposing a new state $y \in \mathcal{X}$ given current state $x \in \mathcal{X}$. Recall from Accept-Reject that 
once a new value was proposed then it was either accepted or rejected according to some decision rule. Let's adopt that idea here and let $\alpha(x, y)$ denote 
the probability of accepting the new state $y$ given the current state $x$. Note that this is completely separate from the probability of proposing $y$ given $x$, but 
we still allow the acceptance probability to depend on the previous state. Therefore, the value $q(x, y)\alpha(x, y)$ is proportional to the probability of transitioning 
to $y$ from $x$. 

Now, let's take a step back and recall our goal: to construct a Markov chain (that is, a transition kernel) $P$ with stationary distribution $\pi$. From the detailed balance 
condition we know that if
\[(\pi \otimes P)(A \times B) = (\pi \otimes P)(B \times A) \text{ for all } A, B \in \mathcal{F}\]
then $P$ is invariant with respect to $\pi$. Thus, we seek to construct $P$ such that this equality is satisfied. But, first we can simplify this expression. We have 
assumed densities exist so we can re-write this as: 
\[h_\pi (x)P(x, y) = h_\pi (y)P(y, x) \text{ for all } x, y \in \mathcal{X}\]
And finally, we have assumed some specific structure on $P$: 
\[h_\pi (x)q(x, y)\alpha(x, y)= h_\pi (y)q(y, x)\alpha(y, x) \text{ for all } x, y \in \mathcal{X}\]
Let's suppose $q$ is fixed; that is, the choice of proposal is a decision we'll have to make when implementing MH in a specific setting. This leaves us to 
define the acceptance probability $\alpha$ such that the above equation is satisfied. Rearranging, 
\[\frac{\alpha(x, y)}{\alpha(y, x)} = \frac{h_\pi(y)q(y, x)}{h_\pi(x)q(x, y)}\]
Note that we have the additional constraint that the acceptance probability must of course be bounded in $[0, 1]$. Thus, letting $\psi$ denote the right-hand side 
of this equation, we can set $\alpha$ as follows:
\[\text{If } \psi \leq 1 \text{, set } \alpha(x, y) := \psi \text{ and } \alpha(y, x) := 1\]
\[\text{If } \psi > 1 \text{, set } \alpha(x, y) := 1 \text{ and } \alpha(y, x) := \frac{1}{\psi}\]
That is, 
\[\alpha(x, y) := \min\left\{\frac{h_\pi(y) q(y, x)}{h_\pi(x)q(x, y)}, 1\right\}\]
Thus, with $\alpha$ defined by this formula it follows from detailed balance that $\pi$ is a stationary distribution of the Markov chain, as desired. This result is a necessary 
first step in designing MCMC algorithms, but let's be clear on what it \textit{doesn't} mean. We have not concluded any results that guarantee that iterating through time 
steps will eventually yield convergence to this stationary distribution, and even if there is convergence we have no idea how quickly the rate of convergence would be. This 
is a serious issue: if we're generating samples at each time step, we have no idea whether the samples are actually following the distribution of interest or not. We will address
this issues later.

Before proceeding, one more remark: did we really need the detailed balance condition to arrive at this acceptance probability? Could we just have used the definition of invariance instead?
Let's try. By definition $P$ is invariant with respect to $\pi$ if $\pi P = \pi$; that is $(\pi P)(A) = \pi(A)$ for all $A \in \mathcal{F}$ or 
\[\pi(A) = \int P(x, A) \pi(dx) \text{ for all } A \in \mathcal{F}\]
Using our assumption that densities exist and the specific structure we have chosen for $P$ we can re-write this as
\[h_\pi(y) = \int q(x, y) \alpha(x, y) h_\pi(x) dx\]
At this point we are kind of stuck: to solve for the $\alpha$ that satisfies this equality we would have to evaluate the integral on the right-hand side. We now see the power 
of the detailed balance condition: it only requires us to check transition probabilities between \textit{two} (arbitrary) states: $x$ and $y$. Using the definition of invariance
directly requires us to consider \textit{all} of the possible ways to get to $y$ from the current distribution $\pi$, hence the integral over $x$. Thus, the detailed balance condition
is much easier to check. 

Thus far, we have talked loosely about how we have defined a Markov chain by considering a kernel of the form ``first propose a new state and then accept with some 
probability''. However, we have not actually formally defined the kernel or proved that the Markov property is satisfied. The latter result should be pretty obvious: both the 
proposal and the acceptance probability only depend on the current state. Now let's consider the formal definition of the kernel. Recall, that $P(x, \cdot)$ must be a probability 
measure such that $P(x, A)$ is the probability of transitioning to $A \in \mathcal{F}$ from state $x \in \mathcal{X}$. One key thing to realize here is that there are two ways 
to end up in $A$: 
\begin{enumerate} 
\item Propose and accept a state in $A$. 
\item Reject the proposed state, but $x$ is already in $A$. 
\end{enumerate}
The probability of 1 can be found by integrating over all ways of proposing/accepting a state in $A$: 
\[\int_{A} q(x, y)\alpha(x, y)\mu(dy)\]
Recall that the densities here are all with respect to the measure $\mu$. The probability of 2 depends on whether or not $x \in A$ so we utilize the delta function 
$\delta_x(A)$ which assumes the value $1$ if $x \in A$ and $0$ otherwise. We also integrate over the whole state space this time as we are considering all of the ways
that a proposed state might be rejected. 
\[\delta_x(A)\int_\mathcal{X} q(x, y)[1 - \alpha(x, y)]\mu(dy)\]
Denoting this second integral by $\overline{\alpha}(x)$ we thus have the full kernel: 
\[P(x, A) = \int_{A} q(x, y)\alpha(x, y)\mu(dy) + \overline{\alpha}(x) \delta_x(A)\]

\textbf{TODO}: 
\begin{itemize} 
\item Independent MH
\item MH with symmetric proposal 
\item Gibbs 
\end{itemize}

In summary, this section has introduced how to construct a transition kernel that is invariant with respect to the target distribution $\pi$. In doing so we have introduced 
a variety of algorithms that fall in the Metropolis-Hastings family. However, what we have developed so far is the bare minimum; we essentially have no theoretical guarantees
for how these algorithms would perform in practice. Here are some questions we have yet to answer: is $\pi$ in fact the \textit{unique} invariant distribution, or should we be worried
we might actually be sampling from some other invariant distribution? If we initialize one of these sampling algorithms with $X_0 \sim \nu_0$ and then iterate 
$\nu_{k} = \nu_0 P^k$, then will $\nu_k \to \pi$ in some sense? If so, how long will it take for $\nu_k \approx \pi$? And if a certain convergence threshold is reached, how will 
$\nu_k$ fluctuate around $\pi$? In particular, we are interested in approximating expectations and hence want to better understand the convergence
$\E X_k \to \E X$. These questions will be answered in the subsequent sections. 

\section{Markov Chains and Ergodicity}
In this section we begin a more formal study of convergence in order to answer the questions presented in the previous section. First, let's take a step 
back and recall the setup. We have an underlying probability space $(\Omega, \mathcal{A}, \Prob)$, and a random variable $X: \Omega \to \mathcal{X}$
with distribution $\pi = \mathcal{L}(X)$. This random variable is the main object of interest: our goal is to sample from it, approximate its expectation, 
etc. We approached this problem in the previous section by defining a Markov chain with transition kernel $P$ on the state space $\mathcal{X}$ with stationary distribution 
$\pi$. By choosing a starting distribution $\nu_0$ and iterating $\nu_k = \nu_0 P^k$ we can obtain a sequence of samples $X_k \sim \nu_k$. For sampling purposes, 
we will obviously want some sort of guarantee of convergence $\nu_k \to \pi$. We will thus need to develop a notion of limits of probability distributions. 
Next considering the goal of 
approximating expectations, a natural choice of estimator to approximate
\[\E_\pi f(X) = \int_{\mathcal{X}} f(x) \mu(dx)\]
is 
\[\frac{1}{k} \sum_{i = 1}^{k} f(X_i)\]
We will thus want to develop a Law of Large Numbers (LLN) type result: 
\[\frac{1}{k} \sum_{i = 1}^{k} f(X_i) \to \E_\pi f(X)\]
The theoretical challenge of course is that the samples $X_k$ are not independent, so the results will be a bit more involved than in the classical independent 
case. We begin this section by developing some of the theory necessary to answer these questions. 




\end{document}

