\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Notes on Markov Chains, Stochastic Stability, and MCMC
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Section: Beyond Independent Sampling
\section{Beyond Independent Sampling}
Discuss some of the shortfalls of Accept/Reject and other independent sampling methods here. 

% Section: Markov Chains
\section{Markov Chains}
\subsection{Defining Markov Chains}
\subsubsection{Measure Theoretic Setup}
Let $(\Omega, \mathcal{A}, \Prob)$ be a measure space. Although we will rarely mention this space going forward, it it important to remember that it is always
underlying all of the theory we develop. Now, consider a measurable space $(\mathcal{X}, \mathcal{F})$ which will call a \textit{state space}. $\mathcal{X}$ is a set of states that the Markov 
Chain can assume, and $\mathcal{F}$ contains all of the subsets of the state space to which we will be able to assign probabilities. We will typically restrict ourselves to 
$(\mathcal{X}, \mathcal{F}) = (\R^n, \mathcal{B})$.
We consider a discrete-time stochastic process $\mathbb{X} = \{X_k\}_{k = 1}^{\infty}$ defined on the state space $(\mathcal{X}, \mathcal{F})$. That is, each $X_k$ is a random variable
$X: \Omega \to \mathcal{X}$. So for $A \in \mathcal{F}$, the event $\{X_k \in A\}$ means ``at time step k, the state of the chain is somewhere in A''. Since the system is random, we 
can only talk about the probability of this event occurring, and the probability measure $\Prob$ assigns such a probability. It is important to emphasis that all of the $X_k$ are defined on 
the \textit{same} underlying probability space; that is, the source of randomness is the same for the system as a whole.

\subsubsection{The Markov Property}
\textbf{TODO}

\subsubsection{Homogenous Chains}
\textbf{TODO}

\subsubsection{Transition Kernels}
Under the assumption of homogeneity, a Markov chain is completely defined by its initial distribution $\nu_0 = \mathcal{L}(X_0)$ (this notation means the distribution or 
\textit{law} of $X_0$) and its \textbf{transition probability kernel} $P$. (include definition)

\subsection{Stationary Distributions}
The basic idea of MCMC is to construct a Markov Chain with stationary distribution equal to the target distribution, and then sample using this chain. 
Thus, it is first necessary to state some basic results regarding stationary distributions of Markov chains. 
\begin{definition}
A probability distribution $\nu$ is said to be \textbf{stationary} (equivalently, \textbf{invariant}) with respect to a homogenous Markov chain with transition kernel $P$ provided
that $\nu P = \nu$. 
\end{definition}
In general, invariant distributions are not unique. Thus, when we're designing Markov chains with an intended invariant distribution in mind, we will have to prove uniqueness
(otherwise we might be sampling from the wrong distribution). Since invariant distributions will be central in our design of sampling algorithms, it will be essential to check whether 
a given distribution is invariant or not. Unfortunately, checking $\nu P = \nu$ involves evaluating a potentially difficult integral. Fortunately, there is a very convenient sufficient condition 
that's much easier to check. To define this condition, we'll first need to define one more concept. But first, let's try to intuitively motivate this condition. Invariance says that if we make one 
time step forward then the distribution of the chain does not change. Now, consider a chain that runs the same forward as it does in reverse. Intuitively, this seems to be a stronger condition than invariance and will indeed see that this reversibility condition is the sufficient condition we seek. 

\begin{definition}
Define the product measure $\mu \otimes P$ on $(\mathcal{X} \times \mathcal{X}, \mathcal{F} \times \mathcal{F})$ by: 
\[(\mu \otimes P)(A \times B) := \int_{A \times B} \mu(dx) P(x, dy)\]
for $A, B \in \mathcal{F}$
\end{definition}
To get a better sense of this, first consider fixing $x$. Then integrating over $y$ gives the probability of transitioning from $x$ to $B$, weighted by 
the probability of being at $x$. Do this 
for all $x \in A$ and you have the probability of transitioning from $A$ to $B$ (again, weighted appropriately). So we can interpret 
$(\mu \otimes P)(A \times B)$ as ``the probability of observing a chain that is currently in A and then one step later is in B'' (note that 
``currently'' means with respect to the distribution $\mu$). Or put more simply, we're just considering the joint distribution between 
a state and the subsequent state. Now we arrive at the sufficient condition hinted at before. 

\begin{definition}
We say that a chain satisfies the \textbf{detailed balance} condition with respect to distribution $\mu$ provided 
\[(\mu \otimes P)(A \times B) = (\mu \otimes P)(B \times A)\]
for all $A, B \in \mathcal{F}$. In this case, we say that the chain is \textbf{reversible} with respect to $\mu$. 
\end{definition}

The following proposition simply verifies our previous intuition that reversibility should be a sufficient condition for invariance. 
\begin{prop}
If a chain is reversible with respect to $\nu$ (that is, satisfies detailed balance), then it is invariant with respect to $\nu$. 
\end{prop}

% Section: MCMC
\section{Markov Chain Monte Carlo (MCMC)}

\subsection{Motivation: Bayesian Inference}
Although MCMC has its origins in physics and finds applications in various domains, it has become known largely for its use in 
Bayesian statistics. In Bayesian inference, we seek to summarize the distribution of the parameter of interest $x$ conditional on observed 
data: 
\[\pi(x) := \pi(x|D) = \frac{p(D|x)\pi_0(x)}{\pi(D)}\]
$\pi(x|D)$ is known as the posterior distribution, $p(D|x)$ the likelihood (this is the model for our data; for example, a regression model), 
$\pi_0(x)$ the prior density, and $\pi(D)$ the marginal likelihood. For complicated distributions, our best bet is to use sampling. The typical 
situation is that $\pi(D)$ is intractable and thus can't be calculated analytically so we only have access to the unnormalized posterior 
through pointless evaluation of $p(D|x)\pi_0(x)$. In this case, the posterior will be the target distribution for our MCMC algorithms. 
Throughout these notes, I will always denote the target distribution as $\pi$. 

\subsection{Simple Monte Carlo}
\textbf{TODO}: approximating expectations, LLN, CLT, Monte Carlo error on order of $\frac{1}{\sqrt{k}}$

\subsubsection{The Big Picture}
MCMC tries to address some of the shortfalls of simple Monte Carlo by ditching independent samples and exploiting dependence 
to produce a more efficient sampling scheme. In particular, MCMC constructs a Markov chain $\mathbb{X} = \{X_k\}$ with unique invariant 
distribution equal to the target distribution $\pi$. The chain also of course must be initialized with some starting distribution $X_0 \sim \pi_0$.
Here is a high-level overview of how we will proceed in analyzing this approach: 
\begin{enumerate}
\item Construct a Markov chain with \textit{unique} stationary distribution $\pi$. This is a necessary first step, but it leaves many open questions: 
will the chain actually converge to the stationary distribution? (that is, is the limiting distribution equal to the stationary distribution?) How fast will 
it converge? Can we quantify the fluctuations around the limit? 
\item To first answer the most basic questions of convergence, we will develop a Law of Large Numbers (LLN) for Markov chains, analogous to the 
standard LLN for independent samples that guarantees convergence for simple Monte Carlo. 
\item Next we will want to develop more quantitative results; that is, to quantify the fluctuations in the limiting process. This will come in the form of a 
type of Central Limit Theorem (CLT) for Markov chains, which will allow us to calculate confidence intervals. 
\end{enumerate} 

\subsection{Step 1: Constructing a Markov Chain with Stationary Distribution $\pi$}
When we assume that the target distribution $\pi$ has a density I will denote this density by $h_\pi$. 

\subsubsection{Metropolis-Hastings}
We begin with one of the most well-known and widely used MCMC algorithms, Metropolis-Hastings (MH). Instead of just stating the algorithm, I will 
try to build up to it to hopefully give more of a sense of discovery. First, let's recall the basic structure of one of the fundamental independent sampling
techniques: the Accept-Reject algorithm. In this method, a sample is \textit{proposed} by sampling from some distribution from which we already know how to 
sample. Next, the sample is either accepted or rejected depending on some criteria. Let's adopt the same basic structure for our new method.

For now, let's assume densities exist to keep things simple - we will generalize this after. Let $q(x, \cdot)$ denote the density of the \textit{proposal distribution}; 
that is $q(x, y)$ is proportional to the probability of proposing a new state $y \in \mathcal{X}$ given current state $x \in \mathcal{X}$. Recall from Accept-Reject that 
once a new value was proposed then it was either accepted or rejected according to some decision rule. Let's adopt that idea here and let $\alpha(x, y)$ denote 
the probability of accepting the new state $y$ given the current state $x$. Note that this is completely separate from the probability of proposing $y$ given $x$, but 
we still allow the acceptance probability to depend on the previous state. Therefore, the value $q(x, y)\alpha(x, y)$ is proportional to the probability of transitioning 
to $y$ from $x$. 

Now, let's take a step back and recall our goal: to construct a Markov chain (that is, a transition kernel) $P$ with stationary distribution $\pi$. From the detailed balance 
condition we know that if
\[(\pi \otimes P)(A \times B) = (\pi \otimes P)(B \times A) \text{ for all } A, B \in \mathcal{F}\]
then $P$ is invariant with respect to $\pi$. Thus, we seek to construct $P$ such that this equality is satisfied. But, first we can simplify this expression. We have 
assumed densities exist so we can re-write this as: 
\[h_\pi (x)P(x, y) = h_\pi (y)P(y, x) \text{ for all } x, y \in \mathcal{X}\]
And finally, we have assumed some specific structure on $P$: 
\[h_\pi (x)q(x, y)\alpha(x, y)= h_\pi (y)q(y, x)\alpha(y, x) \text{ for all } x, y \in \mathcal{X}\]
Let's suppose $q$ is fixed; that is, the choice of proposal is a decision we'll have to make when implementing MH in a specific setting. This leaves us to 
define the acceptance probability $\alpha$ such that the above equation is satisfied. Rearranging, 
\[\frac{\alpha(x, y)}{\alpha(y, x)} = \frac{h_\pi(y)q(y, x)}{h_\pi(x)q(x, y)}\]
Note that we have the additional constraint that the acceptance probability must of course be bounded in $[0, 1]$. Thus, letting $\psi$ denote the right-hand side 
of this equation, we can set $\alpha$ as follows:
\[\text{If } \psi \leq 1 \text{, set } \alpha(x, y) := \psi \text{ and } \alpha(y, x) := 1\]
\[\text{If } \psi > 1 \text{, set } \alpha(x, y) := 1 \text{ and } \alpha(y, x) := \frac{1}{\psi}\]
That is, 
\[\alpha(x, y) := \min\left\{\frac{h_\pi(y) q(y, x)}{h_\pi(x)q(x, y)}, 1\right\}\]

\end{document}

