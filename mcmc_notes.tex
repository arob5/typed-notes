\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Notes on Markov Chains, Stochastic Stability, and MCMC
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Section: Beyond Independent Sampling
\section{Beyond Independent Sampling}
Discuss some of the shortfalls of Accept/Reject and other independent sampling methods here. 

% Section: Markov Chains
\section{Markov Chains}
\subsection{Defining Markov Chains}
\subsubsection{Measure Theoretic Setup}
Let $(\Omega, \mathcal{A}, \Prob)$ be a measure space. Although we will rarely mention this space going forward, it it important to remember that it is always
underlying all of the theory we develop. Now, consider a measurable space $(\mathcal{X}, \mathcal{F})$ which will call a \textit{state space}. $\mathcal{X}$ is a set of states that the Markov 
Chain can assume, and $\mathcal{F}$ contains all of the subsets of the state space to which we will be able to assign probabilities. We will typically restrict ourselves to 
$(\mathcal{X}, \mathcal{F}) = (\R^n, \mathcal{B})$.
We consider a discrete-time stochastic process $\mathbb{X} = \{X_k\}_{k = 1}^{\infty}$ defined on the state space $(\mathcal{X}, \mathcal{F})$. That is, each $X_k$ is a random variable
$X: \Omega \to \mathcal{X}$. So for $A \in \mathcal{F}$, the event $\{X_k \in A\}$ means ``at time step k, the state of the chain is somewhere in A''. Since the system is random, we 
can only talk about the probability of this event occurring, and the probability measure $\Prob$ assigns such a probability. It is important to emphasis that all of the $X_k$ are defined on 
the \textit{same} underlying probability space; that is, the source of randomness is the same for the system as a whole.

\subsubsection{The Markov Property}
\textbf{TODO}

\subsubsection{Homogenous Chains}
\textbf{TODO}

\subsubsection{Transition Kernels}
Under the assumption of homogeneity, a Markov chain is completely defined by its initial distribution $\nu_0 = \mathcal{L}(X_0)$ (this notation means the distribution or 
\textit{law} of $X_0$) and its \textbf{transition probability kernel} $P$. (include definition)

\subsection{Stationary Distributions}
The basic idea of MCMC is to construct a Markov Chain with stationary distribution equal to the target distribution, and then sample using this chain. 
Thus, it is first necessary to state some basic results regarding stationary distributions of Markov chains. 
\begin{definition}
A probability distribution $\nu$ is said to be \textbf{stationary} (equivalently, \textbf{invariant}) with respect to a homogenous Markov chain with transition kernel $P$ provided
that $\nu P = \nu$. 
\end{definition}
In general, invariant distributions are not unique. Thus, when we're designing Markov chains with an intended invariant distribution in mind, we will have to prove uniqueness
(otherwise we might be sampling from the wrong distribution). Since invariant distributions will be central in our design of sampling algorithms, it will be essential to check whether 
a given distribution is invariant or not. Unfortunately, checking $\nu P = \nu$ involves evaluating a potentially difficult integral. Fortunately, there is a very convenient sufficient condition 
that's much easier to check. To define this condition, we'll first need to define one more concept. But first, let's try to intuitively motivate this condition. Invariance says that if we make one 
time step forward then the distribution of the chain does not change. Now, consider a chain that runs the same forward as it does in reverse. Intuitively, this seems to be a stronger condition than invariance and will indeed see that this reversibility condition is the sufficient condition we seek. 

\begin{definition}
Define the product measure $\mu \otimes P$ on $(\mathcal{X} \times \mathcal{X}, \mathcal{F} \times \mathcal{F})$ by: 
\[(\mu \otimes P)(A \times B) := \int_{A \times B} \mu(dx) P(x, dy)\]
for $A, B \in \mathcal{F}$
\end{definition}
To get a better sense of this, first consider fixing $x$. Then integrating over $y$ gives the probability of transitioning from $x$ to $B$, weighted by 
the probability of being at $x$. Do this 
for all $x \in A$ and you have the probability of transitioning from $A$ to $B$ (again, weighted appropriately). So we can interpret 
$(\mu \otimes P)(A \times B)$ as ``the probability of observing a chain that is currently in A and then one step later is in B'' (note that 
``currently'' means with respect to the distribution $\mu$). Or put more simply, we're just considering the joint distribution between 
a state and the subsequent state. Now we arrive at the sufficient condition hinted at before. 

\begin{definition}
We say that a chain satisfies the \textbf{detailed balance} condition with respect to distribution $\mu$ provided 
\[(\mu \otimes P)(A \times B) = (\mu \otimes P)(B \times A)\]
for all $A, B \in \mathcal{F}$. In this case, we say that the chain is \textbf{reversible} with respect to $\mu$. 
\end{definition}

The following proposition simply verifies our previous intuition that reversibility should be a sufficient condition for invariance. 
\begin{prop}
If a chain is reversible with respect to $\nu$ (that is, satisfies detailed balance), then it is invariant with respect to $\nu$. 
\end{prop}

% Section: MCMC
\section{Markov Chain Monte Carlo (MCMC)}

\subsection{Motivation: Bayesian Inference}
Although MCMC has its origins in physics and finds applications in various domains, it has become known largely for its use in 
Bayesian statistics. In Bayesian inference, we seek to summarize the distribution of the parameter of interest $x$ conditional on observed 
data: 
\[\pi(x) := \pi(x|D) = \frac{p(D|x)\pi_0(x)}{\pi(D)}\]
$\pi(x|D)$ is known as the posterior distribution, $p(D|x)$ the likelihood (this is the model for our data; for example, a regression model), 
$\pi_0(x)$ the prior density, and $\pi(D)$ the marginal likelihood. For complicated distributions, our best bet is to use sampling. The typical 
situation is that $\pi(D)$ is intractable and thus can't be calculated analytically so we only have access to the unnormalized posterior 
through pointless evaluation of $p(D|x)\pi_0(x)$. In this case, the posterior will be the target distribution for our MCMC algorithms. 
Throughout these notes, I will always denote the target distribution as $\pi$. 

\subsection{Simple Monte Carlo}
\textbf{TODO}: approximating expectations, LLN, CLT, Monte Carlo error on order of $\frac{1}{\sqrt{k}}$

\subsubsection{The Big Picture}
MCMC tries to address some of the shortfalls of simple Monte Carlo by ditching independent samples and exploiting dependence 
to produce a more efficient sampling scheme. In particular, MCMC constructs a Markov chain $\mathbb{X} = \{X_k\}$ with unique invariant 
distribution equal to the target distribution $\pi$. The chain also of course must be initialized with some starting distribution $X_0 \sim \pi_0$.
Here is a high-level overview of how we will proceed in analyzing this approach: 
\begin{enumerate}
\item Construct a Markov chain with \textit{unique} stationary distribution $\pi$. This is a necessary first step, but it leaves many open questions: 
will the chain actually converge to the stationary distribution? (that is, is the limiting distribution equal to the stationary distribution?) How fast will 
it converge? Can we quantify the fluctuations around the limit? 
\item To first answer the most basic questions of convergence, we will develop a Law of Large Numbers (LLN) for Markov chains, analogous to the 
standard LLN for independent samples that guarantees convergence for simple Monte Carlo. 
\item Next we will want to develop more quantitative results; that is, to quantify the fluctuations in the limiting process. This will come in the form of a 
type of Central Limit Theorem (CLT) for Markov chains, which will allow us to calculate confidence intervals. 
\end{enumerate} 

\subsection{Step 1: Constructing a Markov Chain with Stationary Distribution $\pi$}
When we assume that the target distribution $\pi$ has a density with respect to a measure $\mu$ (typically the Lebesgue or counting measure).
I will denote this density by $h_\pi$. 

\subsubsection{Metropolis-Hastings}
We begin with one of the most well-known and widely used MCMC algorithms, Metropolis-Hastings (MH). Instead of just stating the algorithm, I will 
try to build up to it to hopefully give more of a sense of discovery. First, let's recall the basic structure of one of the fundamental independent sampling
techniques: the Accept-Reject algorithm. In this method, a sample is \textit{proposed} by sampling from some distribution from which we already know how to 
sample. Next, the sample is either accepted or rejected depending on some criteria. Let's adopt the same basic structure for our new method.

For now, let's assume densities exist to keep things simple - we will generalize this after. Let $q(x, \cdot)$ denote the density of the \textit{proposal distribution}; 
that is $q(x, y)$ is proportional to the probability of proposing a new state $y \in \mathcal{X}$ given current state $x \in \mathcal{X}$. Recall from Accept-Reject that 
once a new value was proposed then it was either accepted or rejected according to some decision rule. Let's adopt that idea here and let $\alpha(x, y)$ denote 
the probability of accepting the new state $y$ given the current state $x$. Note that this is completely separate from the probability of proposing $y$ given $x$, but 
we still allow the acceptance probability to depend on the previous state. Therefore, the value $q(x, y)\alpha(x, y)$ is proportional to the probability of transitioning 
to $y$ from $x$. 

Now, let's take a step back and recall our goal: to construct a Markov chain (that is, a transition kernel) $P$ with stationary distribution $\pi$. From the detailed balance 
condition we know that if
\[(\pi \otimes P)(A \times B) = (\pi \otimes P)(B \times A) \text{ for all } A, B \in \mathcal{F}\]
then $P$ is invariant with respect to $\pi$. Thus, we seek to construct $P$ such that this equality is satisfied. But, first we can simplify this expression. We have 
assumed densities exist so we can re-write this as: 
\[h_\pi (x)P(x, y) = h_\pi (y)P(y, x) \text{ for all } x, y \in \mathcal{X}\]
And finally, we have assumed some specific structure on $P$: 
\[h_\pi (x)q(x, y)\alpha(x, y)= h_\pi (y)q(y, x)\alpha(y, x) \text{ for all } x, y \in \mathcal{X}\]
Let's suppose $q$ is fixed; that is, the choice of proposal is a decision we'll have to make when implementing MH in a specific setting. This leaves us to 
define the acceptance probability $\alpha$ such that the above equation is satisfied. Rearranging, 
\[\frac{\alpha(x, y)}{\alpha(y, x)} = \frac{h_\pi(y)q(y, x)}{h_\pi(x)q(x, y)}\]
Note that we have the additional constraint that the acceptance probability must of course be bounded in $[0, 1]$. Thus, letting $\psi$ denote the right-hand side 
of this equation, we can set $\alpha$ as follows:
\[\text{If } \psi \leq 1 \text{, set } \alpha(x, y) := \psi \text{ and } \alpha(y, x) := 1\]
\[\text{If } \psi > 1 \text{, set } \alpha(x, y) := 1 \text{ and } \alpha(y, x) := \frac{1}{\psi}\]
That is, 
\[\alpha(x, y) := \min\left\{\frac{h_\pi(y) q(y, x)}{h_\pi(x)q(x, y)}, 1\right\}\]
Thus, with $\alpha$ defined by this formula it follows from detailed balance that $\pi$ is a stationary distribution of the Markov chain, as desired. This result is a necessary 
first step in designing MCMC algorithms, but let's be clear on what it \textit{doesn't} mean. We have not concluded any results that guarantee that iterating through time 
steps will eventually yield convergence to this stationary distribution, and even if there is convergence we have no idea how quickly the rate of convergence would be. This 
is a serious issue: if we're generating samples at each time step, we have no idea whether the samples are actually following the distribution of interest or not. We will address
this issues later.

Before proceeding, one more remark: did we really need the detailed balance condition to arrive at this acceptance probability? Could we just have used the definition of invariance instead?
Let's try. By definition $P$ is invariant with respect to $\pi$ if $\pi P = \pi$; that is $(\pi P)(A) = \pi(A)$ for all $A \in \mathcal{F}$ or 
\[\pi(A) = \int P(x, A) \pi(dx) \text{ for all } A \in \mathcal{F}\]
Using our assumption that densities exist and the specific structure we have chosen for $P$ we can re-write this as
\[h_\pi(y) = \int q(x, y) \alpha(x, y) h_\pi(x) dx\]
At this point we are kind of stuck: to solve for the $\alpha$ that satisfies this equality we would have to evaluate the integral on the right-hand side. We now see the power 
of the detailed balance condition: it only requires us to check transition probabilities between \textit{two} (arbitrary) states: $x$ and $y$. Using the definition of invariance
directly requires us to consider \textit{all} of the possible ways to get to $y$ from the current distribution $\pi$, hence the integral over $x$. Thus, the detailed balance condition
is much easier to check. 

\bigskip

Thus far, we have talked loosely about how we have defined a Markov chain by considering a kernel of the form ``first propose a new state and then accept with some 
probability''. However, we have not actually formally defined the kernel or proved that the Markov property is satisfied. The latter result should be pretty obvious: both the 
proposal and the acceptance probability only depend on the current state. Now let's consider the formal definition of the kernel. Recall, that $P(x, \cdot)$ must be a probability 
measure such that $P(x, A)$ is the probability of transitioning to $A \in \mathcal{F}$ from state $x \in \mathcal{X}$. One key thing to realize here is that there are two ways 
to end up in $A$: 
\begin{enumerate} 
\item Propose and accept a state in $A$. 
\item Reject the proposed state, but $x$ is already in $A$. 
\end{enumerate}
The probability of 1 can be found by integrating over all ways of proposing/accepting a state in $A$: 
\[\int_{A} q(x, y)\alpha(x, y)\mu(dy)\]
Recall that the densities here are all with respect to the measure $\mu$. The probability of 2 depends on whether or not $x \in A$ so we utilize the delta function 
$\delta_x(A)$ which assumes the value $1$ if $x \in A$ and $0$ otherwise. We also integrate over the whole state space this time as we are considering all of the ways
that a proposed state might be rejected. 
\[\delta_x(A)\int_\mathcal{X} q(x, y)[1 - \alpha(x, y)]\mu(dy)\]
Denoting this second integral by $\overline{\alpha}(x)$ we thus have the full kernel: 
\[P(x, A) = \int_{A} q(x, y)\alpha(x, y)\mu(dy) + \overline{\alpha}(x) \delta_x(A)\]
Many of the common MCMC algorithms fall under this MH umbrella; different choices of proposal kernel yields different algorithms. We briefly detail a few of the well-known 
MH algorithms here. 

\bigskip

\textbf{Independent Proposals.} \textbf{TODO} 

\bigskip

\textbf{Random Walk Monte Carlo.} RWMH uses a proposal defined by
\[X_{t + 1} = X_t + \sigma \xi \]
where $\sigma > 0$ and $\xi \sim \mu$, for some symmetric distribution $\mu$. A common choice for $\mu$ is the standard normal distribution, in which case the next 
state is simply given by $X_{t + 1} \sim N(X_t, \sigma^2 I_d)$. Therefore, the proposal step acts as a random walk, though note that the Markov chain as a whole does not 
exhibit this behavior due to the acceptance step. Assuming $\mu$ has a density (and denoting the density by $\mu$ as well) then let's derive the proposal density. 
First consider
\begin{align*}
F_{X_{t + 1}|X_t=x}(y) = \Prob(X_{t + 1} \leq y|X_t=x) &= \Prob(x + \sigma \xi \leq y) \\
									 	 &= \Prob\left(\xi \leq \frac{y - x}{\sigma}\right) \\
									 	 &= F_{\xi}\left(\frac{y - x}{\sigma}\right) \\
\end{align*}
Thus the proposal density is given by, 
\[q(x, y) = F_{X_{t + 1}|X_t=x}^\prime(y) \frac{d}{dy}F_{\xi}^\prime \left(\xi \leq \frac{y - x}{\sigma}\right) = \frac{1}{\sigma^d}\mu\left(\frac{y - x}{\sigma}\right)\]
which follows from the chain rule. Moreover, by the assumed symmetry of $\mu$, 
\[q(x, y) = \sigma^{-d} \mu\left(\frac{y - x}{\sigma}\right) = \sigma^{-d} \mu\left(\frac{x - y}{\sigma}\right) = q(y, x)\]

This symmetry simplifies the acceptance ratio as follows.  
\[\alpha(x, y) := \min\left\{\frac{h_\pi(y) q(y, x)}{h_\pi(x)q(x, y)}, 1\right\} = \min\left\{\frac{h_\pi(y)}{h_\pi(x)}, 1\right\} \]
Therefore, the acceptance step only depends on the ratio of the target densities at the current and proposed states. If the proposed state has a higher density then it will 
always be accepted. Otherwise, it will only be accepted with some probability that depends on the ratio of densities. Given this behavior, one concern might be that the 
chain will ``get trapped'' in high density areas. This certainly could be an issue, and one way to address this is to choose a heavy-tailed proposal to increase the probability
of ``escaping'' these high density areas. Another potentially challenging issue is choosing the \textit{scale} of the proposal; that is, how should we set $\sigma$? This is 
analogous to the question of how to set the step-size parameter in gradient descent. If we choose $\sigma$ too small in the sense that the variance of the proposal is much 
smaller than the variance of $\pi$, then it may take very large number of iterations in order to explore the support of $\pi$. 

\bigskip

\textbf{The Gibbs Sampler.} \textbf{TODO} 
Excellent StackOverflow \href{https://stats.stackexchange.com/questions/118442/does-the-gibbs-sampling-algorithm-guarantee-detailed-balance#:~:text=You\%20tried\%20to\%20show\%20detailed,detailed\%20balance\%20is\%20not\%20satisfied.}{post} that clarifies the specific sense in which Gibbs is a special case of MH, and the sense in which Gibbs satisfies detailed balance. 

Gibbs sampling is a technique that is motivated by a simple idea that pops up in many places across computational statistics and optimization: tackle a multivariate problem one dimension
at a time. In the case of sampling, this means alternating sampling from the various conditional distributions of the target distribution, thus generating new samples one component (or 
more generally one subset of components) at a time. This straightforward idea immediately raises some questions, the most important being: does this even work? In other words, 
if we iterate the procedure of sampling one component at a time from the conditional distributions, will the resulting samples tend towards an invariant distribution equal to the target 
distribution? As we will see below, the answer is not always yes, but under some basic assumptions this will hold. Secondly, is this even useful? The answer to this question is very 
application-dependent, but in general, yes. Gibbs sampling is only helpful if we know how to sample from the conditional distributions. It turns out that there are quite a few problems
of interest where the target distribution is quite complicated, but the conditional distributions can be explicitly calculated and sampled from. This situation shows up often in Bayesian 
hierarchical models, in which we often set up the hierarchical model by defining the conditional distributions of interest. 



\bigskip

\textbf{Langevin Monte Carlo.} Our goal is to construct a Markov chain that will converge as quickly as possible to the stationary distribution. In the above proposals, the 
accept-reject mechanism ($\alpha$) seems to be carrying most of the load as to ensuring ``smart'' decisions as to where to transition next. Though this mechanism does 
only use two points of information about the target density in making the decision: the density at the current state and the density at the proposed state. The accept-reject 
mechanism accepts or rejects based on the ratio of densities at this point (along with an adjustment for the proposal density as well). Given that the other half of the 
transition kernel is the proposal kernel, a natural question is to whether we can design a more sophisticated proposal that helps produce better informed proposals and 
hence speeds convergence. So far, the proposals have not been very informed; take RWMH for instance: the proposal is just a random draw from some distribution 
centered at the current state. For those familiar with optimization, we know that leveraging \textit{gradient} information of an objective function is incredibly 
useful in designing optimization algorithms, the most obvious example being gradient descent. It is thus natural to wonder if we might leverage gradient information 
of a differentiable target density. We might call such a proposal a \textit{local optimization proposal}. With this in mind, consider a proposal defined by: 
\[X_{t + 1} := X_{t} + \frac{\sigma}{2} \nabla \log \pi(X_{t - 1}) + \sigma Z\]
where $Z \sim N(0, I_d)$. Let's break this down. \textbf{TODO: complete}


In summary, this section has introduced how to construct a transition kernel that is invariant with respect to the target distribution $\pi$. In doing so we have introduced 
a variety of algorithms that fall in the Metropolis-Hastings family. However, what we have developed so far is the bare minimum; we essentially have no theoretical guarantees
for how these algorithms would perform in practice. Here are some questions we have yet to answer: is $\pi$ in fact the \textit{unique} invariant distribution, or should we be worried
we might actually be sampling from some other invariant distribution? If we initialize one of these sampling algorithms with $X_0 \sim \nu_0$ and then iterate 
$\nu_{k} = \nu_0 P^k$, then will $\nu_k \to \pi$ in some sense? If so, how long will it take for $\nu_k \approx \pi$? And if a certain convergence threshold is reached, how will 
$\nu_k$ fluctuate around $\pi$? In particular, we are interested in approximating expectations and hence want to better understand the convergence
$\E X_k \to \E X$. These questions will be answered in the subsequent sections. 

\section{Markov Chains and Ergodicity}
In this section we begin a more formal study of convergence in order to answer the questions presented in the previous section. First, let's take a step 
back and recall the setup. We have an underlying probability space $(\Omega, \mathcal{A}, \Prob)$, and a random variable $X: \Omega \to \mathcal{X}$
with distribution $\pi = \mathcal{L}(X)$. This random variable is the main object of interest: our goal is to sample from it, approximate its expectation, 
etc. We approached this problem in the previous section by defining a Markov chain with transition kernel $P$ on the state space $\mathcal{X}$ with stationary distribution 
$\pi$. By choosing a starting distribution $\nu_0$ and iterating $\nu_k = \nu_0 P^k$ we can obtain a sequence of samples $X_k \sim \nu_k$. For sampling purposes, 
we will obviously want some sort of guarantee of convergence $\nu_k \to \pi$. We will thus need to develop a notion of limits of probability distributions. 
Next considering the goal of 
approximating expectations, a natural choice of estimator to approximate
\[\E_\pi f(X) = \int_{\mathcal{X}} f(x) \mu(dx)\]
is 
\[\frac{1}{k} \sum_{i = 1}^{k} f(X_i)\]
We will thus want to develop a Law of Large Numbers (LLN) type result: 
\[\frac{1}{k} \sum_{i = 1}^{k} f(X_i) \to \E_\pi f(X)\]
The theoretical challenge of course is that the samples $X_k$ are not independent, so the results will be a bit more involved than in the classical independent 
case. We begin this section by developing some of the theory necessary to answer these questions. 

\subsection{Qualitative Convergence}
By ``qualitative convergence'' we refer to convergence results that do not quantify the fluctuations that a sequence exhibits around its limit; instead, we simply
seek to define the binary notion of whether a sequence converges or not. In general, we will consider different notions of what it means for a sequence of random 
variables to converge, but to begin we instead consider a sequence of distributions. 

\begin{definition}
A sequence of probability measures $(\nu_k)$ are said to \textbf{converge weakly} to a probability measure $\nu$, written $\nu_k \overset{w}{\to} \nu$, 
provided $\E_{\nu_k} f(X) \to \E_\nu f(X)$ for all bounded, continuous functions $f$. Put differently, the sequence of real numbers
\[\int f(x) \nu_k(dx) \]
converges pointwise to 
\[\int f(x) \nu(dx)\]
\end{definition}
We have talked about wanting to approximate expectations of functions of the random variable $X$, so having a definition of convergence that says 
``the expectations of all (continuous and bounded) functions converge to the expectation with respect to the limiting distribution'' does indeed seem like 
a useful concept. We should be very clear here that we're considering a sequence of distributions, not random variables. The definition involves taking 
expectations with respect to these different distributions and considering how these expectations converge. One cool thing about this is that it allows us to 
define what it means for a sequence of random variables to converge even if they are not defined on the same probability space (but note that for are purposes 
all of the random variables are defined on the same space). 
\begin{definition}
A sequence of random variables $(X_k)$ is said to \textbf{converge in distribution} to a random variable $X$, written $X_k \overset{d}{\to} X$, provided that 
$\mathcal{L}(X_k) \overset{w}{\to} \mathcal{L}(X)$. 
\end{definition}

If the random variables are all defined on the same space, we have a few other notions of convergence. 
\textbf{TODO}: 
\begin{itemize}
\item Define convergence in prob and a.s. convergence. 
\item Mention chain of implications for different kinds of convergence. 
\end{itemize}

\subsection{Ergodicity and the Law of Large Numbers}
Recall the classical Strong Law of Large Numbers (SLLN). 
\begin{thm}
Let $(X_k)$ be a sequence of iid random variables with $\E \abs{X_1} < \infty$ (that is, $X_k \in L_1$ for all $k$). Then 
$\overline{X}_N := \frac{1}{N} \sum_{k = 1}^{N} X_k \overset{\text{a.s.}}{\to} \E X_1$. 
\end{thm} 
Note that this implies $f(\overline{X}_N) \overset{\text{a.s.}}{\to} \E f(X_1)$ provided $\E \abs{f(X_1)} < \infty$. Now, the challenge is to derive an analogous result
that applies to the current setting, where the $(X_k)$ are states of the Markov chain and are certainly not independent. Unsurprisingly, in order to conclude a useful result
in this more complicated setting, we will require some additional assumptions; namely, the concept of \textit{ergodicity}. 

\subsubsection{Motivating Ergodicity}
The concept of ergodicity is in general defined for dynamical systems. Markov chains can be formulated as dynamical systems, and therefore the definition we will
give for ergodicity here is really a special case of the more general concept. To motivate why we even need this concept let's think about what challenges we might 
run into when trying to generalize the LLN to this setting. In the independent case, each sample is by definition from the distribution of interest. In the Markov chain case, 
we are sampling from a sequence of distributions that converge to the distribution of interest. One cause for concern is that the sequence constructed from the Markov chain 
gets ``stuck'' in some subset of $\mathcal{X}$ and thus fails to explore the entire space. More generally, we might worry that certain trajectories of the chain might err and 
not sample according to the target distribution. In practice, we want to run the algorithm (that is, generate a single trajectory of the system) and have confidence that the trajectory 
is tending to the desired distribution. Put differently, we hope that the time average of any specific trajectory is equal to the ``ensemble'' average, or the average tendency of the system
as a whole. This is the essence of \textit{ergodicity}, that the time average of any trajectory equals the ensemble average. More concretely, we say a Markov chain with stationary 
distribution $\pi$ is ergodic if, for any $x \in \mathcal{X}$, the probability of transitioning to the set $A \in \mathcal{F}$, in time, tends towards $\pi(A)$. 

\begin{definition}
A Markov chain with transition kernel $P$ and invariant distribution $\pi$ is said to be \textbf{ergodic} provided $\lim_{k \to \infty} P^k(x, A) = \pi(A)$ for all 
$x \in \mathcal{X}$ and all $A \in \mathcal{F}$. 
\end{definition}

Intuitively, this means that the samples we generate will be distributed according to distributions that are approaching the target distribution $\pi$ as $k$ grows large. 
This definition should not sound too crazy; we had previously constructed Markov chains with invariant distribution $\pi$, and noted that we would need to prove some 
sort of convergence result that tells us the chain actually tends towards $\pi$. Thus, ergodicity is precisely the property that we want to prove. 

\subsection{Irreducibility and LLN}
The goal is thus to consider the conditions under which a Markov chain is ergodic. A slightly easier goal is to consider the conditions under which  LLN type result 
will hold. This is not surprising: the SLLN guarantees almost sure convergence of expectations, while ergodicity takes things a step farther and considers the convergence
of distributions. We would thus expect ergodicity to imply a SLLN.  

With this in mind, let's begin contemplating some sufficient conditions for these results to hold. 

\section{Mixing and Markov Chain CLTs}
Let's review what we've accomplished so far: 
\begin{enumerate}
\item We have constructed Markov chains with stationary distribution $\pi$. 
\item We have derived a form of the LLN for Markov chains, which tells us that the sample mean calculated from the MCMC samples will converge almost surely 
to the expectation with respect to the stationary distribution $\pi$: 
\[\lim_{i \to \infty} \frac{1}{k} \sum_{i = 1}^{k} \phi(X_i) = \phi(\pi) \text{ a.s.}\]
\item We showed under which conditions the chain is \textit{geometrically ergodic}, which helped us quantify the \textit{rate} of convergence in the LLN. In particular, 
we considered the distance from $\pi$ in total variation distance: 
\[d_V(\nu P^k, \pi) \leq O(\rho^k d_V(\nu, \pi))\]
\item Finally, we considered a stronger form of convergence known as \textit{quantitative convergence}, in which we introduced the concepts of metric convergence 
and Wasserstein distance.
\end{enumerate}

This is all helpful, but we are missing one key ingredient; we have yet to consider the distribution of the fluctuations around $\pi$ as $k$ grows large, which will help us 
construct confidence intervals. In particular, letting $\hat{\pi}(\phi) := \frac{1}{k} \sum_{i = 1}^{k} \phi(X_i)$ we seek to establish the conditions under which: 
\[\sqrt{k}[\hat{\pi}(\phi) - \pi(\phi)] \overset{d}{\to} N(0, \sigma_\phi^2)\]
for some asymptotic variance $\sigma_\phi^2$. If we're able to obtain an estimate of this variance $\hat{\sigma}_\phi^2$ then we can construct the typical 
$100(1 - \alpha)\%$ confidence intervals: 
\[\left(\hat{\pi}(\phi) - \frac{z_{\alpha/2}}{\sqrt{k}}\hat{\sigma}_\phi, \hat{\pi}(\phi) + \frac{z_{\alpha/2}}{\sqrt{k}}\hat{\sigma}_\phi \right)\]
such that the probability that $\pi(\phi)$ falls in this interval tends to $1 - \alpha$ in the limit. The reason this is so valuable is that it allows us to tune the precision 
of our estimate for the expectation by adjusting the number of samples $k$. 

\section{Stochastic Differential Equations (SDEs)}
We now begin to build up the theory necessary to develop more sophisticated MCMC algorithms, in particular Langevin Monte Carlo. This algorithm is based on the 
discretization of a \textit{continuous time} stochastic process. Up until this point we've been considering Markov chains in which the index set is the natural numbers. 
We now allow this index set to be $\R$ or $\R_+$. The basic idea going forward will be to consider these ``idealized'' continuous processes as a model for MCMC algorithms. 
Then to actually be able to implement such algorithms in a computer, we discretize these idealized processes; that is, approximate them with Markov chains. Throughout this 
section we work in the Euclidean state-space $\mathcal{X} = \R^d$. 

\subsection{Review of Random Walks and Gaussian Processes}
\textbf{TODO}: Use Stochastic Processes II MIT Lecture
Comment on how we describe the probability distribution of discrete-time stochastic processes. For example, in the case of a random walk defining the distribution of length-1
increments completely describes the distribution of the process. This doesn't work for continuous time processes. 

\subsection{Transitioning from ODEs to SDEs}
Recall from ODEs the form of a (potentially nonlinear) ordinary differential equation: 
\[\frac{dx(t)}{dt} = F(x(t)) + G(x(t)) W(t)\]
We might first consider the simpler equation 
\[\frac{dx(t)}{dt} = F(x(t))\]
Now consider this: what if the process we are modeling is noisy or random in nature and thus we only want to consider $F(x(t))$ to be the 
\textit{average} value of $\frac{dx(t)}{dt}$. Well, why not add on a Gaussian white noise term? 
 \[\frac{dx(t)}{dt} = F(x(t)) + \epsilon_t, \ \epsilon_t \overset{\text{iid}}{\sim} N(0, 1)\]
This seems to be a reasonable thing to do, but note that it takes us out of the familiar realm of ODEs; we are now faced with a completely new mathematic object and 
at this point we don't even have a sense of what it would mean to ``solve'' the above equation. Generalizing one step further, we might add back in the G term: 
 \[\frac{dx(t)}{dt} = F(x(t)) + G(x(t))\epsilon_t, \ \epsilon_t \overset{\text{iid}}{\sim} N(0, 1)\]
 Now $G(x(t))$ acts as an ``amplitude'' for the noise term, magnifying or reducing the impact of the Gaussian noise over time and space. Note that this equation 
 mirrors the general equation for a non-linear ODE given above, with $W(t) = \epsilon_t$. In this case we call $W(t)$ a \textit{white noise process}. What if we did just 
 try to pave forward and tackle this equation with the ODE methods? Well, the first step is to integrate both sides with respect to $t$, so we immediately run into a problem. 
 What does it even mean to integrate $\int_{0}^{t} W(s) ds = \int_{0}^{t} \epsilon_{s} ds$? This is currently undefined. Note that we know how to integrate random variables 
 over a probability space and with respect to a probability measure, but this is not that. This is a time integral over a stochastic process.
 
 Let's start thinking about how we might find a suitable and useful definition for this integral. An immediate difficulty is that the Gaussian distribution is supported on the entire 
 real line and hence $\{W(t)\}_{t \in [a, b]}$ unbounded on any interval with $a < b$. As we try to overcome these difficulties we will be very informal, just trying to find a method 
 of attack that seems promising before putting things on formal ground. In this spirit, let's assume $B(t) := \int_{0}^{t} W(s) ds$ is a Gaussian random variable. This seems somewhat
 reasonable since any linear combination of Gaussians $\sum_{i = 1}^{N} Z_i$ is Gaussian and an integrate is like a continuous version of a sum. Under this assumption, and assuming
 we can exchange integration and expectation we have: 
 \begin{align*}
 \E B(t) &= \int_{0}^{t} \E W(s) ds = \int_{0}^{t} 0 ds = 0 \\
 \Var(B(t)) &= \int_{0}^{t} \Var(W(s)) ds = \int_{0}^{t} 1 ds = t
 \end{align*}
where to find the variance I (again without any rigorous justification) generalized the notion that the variance of a sum of independent random variables is the sum of the variances. 
Using this heuristic we've found that the variance of the process $B(t)$ increases linearly in time. Let's go a step further here and consider $W(t) \overset{\text{iid}}{\sim} N(0, I_d)$, 
a white noise process in $d$ dimensions. We'd still find $\E B(t) = 0$ (interpreting the integral component-wise) and now $\Cov(B(t))$ is the covariance matrix of $B(t)$. 
To find this covariance matrix, first note that by independence that $\E W(t)W(s)^T = \mathbb{I}[t = s]I_d$ (since $\E W(t)_i W(s)_j = 1$ $\iff$ $t = s$ and $i = j$). 
Below I consider the covariance between two different time periods; $\Cov(B(t))$ follows as a special case when $t = s$.  
\begin{align*}
\Cov(B(t), B(s))_{ij} = \left(\E B(t)B(s)^T\right)_{ij} &= \E\left[\int_{0}^{t} W(t^\prime)_i dt^\prime \cdot \int_{0}^{s} W(s^\prime)_j ds^\prime \right] \\
		    			  &= \E\left[\int_{0}^{t} \int_{0}^{s} W(t^\prime)_i W(s^\prime)_j dt^\prime ds^\prime \right] \\
					  &= \int_{0}^{t}\int_{0}^{s}  \E \left[W(t^\prime)_i W(s^\prime)_j\right] dt^\prime ds^\prime \\
					  &= \int_{0}^{t}\int_{0}^{s} \mathbb{I}[t^\prime = s^\prime] \mathbb{I}[i = j] dt^\prime ds^\prime \\
					  &= \int_{0}^{\min\{t, s\}} \mathbb{I}[i = j] dt^\prime \\
					  &= \min\{t, s\} \mathbb{I}[i = j]\\
\end{align*}
Thus, $\E B(t)B(s)^T = \min\{t, s\} I_d$. So the covariance of $B(t)$ between two time values is governed by the minimum of the two times. If we fix the smaller time 
value, say $t$, and arbitrarily increase $s$ then the covariance will remain constant. However, if we increase $t$ and $s$ by the same amount then the covariance 
will increase. In particular, $\Var(B(t)_i) = t$ so the variance increases linearly in time, which coincides with our previous calculation. We will come to know the process $B(t)$ as a \textit{Brownian motion}. As a recap, let's consider what we've found: 
\begin{align*}
&\text{White Noise Process: } \E W(t) = 0 \text{ and } \Cov(W(t), W(s)) = \mathbb{I}[t = s]I_d \\
&\text{Brownian Motion: }: \E B(t) = 0 \text{ and } \Cov(B(t), B(s)) =  \min\{t, s\} I_d
\end{align*}

\textbf{TODO: include conditional distribution of Brownian motion, and the fact that increments are independent (Notes, pg. 98}

\subsection{Different Ways to Think about Brownian Motion}
\textbf{TODO}
\begin{enumerate}
\item As solution to the differential equation presented above 
\item Continuous generalization of linear combination of Gaussians
\item Bayesian approach to modeling unknown quantities 
\item Generalization of a random walk; in particular, the limit of simple random walks (see MIT lecture)
\item As an answer to the question: how to we define a probability distribution of a continuous stochastic process? (See Stochastic Processes II MIT Lecture)
\end{enumerate}

\subsection{Returning to the Differential Equation}
Going forward, we will formulate SDEs in terms of Brownian motion instead of white noise. The reason for this will become clear later but for now I will 
just saw that (in a sense that can be made formal) white noise can be thought of as the time derivative of Brownian motion: 
\[\frac{dB(t)}{dt} = W(t)\]
I will also henceforth switch to writing the time dependence as a subscript instead of using functional notation: $B_t := B(t)$. 
Recall that there is an underlying probability space $(\Omega, \mathcal{A}, \Prob)$. We should note that, for $\omega \in \Omega$, $B_t(\omega)$
is continuous with respect to $t$ but \textit{not differentiable} (Brownian motion is ``jagged''). Thus, claiming that white noise can be thought of as the 
time derivative of Brownian motion implies that we are considering a different notion of derivatives here. This is all I will say on this matter for the time 
being. For matrices, $\Gamma, \Lambda \in \R^{d \times d}$ we recall the linear ODE:
\[\frac{dx(t)}{dt} = \Gamma x(t) + \Lambda W(t) \]
We previously considered generalizing the deterministic function $W(t)$ by considering it as a white-noise Gaussian process. We now replace this term 
using the equality $\frac{dB(t)}{dt} = W(t)$ mentioned above: 
\[\frac{dX_t}{dt} = \Gamma X_t + \Lambda \frac{dB_t}{dt} \]
This is often written 
\[dX_t = \Gamma X_t dt + \Gamma dB_t\]
We can also generalize the non-linear ODE as: 
\[dX_t = F(X_t) dt + G(X_t) dB_t\]
Things might seem a bit frustrating up to this point, as none of the above expressions have yet been given rigorous meaning. However, I believe it is helpful 
to give a general, informal flavor of what we're trying to do prior to pursuing rigor, which we will do in the next section. 

\subsection{Ito Calculus}
We seek to give formal meaning to the expressions given in the previous section. The plan of attack is to formulate SDEs as equivalent integral equations, which will 
then require defining \textit{stochastic integrals}. We will see that this approach naturally leads to the requirement that stochastic processes be integrated with respect to
 a Brownian motion. In measure theory, the Lebesgue integral is defined with respect to a measure and in the theory of differential forms, $k$-forms are the natural objects 
 of integration. Here, it turns out that Brownian motions play the central role. In general, Ito calculus can be thought of as a theory of integration of stochastic processes. 
 Even though we are interested in stochastic \textit{differential} equations, the workhorse of the theory is actually a new notion of integral. 
 
 Let us first recall the heuristic SDE we originally considered by simply supposing that $W_t \overset{iid}{\sim} N(0, 1)$. 
 \[\frac{dX_t}{dt} = F(X_t) + G(X_t) W_t\]
 It should immediately be clear that standard calculus cannot handle this expression. Even for a fixed trajectory (i.e. fix $\omega \in \Omega$), $W_t(\omega)$
 will be discontinuous (it's white noise so will bounce around). So trying to do much more with this expression seems like a dead end. Let's instead consider integrating
 both sides of this expression with respect to time $t$. This yields the integral equation
 \[X_t - X_{t_0} = \int_{t_0}^{t} F(X_t) dt + \int_{t_0}^{t} G(X_t) W_t dt\]
The first integral doesn't actually pose any major issues. Once a realization $\omega \in \Omega$ is fixed, we have a standard integral over time
\[\int_{t_0}^{t} F(X_t(\omega)) dt \]
which we can define in the Riemann or Lebesgue (with respect to Lebesgue measure) sense provided that the function $t \mapsto F(X_t(\omega))$ is reasonably well-behaved. 
It's the second integral where we run into problems:
\[\int_{t_0}^{t} G(X_t(\omega)) W_t(\omega) dt\]
To see why this is an issue, let's try to proceed by defining this as a Riemann integral. The Riemann integral is defined by considering a partition of $[t_0, t_k]$ and calculating
lower and upper Riemann sums with respect to this partition. The lower sums use the infimum of the integrand in each segment of the partition and the upper sums use 
the supremum. We then consider the supremum of the lower sums and infimum of the upper sums each taken over the set of all possible partitions. If this infimum and 
supremum are equal then the function is said to be Riemann integrable. The issue with using this formulation here is that $W_t(\omega)$ is unbounded. Since it is 
a white noise process then we can essentially make no assumptions on what a realization of it might look like. For example, it could be that $W_s(\omega) \to -\infty$
as $s \to t$. Then the infimum of the integrand over the segment of a partition containing the latter portion of this interval will be $-\infty$. Thus, we cannot define this 
as a Riemann integral. Okay, so it's time to consider generalizing; one idea is to consider the Stieltjes (which can be defined in a Riemann or Lebesgue sense). Recall 
that the Stieltjes integral looks something like 
\[\int_{a}^{b} f(x) dg(x)\]
where the $dg(x)$ indicates (in the Riemann case) that the increments in the Riemann sum $t_{i + 1} - t_i$ are first scaled by $g$: $g(t_{i + 1}) - g(t_i)$. Thus it is in fact 
a generalization of the Riemann integral. Obviously, we can't use this integral directly in our case but we can borrow the basic idea and try to consider $W_t(\omega) dt$ 
as the increment of some other stochastic process $\beta_t$ so that 
\[\int_{t_0}^{t} G(X_t) W_t dt = \int_{t_0}^{t} G(X_t) d\beta_t \]
Now recall that for the Stieltjes integral we typically take $g$ to be monotone and right semi-continuous. Now recall some facts we stated about Brownian motion in 
the previous section: its variance increases linearly and it is continuous. Following this hint, it does turn out that Brownian motion is the ``right'' process to integrate with 
respect to. Also recall that we previously mentioned that the white noise process can be thought of as the derivative of a Brownian motion. We now see why: 
to arrive at this expression we considered $W_t dt = d\beta_t$, which we can rearrange as $\frac{d\beta_t}{dt} = W_t$. 
Before getting ahead of ourselves, we should actually give a formal definition of Brownian motion. 
\begin{definition}
A \textbf{Brownian motion} $B_t \in \R^d$ is a continuous stochastic process satisfying the following properties: 
\begin{enumerate} 
\item Any increment $\Delta B_k := B_{t_{k + 1}} - B_{t_k}$ satisfies $\Delta B_t \sim N(0, (t_{k + 1} - t_{k})I_d)$\footnote{Brownian motion can actually be defined more generally 
by considering a white noise process other than Gaussian, in which case the covariance is not proportional to $I_d$. See Sarkka and Solin (2019)}
\item Increments with disjoint time spans are independent. 
\item The process starts at the origin: $B(0) = 0$. 
\end{enumerate}
\end{definition}

Now some basic facts: 
\begin{enumerate}
\item The function $t \mapsto B_t$ is almost surely continuous. This is often given in the definition of Brownian motion. 
\item $t \mapsto B_t$ is nowhere differentiable.  
\end{enumerate}

\textbf{TODO: left off at page 44 of book}




\end{document}

