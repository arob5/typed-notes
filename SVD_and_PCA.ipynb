{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1852dd9",
   "metadata": {},
   "source": [
    "# SVD, PCA, and Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaaf83b",
   "metadata": {},
   "source": [
    "# The Singular Value Decomposition (SVD)\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea78ec9",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141df30",
   "metadata": {},
   "source": [
    "## The General Problem\n",
    "\n",
    "PCA is closely related to SVD, and can be thought of as providing a statistical interpretation of SVD. To begin, consider the $N \\times D$ design matrix $\\mathbf{X}$. Note that this follows the standard statistical convention of arranging observations $\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D$ in the rows of the matrix, rather than columns as is the case with the \"snapshots\" arrangement common in the engineering sciences. \n",
    "\n",
    "The general problem that PCA answers is to find the best affine approximation to $\\mathbf{X}$, where \"best\" is defined in the squared error sense. In other words, we seek a shifted linear $R$-dimensional subspace such that the projection of the observed data onto this subspace has minimal $L_2$ error with respect to the original data. Of course, for this to be useful we choose $R < D$. In particular, we seek an orthonormal basis for this optimal subspace. In other words, we seek orthonormal basis vectors $\\mathbf{b}_1, \\dots, \\mathbf{b}_R \\in \\mathbb{R}^D$, associated weights $w_1, \\dots, w_R \\in \\mathbb{R}$, and an intercept $\\mathbf{w}_0 \\in \\mathbb{R}^D$ such that \n",
    "\n",
    "$$ \\mathbf{x}_n \\approx \\mathbf{w}_0 + \\sum_{r = 1}^{R} w_r \\mathbf{b}_r$$\n",
    "\n",
    "for all $n = 1, \\dots, N$. Collecting the weights in a vector $\\mathbf{w} \\in \\mathbb{R}^D$ and the the basis vectors of the columns of $D \\times R$ matrix $\\mathbf{B}$ we can write the approximation in matrix form as \n",
    "\n",
    "$$ \\mathbf{x}_n \\approx \\mathbf{w}_0 + \\mathbf{B}\\mathbf{w}$$\n",
    "\n",
    "To be clear, we will be seeking a fixed basis $\\mathbf{B}$ and intercept $\\mathbf{w}_0$ that is optized to the specific dataset $\\mathbf{X}$, but the weights $w_1, \\dots, w_R$ will of course be different for each $\\mathbf{x}_n$. In its full generality, the optimization problem of interest is\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{B}, \\mathbf{w}_0, \\{\\mathbf{w}_n\\}_{n = 1}^{N}} \\sum_{n = 1}^{N} ||\\mathbf{x}_n - (\\mathbf{w}_0 + \\mathbf{B}\\mathbf{w}_n)||_2^2\n",
    "$$\n",
    "\n",
    "where the matrix $\\mathbf{B}$ is constrained to have $R$ orthonormal columns (meaning that $\\mathbf{B}^T \\mathbf{B} = \\mathbf{I}_R$). There are really two different questions here: 1.) finding the optimal basis, and 2.) given the optimal basis, finding the optimal way to represent the data with respect to the basis. The second question may seem obvious to those familiar with the properties of orthogonal projections. Indeed, given fixed basis $\\mathbf{B}$ it is not difficult to show that the optimal values of the other parameters are \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\hat{\\mathbf{w}}_0 = \\overline{\\mathbf{x}} &&\\hat{\\mathbf{w}}_n = \\mathbf{B}^T (\\mathbf{x}_n - \\overline{\\mathbf{x}})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We see that these equations essentially tell us that the data should first be centered by the sample mean\n",
    "\n",
    "$$ \\overline{\\mathbf{x}} := \\frac{1}{N} \\sum_{n = 1}^{N} \\mathbf{x}_n $$\n",
    "\n",
    "Thus, it makes things much easier to subtract $\\overline{\\mathbf{x}}$ from each column of $\\mathbf{X}$ before proceeding, resulting in a centered data matrix. I henceforth assume that $\\mathbf{X}$ has been centered in this manner. Given this simplification, the above optima tell us that the weights used to approximate the $n^{\\text{th}}$ data point are given by \n",
    "\n",
    "$$ \\mathbf{w}_n = \\mathbf{B}^T \\mathbf{x}_n = \\begin{pmatrix} \\langle \\mathbf{b}_1, \\mathbf{x}_n \\rangle \\\\ \\vdots \\\\ \\langle \\mathbf{b}_R, \\mathbf{x}_n \\rangle  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and therefore the approximation to this data point is\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}_n = \\mathbf{B}\\hat{\\mathbf{w}}_n = \\mathbf{B}\\mathbf{B}^T \\mathbf{x}_n = \\sum_{r = 1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n",
    "$$\n",
    "\n",
    "This is why I mentioned that the solution to the optimization problem with $\\mathbf{B}$ fixed is unsurprising; all it says it that the data points are projected onto the subspace spanned by the basis vectors! The associated weights on the basis vectors are simply the inner products as above, due to the fact that the basis vectors are orthonormal. Another way to see this is that $\\mathbf{P} := \\mathbf{B}\\mathbf{B}^T$ is a projection matrix. Indeed, as an outer product it is symmetric positive definite. It is also idempotent following from the orthogonality of the basis vectors: \n",
    "\n",
    "$$ \\mathbf{P}^2 = \\mathbf{P}\\mathbf{P} = \\mathbf{B}\\left(\\mathbf{B}^T \\mathbf{B}\\right) \\mathbf{B}^T = \\mathbf{B} \\mathbf{I}_R \\mathbf{B}^T = \\mathbf{B}\\mathbf{B}^T = \\mathbf{P}$$\n",
    "\n",
    "So once we have a fixerd subspace with an orthogonal basis, all we do is project onto it. The main problem is finding such a basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ef6e2",
   "metadata": {},
   "source": [
    "## Finding the Optimal Subspace\n",
    "We now turn to the question of finding the optimal $R$-dimensional subspace, along with an orthonormal basis $\\mathbf{B}$ for the subspace. Let us revisit the revisit the original optimization problem, plugging in the values $\\hat{\\mathbf{w}}_n$, $n = 1, \\dots, N$ so that the optimization is now only over $\\mathbf{B}$. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{\\mathbf{B}} \\sum_{n = 1}^{N} ||\\mathbf{x}_n - \\mathbf{B}\\hat{\\mathbf{w}}_n||_2^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can re-write the sum (which is known as the *reconstruction error*) as \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{n = 1}^{N} ||\\mathbf{x}_n - \\mathbf{\\hat{x}}_n||_2^2 &= \\sum_{n = 1}^{N} ||\\mathbf{x}_n - \\mathbf{B}\\hat{\\mathbf{w}}_n||_2^2 \\\\ &= \\sum_{n = 1}^{N} ||\\mathbf{x}_n - \\mathbf{B}\\mathbf{B}^T \\mathbf{x}_n||_2^2 \\\\ \n",
    "&= \\sum_{n = 1}^{N} ||\\mathbf{x}_n - \\mathbf{P} \\mathbf{x}_n||_2^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "It can be shown that the solution to this optimization can be written in terms of the SVD $\\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T$. Let $\\mathbf{X}_R = \\mathbf{U}_R \\mathbf{D}_R \\mathbf{V}_R^T$ be the rank-R SVD approximation. The optimal basis and projection matrix can be written \n",
    "$$ \n",
    "\\begin{align*}\n",
    "&\\hat{\\mathbf{B}} = \\mathbf{V}_R &&\\hat{\\mathbf{P}} = \\hat{\\mathbf{B}}\\hat{\\mathbf{B}}^T = \\mathbf{V}_R \\mathbf{V}_R^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that $\\mathbf{V}_R \\mathbf{V}_R^T$ is a $D \\times D$ matrix, representing a projection in $\\mathbb{R}^D$. Its output is therefore still a $D$-dimensional vector, but its range is restricted to the optimal $R$-dimensional subspace. In other words, $\\mathbf{V}_R \\mathbf{V}_R^T \\mathbf{x}_i$ gives the projection of the data point $\\mathbf{x}_i$ onto the $R$-dimensional subspace, but represents it with respect to the original $D$-dimensional coordinate system. Of course, this is innefficient since we only require $R$ coordinates to represent this vector. Therefore, the next question is finding the weights (coordinates) $\\mathbf{w}_i$; in particular, we want to be able to write the weights in terms of the components of the SVD. \n",
    "\n",
    "Recall that the weight vector $\\mathbf{w}$ provides the coefficients in the linear combination of the basis vectors used to approximate the $i^{\\text{th}}$ observation $\\mathbf{x}_i$. Also recall that the optimal weights are simply the inner products of the data point with each of the basis vectors. \n",
    "\n",
    "$$ \\mathbf{w}_i = \\mathbf{V}_R^T \\mathbf{x}_i $$\n",
    "\n",
    "By stacking the weights in the rows of a $N \\times R$ matrix $\\mathbf{W}_R$, the weights for all data points can be written in a single line as \n",
    "\n",
    "$$ \\mathbf{W}_R = \\mathbf{X} \\mathbf{V}_R$$\n",
    "\n",
    "Before considering the reduced basis, let's consider the exact one that does not throw away any singular vectors. In this case, $\\mathbf{W}$ is a $N \\times D$ matrix given by \n",
    "\n",
    "$$ \\mathbf{W} = \\mathbf{X} \\mathbf{V} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T \\mathbf{V} = \\mathbf{U} \\mathbf{D}$$\n",
    "\n",
    "The weights $\\mathbf{W}_R$ are given by,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W}_R = \\mathbf{X} \\mathbf{V}_R &= \\mathbf{U} \\mathbf{D} \\mathbf{V}^T \\mathbf{V}_R \\\\\n",
    "&= \\mathbf{U} \\mathbf{D} \\begin{pmatrix} \\mathbf{I}_R \\\\ \\mathbf{0}_{D-R} \\end{pmatrix} \\\\\n",
    "&= \\mathbf{U} \\begin{pmatrix} \\mathbf{D}_R \\\\ \\mathbf{0}_{D-R} \\end{pmatrix} \\\\\n",
    "&= \\mathbf{U}_R \\mathbf{D}_R\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so that the truncated SVD component matrices are simply swapped in for the full ones. The $i^{\\text{th}}$ row of $\\mathbf{U}_R \\mathbf{D}_R$ corresponds to the weights for the $i^{\\text{th}}$ data point. The columns $\\mathbf{U}_R \\mathbf{D}_R$ also have special names in the PCA world. They are known as the *principal components* (PCs). \n",
    "\n",
    "The next natural question is how to represent the projected data itself $\\mathbf{V}_R^T \\mathbf{V}^T \\mathbf{X}$ in terms of the components of the SVD. We have actually already done this, but we run through the exercise nonetheless for clarity. \n",
    "We now continue by deriving a formula for the projected data; we denote by $\\tilde{\\mathbf{X}}_R$ the $N \\times D$ with $i^{\\text{th}}$ row equal to the projection of the data point $\\mathbf{x}_i$ onto the optimal rank $R$ subspace. Similarly, let $\\tilde{\\mathbf{X}}$ denote the result when no truncation is performed; in this case, the transformation is simply a coordinate transformation. We start with the non-truncated data:\n",
    "\n",
    "$$ \\tilde{\\mathbf{X}} = \\mathbf{X} \\mathbf{V} \\mathbf{V}^T = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T \\mathbf{V} \\mathbf{V}^T = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T$$\n",
    "\n",
    "This was not surprising at all; it is just the SVD again, which we already knew. Just a sanity check. Now for the low-rank projection, \n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\tilde{\\mathbf{X}}_R &= \\mathbf{X} \\mathbf{V}_R \\mathbf{V}_R^T = \\mathbf{U}_R \\mathbf{D}_R \\mathbf{V}_R^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we applied the representation of the weights $\\mathbf{X} \\mathbf{V}_R$ derived above. This is simply the rank $R$ SVD truncation, again as expected. As a summary, the SVD tells you a lot. The projected data is given by the truncated SVD $\\mathbf{U}_R \\mathbf{D}_R \\mathbf{V}_R^T$, where the columns of $\\mathbf{V}_R$ are the basis vectors of the optimal rank $R$ subspace, and the rows of $\\mathbf{U}_R \\mathbf{D}_R$ are the weights for the corresponding data points which are used to perform the projection. Another way to say this is that $\\mathbf{U}_R \\mathbf{D}_R$ provides the coordinates used to represent the projected data with respect to the basis vectors in $\\mathbf{V}_R$. Just as the $j^{\\text{th}}$ column of $\\mathbf{X}$ provides the scalings of each data point in the $j^{\\text{th}}$ coordinate direction in the original coordinate space, the $j^{\\text{th}}$ column of $\\mathbf{U}_R \\mathbf{D}_R$ has the same interpretation but with respect to the new basis. Thus, the columns of $\\mathbf{U}_R \\mathbf{D}_R$ (the PCs) can be thought of as new *features*; that is, derived features that according to PCA carry the most information about the data. We can think of $\\mathbf{U}_R \\mathbf{D}_R$ as a sort of \"new data matrix\", and can operate on it in all of the ways we are used to operating on $\\mathbf{X}$. For example, whereas we typically might run a regression using the columns of $\\mathbf{X}$ as features, we can consider running a regression instead using the derived features provided in by the columns of $\\mathbf{U}_R \\mathbf{D}_R$. This is known as *principal component regression* and is discussed later on. \n",
    "\n",
    "As a summary, recall that \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\tilde{\\mathbf{X}}_R = \\mathbf{U}_R \\mathbf{D}_R \\mathbf{V}^T_R &&\\mathbf{W}_R = \\mathbf{U}_R \\mathbf{D}_R\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Both $\\tilde{\\mathbf{X}}_R$ and $\\mathbf{W}_R$ are describing the *same points* (the projected data points), just represented with respect to different coordinate systems. In particular, the rows of $\\tilde{\\mathbf{X}}_R$ give the coordinates of the projected points with respect to the original coordinate system, while the rows of $\\mathbf{W}_R$ give the coordinates of these same points with respect to the PCA basis $\\mathbf{V}_R$. For a bit more terminology, the basis vectors (the columns of $\\mathbf{V}_R$) are often called the *PC directions* or \n",
    "*Karhunen-Loeve directions* of $\\mathbf{X}$. Before moving on, let's think a bit more about the PCs - the *derived variables* in the columns of $\\mathbf{W}$.\n",
    "\n",
    "$$ \\mathbf{W} = \\begin{pmatrix} d_1 \\mathbf{U}_1 & d_2 \\mathbf{U}_2 & \\cdots & d_D \\mathbf{U}_D \\end{pmatrix}$$\n",
    "\n",
    "Thus, $\\mathbf{W}_j = d_j \\mathbf{U}_j$ is the $j^{\\text{th}}$ PC, representing the $j^{\\text{th}}$ *derived feature* or *derived variable*. Note also that \n",
    "\n",
    "$$ \\mathbf{X} \\mathbf{V}_j = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T \\mathbf{V}_1 = \\mathbf{U} \\mathbf{D} \\mathbf{e}_j = \\mathbf{W}_j$$\n",
    "\n",
    "We have found, \n",
    "\n",
    "$$ \\mathbf{W}_j = \\left(\\mathbf{U} \\mathbf{D}\\right)_j = \\sum_{d = 1}^{D} \\mathbf{V}_{dj} \\mathbf{X}_d $$\n",
    "\n",
    "In words, the $j^{\\text{th}}$ PC is simply a linear combination of the original features, with weights supplied by the entries in the $j^{\\text{th}}$ column of $\\mathbf{V}$. This is the primary sense is which PCA is a *linear* method. The task of finding the optimal (in the $L_2$ sense) *linear* subspace to represent the data also results in the derived features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312bc7b0",
   "metadata": {},
   "source": [
    "### Principal Component Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121e4d2",
   "metadata": {},
   "source": [
    "# Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0ae0d",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "Prove that the SVD solves the PCA reconstruction error minimization problem. \n",
    "See: https://www.cs.cornell.edu/courses/cs4786/2016sp/lectures/lec03.pdf\n",
    "\n",
    "I would like to solve this problem by showing the eigenvalues of the empirical covariance matrix formulation; then from there show that the SVD provides an alternative algorithm to compute it. \n",
    "\n",
    "Better: https://graphics.stanford.edu/courses/cs233-22-spring/ReferencedPapers/LectureNotes-PCA.pdf\n",
    "\n",
    "Discuss the maximized Raleigh coefficient interpretation of PCA:\n",
    "https://stats.stackexchange.com/questions/10251/what-is-the-objective-function-of-pca\n",
    "\n",
    "Also discuss the recursive variance maximization perspective (e.g. there is a proof of this in Bishop's pattern recognition book); what is the variance of the projected points, as represented in the new coordinate system? how about the original coordinate system?\n",
    "\n",
    "\n",
    "Other resources to consult: \n",
    "Tibshirani's data mining notes and \n",
    "Mathematics for machine learning book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
