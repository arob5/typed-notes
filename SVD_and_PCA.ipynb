{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3666e726",
   "metadata": {},
   "source": [
    "# SVD, PCA, and Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63511848",
   "metadata": {},
   "source": [
    "# The Singular Value Decomposition (SVD)\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8dc618",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a3f75",
   "metadata": {},
   "source": [
    "## The General Problem\n",
    "\n",
    "PCA is closely related to SVD, and can be thought of as providing a statistical interpretation of SVD. To begin, consider the $N \\times D$ design matrix $\\mathbf{X}$. Note that this follows the standard statistical convention of arranging observations $\\mathbf{x}_1, \\dots, \\mathbf{x}_N \\in \\mathbb{R}^D$ in the rows of the matrix, rather than columns as is the case with the \"snapshots\" arrangement common in the engineering sciences. \n",
    "\n",
    "The general problem that PCA answers is to find the best affine approximation to $\\mathbf{X}$, where \"best\" is defined in the squared error sense. In other words, we seek a shifted linear $R$-dimensional subspace such that the projection of the observed data onto this subspace has minimal $L_2$ error with respect to the original data. Of course, for this to be useful we choose $R < D$. In particular, we seek an orthonormal basis for this optimal subspace. In other words, we seek orthonormal basis vectors $\\mathbf{b}_1, \\dots, \\mathbf{b}_R \\in \\mathbb{R}^D$, associated weights $w_1, \\dots, w_R \\in \\mathbb{R}$, and an intercept $\\mathbf{w}_0 \\in \\mathbb{R}^D$ such that \n",
    "\n",
    "$$ \\mathbf{x}_n \\approx \\mathbf{w}_0 + \\sum_{r = 1}^{R} w_r \\mathbf{b}_r$$\n",
    "\n",
    "for all $n = 1, \\dots, N$. Collecting the weights in a vector $\\mathbf{w} \\in \\mathbb{R}^D$ and the the basis vectors of the columns of $D \\times R$ matrix $\\mathbf{B}$ we can write the approximation in matrix form as \n",
    "\n",
    "$$ \\mathbf{x}_n \\approx \\mathbf{w}_0 + \\mathbf{B}\\mathbf{w}$$\n",
    "\n",
    "To be clear, we will be seeking a fixed basis $\\mathbf{B}$ and intercept $\\mathbf{w}_0$ that is optized to the specific dataset $\\mathbf{X}$, but the weights $w_1, \\dots, w_R$ will of course be different for each $\\mathbf{x}_n$. In its full generality, the optimization problem of interest is\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{B}, \\mathbf{w}_0, \\{\\mathbf{w}_n\\}_{n = 1}^{N}} \\sum_{n = 1}^{N} ||\\mathbf{x}_n - (\\mathbf{w}_0 + \\mathbf{B}\\mathbf{w}_n)||_2^2\n",
    "$$\n",
    "\n",
    "where the matrix $\\mathbf{B}$ is constrained to have $R$ orthonormal columns (meaning that $\\mathbf{B}^T \\mathbf{B} = \\mathbf{I}_R$). There are really two different questions here: 1.) finding the optimal basis, and 2.) given the optimal basis, finding the optimal way to represent the data with respect to the basis. The second question may seem obvious to those familiar with the properties of orthogonal projections. Indeed, given fixed basis $\\mathbf{B}$ it is not difficult to show that the optimal values of the other parameters are \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\hat{\\mathbf{w}}_0 = \\overline{\\mathbf{x}} &&\\hat{\\mathbf{w}}_n = \\mathbf{B}^T (\\mathbf{x}_n - \\overline{\\mathbf{x}})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We see that these equations essentially tell us that the data should first be centered by the sample mean\n",
    "\n",
    "$$ \\overline{\\mathbf{x}} := \\frac{1}{N} \\sum_{n = 1}^{N} \\mathbf{x}_n $$\n",
    "\n",
    "Thus, it makes things much easier to subtract $\\overline{\\mathbf{x}}$ from each column of $\\mathbf{X}$ before proceeding, resulting in a centered data matrix. I henceforth assume that $\\mathbf{X}$ has been centered in this manner. Given this simplification, the above optima tell us that the weights used to approximate the $n^{\\text{th}}$ data point are given by \n",
    "\n",
    "$$ \\mathbf{w}_n = \\mathbf{B}^T \\mathbf{x}_n = \\begin{pmatrix} \\langle \\mathbf{b}_1, \\mathbf{x}_n \\rangle \\\\ \\vdots \\\\ \\langle \\mathbf{b}_R, \\mathbf{x}_n \\rangle  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "and therefore the approximation to this data point is\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}}_n = \\mathbf{B}\\hat{\\mathbf{w}}_n = \\mathbf{B}\\mathbf{B}^T \\mathbf{x}_n = \\sum_{r = 1}^{R} \\langle \\mathbf{x}_n, \\mathbf{b}_r \\rangle \\mathbf{b}_r\n",
    "$$\n",
    "\n",
    "This is why I mentioned that the solution to the optimization problem with $\\mathbf{B}$ fixed is unsurprising; all it says it that the data points are projected onto the subspace spanned by the basis vectors! The associated weights on the basis vectors are simply the inner products as above, due to the fact that the basis vectors are orthonormal. Another way to see this is that $\\mathbf{P} := \\mathbf{B}\\mathbf{B}^T$ is a projection matrix. Indeed, as an outer product it is symmetric positive definite. It is also idempotent following from the orthogonality of the basis vectors: \n",
    "\n",
    "$$ \\mathbf{P}^2 = \\mathbf{P}\\mathbf{P} = \\mathbf{B}\\left(\\mathbf{B}^T \\mathbf{B}\\right) \\mathbf{B}^T = \\mathbf{B} \\mathbf{I}_R \\mathbf{B}^T = \\mathbf{B}\\mathbf{B}^T = \\mathbf{P}$$\n",
    "\n",
    "So once we have a fixerd subspace with an orthogonal basis, all we do is project onto it. The main problem is finding such a basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377aa74f",
   "metadata": {},
   "source": [
    "## Finding the Optimal Subspace\n",
    "We now turn to the question of finding the optimal $R$-dimensional subspace, along with an orthonormal basis $\\mathbf{B}$ for the subspace. Let us revisit the revisit the original optimization problem, plugging in the values $\\hat{\\mathbf{w}}_n$, $n = 1, \\dots, N$ so that the optimization is now only over $\\mathbf{B}$. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\min_{\\mathbf{B}} \\sum_{n = 1}^{N} ||\\mathbf{x}_n - \\mathbf{B}\\hat{\\mathbf{w}}_n||_2^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can re-write the sum (which is known as the *reconstruction error*) as \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{n = 1}^{N} ||\\mathbf{x}_n - \\mathbf{\\hat{x}}_n||_2^2 &= \\sum_{n = 1}^{N} ||\\mathbf{x}_n - \\mathbf{B}\\hat{\\mathbf{w}}_n||_2^2 \\\\ &= \\sum_{n = 1}^{N} ||\\mathbf{x}_n - \\mathbf{B}\\mathbf{B}^T \\mathbf{x}_n||_2^2 \\\\ \n",
    "&= \\sum_{n = 1}^{N} ||\\mathbf{x}_n - \\mathbf{P} \\mathbf{x}_n||_2^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "It can be shown that the solution to this optimization can be written in terms of the SVD $\\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T$. Let $\\mathbf{X}_R = \\mathbf{U}_R \\mathbf{D}_R \\mathbf{V}_R^T$ be the rank-R SVD approximation. The optimal basis and projection matrix can be written \n",
    "$$ \n",
    "\\begin{align*}\n",
    "&\\hat{\\mathbf{B}} = \\mathbf{V}_R &&\\hat{\\mathbf{P}} = \\hat{\\mathbf{B}}\\hat{\\mathbf{B}}^T = \\mathbf{V}_R \\mathbf{V}_R^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note that $\\mathbf{V}_R \\mathbf{V}_R^T$ is a $D \\times D$ matrix, representing a projection in $\\mathbb{R}^D$. So now we know that the columns of $\\mathbf{V}_R$ form an orthonormal basis for the optimal rank $R$ subspace. The next question is how the weights $\\mathbf{w}_i$ and the projected data itself \n",
    "$\\mathbf{V}_R^T \\mathbf{V}^T \\mathbf{X}$ can be represented in terms of the components of the SVD decomposition. \n",
    "\n",
    "We start with the weights. Recall that the weight vector $\\mathbf{w}$ provides the coefficients in the linear combination of the basis vectors used to approximate the $i^{\\text{th}}$ observation $\\mathbf{x}_i$. Also recall that the optimal weights are simply the inner products of the data point with each of the basis vectors. \n",
    "\n",
    "$$ \\mathbf{w}_i = \\mathbf{V}_R^T \\mathbf{x}_i $$\n",
    "\n",
    "By stacking the weights in the rows of a $N \\times R$ matrix $\\mathbf{W}_R$, the weights for all data points can be written in a single line as \n",
    "\n",
    "$$ \\mathbf{W}_R = \\mathbf{X} \\mathbf{V}_R$$\n",
    "\n",
    "Before considering the reduced basis, let's consider the exact one that does not throw away any singular vectors. In this case, $\\mathbf{W}$ is a $N \\times D$ matrix given by \n",
    "\n",
    "$$ \\mathbf{W} = \\mathbf{X} \\mathbf{V} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T \\mathbf{V} = \\mathbf{U} \\mathbf{D}$$\n",
    "\n",
    "The weights $\\mathbf{W}_R$ are given by,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{W}_R = \\mathbf{X} \\mathbf{V}_R &= \\mathbf{U} \\mathbf{D} \\mathbf{V}^T \\mathbf{V}_R \\\\\n",
    "&= \\mathbf{U} \\mathbf{D} \\begin{pmatrix} \\mathbf{I}_R \\\\ \\mathbf{0}_{D-R} \\end{pmatrix} \\\\\n",
    "&= \\mathbf{U} \\begin{pmatrix} \\mathbf{D}_R \\\\ \\mathbf{0}_{D-R} \\end{pmatrix} \\\\\n",
    "&= \\mathbf{U}_R \\mathbf{D}_R\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "so that the truncated SVD component matrices are simply swapped in for the full ones. The $i^{\\text{th}}$ row of $\\mathbf{U}_R \\mathbf{D}_R$ corresponds to the weights for the $i^{\\text{th}}$ data point. The columns $\\mathbf{U}_R \\mathbf{D}_R$ also have special names in the PCA world. They are known as the *principal components* (PCs). \n",
    "\n",
    "We now continue by deriving a formula for the projected data; we denote by $\\tilde{\\mathbf{X}}_R$ the $N \\times D$ with $i^{\\text{th}}$ row equal to the projection of the data point $\\mathbf{x}_i$ onto the optimal rank $R$ subspace. Similarly, let $\\tilde{\\mathbf{X}}$ denote the result when no truncation is performed; in this case, the transformation is simply a coordinate transformation. We start with the non-truncated data:\n",
    "\n",
    "$$ \\tilde{\\mathbf{X}} = \\mathbf{X} \\mathbf{V} \\mathbf{V}^T = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T \\mathbf{V} \\mathbf{V}^T = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T$$\n",
    "\n",
    "This was not surprising at all; it is just the SVD again, which we already knew. Just a sanity check. Now for the low-rank projection, \n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\tilde{\\mathbf{X}}_R &= \\mathbf{X} \\mathbf{V}_R \\mathbf{V}_R^T = \\mathbf{U}_R \\mathbf{D}_R \\mathbf{V}_R^T\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we applied the representation of the weights $\\mathbf{X} \\mathbf{V}_R$ derived above. This is simply the rank $R$ SVD truncation, again as expected. As a summary, the SVD tells you a lot. The projected data is given by the truncated SVD $\\mathbf{U}_R \\mathbf{D}_R \\mathbf{V}_R^T$, where the columns of $\\mathbf{V}_R$ are the basis vectors of the optimal rank $R$ subspace, and the rows of $\\mathbf{U}_R \\mathbf{D}_R$ are the weights for the corresponding data points which are used to perform the projection. Another way to say this is that $\\mathbf{U}_R \\mathbf{D}_R$ provides the coordinates used to represent the projected data with respect to the basis vectors in $\\mathbf{V}_R$. Just as the $j^{\\text{th}}$ column of $\\mathbf{X}$ provides the scalings of each data point in the $j^{\\text{th}}$ coordinate direction in the original coordinate space, the $j^{\\text{th}}$ column of $\\mathbf{U}_R \\mathbf{D}_R$ has the same interpretation but with respect to the new basis. Thus, the columns of $\\mathbf{U}_R \\mathbf{D}_R$ (the PCs) can be thought of as new *features*; that is, derived features that according to PCA carry the most information about the data. We can think of $\\mathbf{U}_R \\mathbf{D}_R$ as a sort of \"new data matrix\", and can operate on it in all of the ways we are used to operating on $\\mathbf{X}$. For example, whereas we typically might run a regression using the columns of $\\mathbf{X}$ as features, we can consider running a regression instead using the derived features provided in by the columns of $\\mathbf{U}_R \\mathbf{D}_R$. This is known as *principal component regression* and is discussed in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dade4bc",
   "metadata": {},
   "source": [
    "### Principal Component Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf6339",
   "metadata": {},
   "source": [
    "# Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01987a",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "Prove that the SVD solves the PCA reconstruction error minimization problem. \n",
    "See: https://www.cs.cornell.edu/courses/cs4786/2016sp/lectures/lec03.pdf\n",
    "\n",
    "I would like to solve this problem by showing the eigenvalues of the empirical covariance matrix formulation; then from there show that the SVD provides an alternative algorithm to compute it. \n",
    "\n",
    "Better: https://graphics.stanford.edu/courses/cs233-22-spring/ReferencedPapers/LectureNotes-PCA.pdf\n",
    "\n",
    "Other resources to consult: \n",
    "Tibshirani's data mining notes and \n",
    "Mathematics for machine learning book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
