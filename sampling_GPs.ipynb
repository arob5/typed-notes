{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defc4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede23f94",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This document discusses sampling from Gaussian process (GP) posterior distributions. Since GPs can be considered priors on functions, we can conceptualize this problem as that of sampling functions from some distribution. In practice though, we of course only sample the functions at a finite number of points. Nonetheless, we are often interested in function values at many points, and the function values are typically quite correlated. These features make this sampling task a difficult one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0501a99",
   "metadata": {},
   "source": [
    "# Setup and Notation\n",
    "\n",
    "## GP Regression\n",
    "Consider the standard GP regression model \n",
    "\n",
    "$$y = f(x) + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0, \\sigma^2)$ is independent Gaussian noise. We place a GP prior on the latent function $f$:\n",
    "\n",
    "$$ f \\sim \\mathcal{GP}(0, k)$$\n",
    "\n",
    "where I have assumed the mean function is identically zero for simplicity. Suppose we have observations $(x^\\prime_1, y_1), \\dots, (x^\\prime_N, y_N)$, collected in a $N \\times d$ matrix of input locations $\\mathbf{X}^\\prime$ and an $N \\times 1$ response vector $\\mathbf{y}$. Also let $\\mathbf{f}^\\prime$ be the $N \\times 1$ vector of (unobserved) latent function values at the points $\\mathbf{X}^\\prime$. Suppose we also have a set of input locations $x_1, \\dots, x_M$ at which we would like to predict, similarly collected in a matrix $\\mathbf{X}$. Let $\\mathbf{f}$ denote the $M \\times 1$ vector of latent function values at these inputs. In general, these locations might include the training locations or not. In this setting, two quantities are typically of primary interest: the marginal likelihood and the predictive (posterior) distribution over function values. The former is given by \n",
    "$$ p(\\mathbf{y}|\\mathbf{X}^\\prime) = \\int p(\\mathbf{y}, \\mathbf{f}^\\prime|\\mathbf{X}^\\prime) d\\mathbf{f}^\\prime = \\int p(\\mathbf{y}|\\mathbf{f}^\\prime, \\mathbf{X}^\\prime)p(\\mathbf{f}^\\prime|\\mathbf{X}^\\prime) d\\mathbf{f}^\\prime$$\n",
    "The likelihood is called *marginal*, as its computation requires marginalizing out the unobserved latent function values $\\mathbf{f}^\\prime$ at the training input points. In this setting, this integral is analytically tractable since both terms are Gaussian\n",
    "\n",
    "\\begin{align*}\n",
    "p(\\mathbf{y}|\\mathbf{f}^\\prime, \\mathbf{X}^\\prime) &= \\mathcal{N}_N(\\mathbf{y}|\\mathbf{f}^\\prime, \\sigma^2 \\mathbf{I}_N) \\\\\n",
    "p(\\mathbf{f}^\\prime|\\mathbf{X}^\\prime) &= \\mathcal{N}_N(\\mathbf{f}^\\prime|\\mathbf{0}, \\mathbf{K}^\\prime)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{K}^\\prime$ is the $N \\times N$ matrix resulting from evaluating the kernel at the training inputs. This integral is only analytically tractable due to the simple choice of likelihood and the linear connection between $y$ and $f$. We are interested in more general settings where we lose this analytical tractability. \n",
    "\n",
    "The second primary quantity of interest is the posterior over the latent function values $\\mathbf{f}$. \n",
    "$$p(\\mathbf{f}|\\mathbf{y}) = \\int p(\\mathbf{f}|\\mathbf{f}^\\prime, \\mathbf{y})p(\\mathbf{f}^\\prime|\\mathbf{y})d\\mathbf{f}^\\prime = \\int p(\\mathbf{f}|\\mathbf{f}^\\prime)p(\\mathbf{f}^\\prime|\\mathbf{y})d\\mathbf{f}^\\prime$$\n",
    "Once again, this requires marginalizing out the latent function values $\\mathbf{f}^\\prime$ at the training inputs, and once again this is analytically tractable in this simple regression setting. Note that technically I should be conditioning on $\\mathbf{X}^\\prime$ and $\\mathbf{X}$ as well, but I suppress this to lighten notation. This document focuses on sampling from distributions like $p(\\mathbf{f}|\\mathbf{y})$, but in the more general case where the resulting  \n",
    "\n",
    "## GP Classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
