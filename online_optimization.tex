\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bbm} % For mathbbm for indicator function
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

\begin{document}

\begin{center}
\Large
Optimization Under Uncertainty/Online Optimization
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

\section{Introduction}
This text contains a brief introduction to the field of online optimization. ``Online'' here refers to settings in which one has limited or no knowledge about the future; thus, we might 
also call this subject ``optimization under uncertainty''. This is opposed to the ``offline'' setting dealt with in classical optimization theory. Take linear programming for instance: here we
assume complete knowledge of the problem in the form of the objective function and relevant constraints. This complete information assumption may be unrealistic in certain applications. 
As an example, let's consider two-player games from game theory. In the classical setup, it is assumed that both players have perfect knowledge of the others strategies (or mixed strategies). 
A natural extension that falls under the umbrella of online learning is to consider the situation in which the adversary's strategies are unknown. In these kind of applications the decision problem 
may be sequential in nature; that is, a player is required to make sequential decisions responding to those of the adversary. However, online learning also includes problems in which a decision is 
made only once. 

\section{General Setting and Regret Minimization}
Consider the following game repeated over $T$ rounds: 
\begin{itemize}
\item An adversary picks $y_t \in [0, 1]$
\item You pick $x_t \in [0, 1]$, trying to guess the adversary's number
\item The adversary's number is revealed and you face an $\ell_2$ penalty: $(x_t - y_t)^2$
\end{itemize}
The question is: how should you guess in order to incur minimal cumulative loss? Currently, under these very light assumptions we can essentially say nothing. For example, suppose the adversary draws 
$y_t$ randomly from a completely different distribution (unknown to you) at each round $t$. This game seems somewhat hopeless for the player. However, what if we add more structure? Suppose we know 
in advance that the adversary always draws $y_t \overset{iid}{\sim} \mathcal{U}[0, 1]$. Let $Y \sim \mathcal{U}[0, 1]. $In this case, since we know that the mean minimizes $\ell_2$ loss we should guess 
$x_t := \E Y= \frac{1}{2}$. Letting $x = (x_1, \dots, x_T)^T$ and $y = (y_1, \dots, y_T)^T$ the cumulative loss incurred would thus be
\[\text{loss}(x, y) := \norm{x - y}_2^2 = \sum_{t = 1}^{T} (y_t - x_t)^2 = \sum_{t = 1}^{T} (Y_t - \E Y_t)^2 \]
\[\E_Y[\text{loss}(x, y)] =  \sum_{t = 1}^{T} \E(Y_t - \E Y_t)^2 = \sum_{t = 1}^{T} \Var(Y) = T\Var(Y) = \frac{T}{12}\]
where we use the fact that the $Y_t$ are iid and the final step recalls the variance of a uniform random variable. With respect to $\ell_2$ loss, this is the best we can do. Note that there was nothing special 
about the uniform distribution here; the key assumption was that the adversary picked numbers iid from a distribution that we knew in advance. In general, if this distribution has variance $\sigma^2$ then 
by playing our optimal choice of the mean of the distribution we will incur cumulative loss $\sigma^2 T$. 

The assumption that we know the distribution is quite strong, and seems unreasonable to hold in real applications. So what can we say about the situation where the distribution is unknown in advance?
We're still assuming that the adversary picks a fixed distribution at the beginning and samples iid from that distribution each round. Intuitively, we would imagine that our guesses will likely be pretty bad at first, 
but that if $T$ is sufficiently large we can, over time, start to get a sense of the underlying distribution and tune our guesses accordingly. We certainly can't hope to compete with the minimal loss incurred in the 
known distribution setting. Thus, $\sigma^2 T$ seems to be a good benchmark - we can measure our performance relative to this idealized setting. 
\[\text{loss}(x, y) := \E_Y \norm{X - Y}_2^2 - \sigma^2 T\]
Note that the expectation is taken with respect to the distribution chosen by the adversary, which is unknown the the player. 
It is also useful to divide through by $T$ and consider the average loss instead. 
\[\text{loss}_{\text{avg}}(x, y) := \frac{1}{T} \E_Y \norm{X - Y}_2^2 - \sigma^2\]
What can we hope for? Recall our intuition that over time we hope to be able to \textit{learn} the underlying distribution and thus hope that our estimates will tend towards the estimates we would make
when the distribution is known. That is, we hope to be able to show that the average loss tends to $0$ in the limit as $T \to \infty$. What does this imply about $\text{loss}(x, y)$? Well, first of all we should 
not expect this to tend to $0$ since this measures the difference in the \textit{cumulative} loss with respect to the cumulative loss of the idealized setting. Since at first we expect our guesses to be significantly
worse than in the idealized case, then the large loss accumulated for these early guesses will not go away - the cumulative loss will just keep adding to this. However, since we expect our guesses to get better
in time then we expect the \textit{rate} of accumulation to decrease over time. For example, if $\text{loss}(x, y)$ grew linearly then this would imply a constant rate of accumulation, which would mean our guesses
are not improving over time. Thus, our hope is that $\text{loss}(x, y)$ grows \textit{sub-linearly} (could be logarithmically, square root, etc.). This makes sense because when we divide through by $T$ in order 
to consider the average growth relative to the baseline then in order for this average distance to tend to $0$ then the denominator (which is growing linearly) must grow faster than the numerator (hence the 
numerator must grow \textit{sub-linearly}). 

With this intuition established, we proceed with the last couple steps in order to arrive at our final performance metric. I begin by re-writing $\text{loss}(x, y)$ in an equivalent way
\[\text{loss}(x, y) = \E_Y \norm{X - Y}_2^2 - \min_{x \in [0, 1]} \E_Y \norm{X - Y}_2^2 \]
Recall that this follows from the fact that $\sigma^2 T$ was the loss when the optimal $x \equiv x_t$ was chosen to minimize the $\ell_2$ distance. This above expression just explicitly 
writes out this optimization perspective. Now for another generalization: we have thus far been making the restrictive assumption that the adversary chooses a fixed distribution at the start of the game. We want to 
define a general performance metric to applies in cases without any sort of distributional assumption. We can thus simply take the previous expression and remove all randomness from the expression, 
just treating the $x_t$ and $y_t$ as sequences of real numbers. The resulting expression is known as \textit{regret}, which captures the notion that once the true realizations are revealed you regret 
the choices you made  and would have rather chosen the true values. In other words, it expresses what you would have chosen \textit{in hindsight}. 
\[\text{Regret}_T(x, y) = \norm{x - y}_2^2 - \min_{x \in [0, 1]} \norm{x - y}_2^2 \]
We will consider that we've ``won'' the game if $\text{Regret}_T(x, y)$ grows sub-linearly in $T$: 
\[\lim_{T \to \infty} \frac{\text{Regret}_T(x, y)}{T} = 0\]
which we've seen is another way of saying that the average regret tends to $0$. We have now arrived at the final definition of loss that we will consider. This definition will generalize a few restricting assumptions
we have been making: 
\begin{itemize}
\item Instead of considering $\ell_2$ loss we will instead consider an arbitrary loss function $\ell$. We need not even explicitly consider $y$ anymore; all we need to be able to say is that 
a guess $x_t$ generalizes some loss $\ell(x_t)$. 
\item We will now allow for different loss functions at each round of the game, $\ell_t$, $t = 1, \dots, T$. 
\item We drop the assumption that the values must be in $[0, 1]$, instead considering subsets of $d$-dimensional Euclidean space.  
\item Instead of parameterizing the regret with respect to the idealized setting we have been considering, we allow the framework to be flexible and just parameterize it with respect to 
some guess $u \in \R^d$ (in the above examples, $u$ was the mean of the distribution). 
\end{itemize}
\begin{definition}
Let $x = (x_1, \dots, x_T)^T$, where $x_t \in V \subset \R^d$ be the vectors output by an algorithm at each of $T$ rounds. Suppose the values incur loss $\ell_t(x_t)$ at round $t$, where $\ell_t: \R^d \to \R$
for $t = 1, \dots, T$. Also let $u \in V$ be a ``predictor'' which the loss will be measured with respect to. We then define the \textit{regret} as: 
\[\text{Regret}_T(x, u) := \sum_{t = 1}^{T} \ell_t(x_t) - \sum_{t = 1}^{T} \ell_t(u) \]
\end{definition}
As we will see, the generality of this definition will be powerful in allowing it to apply in a diverse array of settings. Note that there are many different assumptions we could tweak in this framework; for example, 
we can suppose that the loss functions $\ell_t$ are known to the player in advance, or are they chosen by the adversary and only revealed once the player has made their guess $x_t$? The latter assumption, along 
with the assumption that the $\ell_t$ are convex, form the general \textit{online convex optimization} framework. We can also generalize further to allow the baseline $u$ be a sequence $u_1, \dots, u_T$ which 
we measure loss with respect to. 

\section{Regret Minimization: A Simple Example}
\textbf{TODO: include the example from Prof Orabana's first week of notes}

\section{A Note on Potential Function Proofs}
This section provides a very brief introduction to a common proof technique commonly utilized in proofs of convergence for iterative optimization algorithms. For an in-depth discussion of this topic see 
Banal and Gupta's paper mentioned in the references. The potential function framework (not to be confused with potential functions from vector calculus/physics) is utilized in the proofs of many optimization algorithms, 
including in the online setting. 

First recall the general goal of these algorithms: supposing $f$ is convex, we seek to approximate its minimum $f(x^*)$. It is common to try to bound the additive error $f(x_T) - f(x^*) < \epsilon$, where $x_T$
is the estimate of the true minimizer $x^*$ produced by an algorithm after $T$ iterations. The goal is to construct an upper bound on this additive error as a function of $T$. 

Potential functions are a unifying technique that helps in constructing such a bound. Note that above we are considering the distance in function values $f(x_T) - f(x^*)$. We might also consider the distance 
between the minimizers $d(x_T, x^*)$, where I intentionally keep the distance metric unspecified for now (could be $\ell_2$, etc.). If $f$ is continuous then we can easily convert between bounds on the function
value distance and bounds on the minimizing vector distance. The potential function is defined to take into account both of these distances: 
\[\Phi_t := a_t [f(x_t) - f(x^*)] + b_t \cdot d(x_t, x^*)\]
where $a_t, b_t$ are non-negative multipliers that can be tailored to the needs of different proofs. The justification for the above definition is that bounding the potential function will induce a bound on 
$f(x_T) - f(x^*)$. Indeed, suppose we obtain the bound: 
\[\Phi_{t + 1} - \Phi_t \leq B_t\]
Then this implies
\[\sum_{t = 1}^{T - 1} [\Phi_{t + 1} - \Phi_t] \leq  \sum_{t = 1}^{T - 1} B_t\]
The lefthand side is a telescoping sum and therefore we're left with 
\[\Phi_{T} - \Phi_0 \leq  \sum_{t = 1}^{T - 1} B_t\]
Moving $\Phi_0$ to the other side and expanding $\Phi_{T}$, 
\[ a_T [f(x_T) - f(x^*)] + b_T \cdot d(x_T, x^*) \leq \Phi_0 + \sum_{t = 1}^{T - 1} B_t\]
Thus, 
\[ f(x_t) - f(x^*) \leq \frac{1}{a_T}\left[\Phi_0 - b_T \cdot d(x_T, x^*) + \sum_{t = 1}^{T - 1} B_t\right] \leq \frac{\Phi_0 + \sum_{t = 1}^{T - 1} B_t}{a_T}\]
A couple notes: first, in the last step we simplify the bound by dropping $b_T \cdot d(x_T, x^*)$. In general, we're considering $T$ large enough such that $d(x_T, x^*)$ is small and thus
this final inequality shouldn't give up too much in the bound. Next, we observe that $\Phi_0$ encodes the error given by the initialization $x_0$ of the algorithm (the ``initial guess''). This potential
function framework provides a nice organization for many convergence proofs; you'll see many convergence proofs of gradient descent, etc. that feature this telescoping sum step without explicitly 
considering a potential function, but I find that explicitly defining a potential function helps me draw parallels between similar proofs. 

\subsection{TODO: include example of a potential function proof from the Banal and Gupta paper}


% Section: Learning From Experts
\section{Learning from Experts}
One important online optimization problem is that of learning from expert advice. The problem is typically defined as follows: a decision-maker must make a decision at each of $T$ rounds. Here let's 
consider a simple set of possible realizations $V = \{A, B\}$ so that this can be thought of as a repeated binary classification problem. We consider the zero-one loss 
$\ell(x_t, y_t) = \mathbbm{1}(x_t \neq y_t)$. Typically in binary classification we base the classification on information 
given in the form of \textit{features} or \textit{predictors}. In this setting we refer to the analogous concept as \textit{experts}; in particular, we consider a set of $N$ experts that each recommend a decision
in $\{A, B\}$ each time step. Your goal is thus to use this expert advice in order to make the best possible decisions. Note that one's first approach in solving this problem would likely be to develop an 
algorithm to minimize the number of mistakes: 
\[\min_{x_t} \sum_{t = 1}^{T} \ell(x_t, y_t)\]
However, this is not a good baseline to evaluate our algorithms. Indeed, it can be shown that no randomized algorithm can make fewer than $\frac{T}{2}$ mistakes in expectation. Moreover, the trivial algorithm 
of simply flipping a coin at each step $x_t \overset{iid}{\sim} \text{Bern}(1/2)$ achieves this lower bound. So this turned out to be very uninteresting: $\frac{T}{2}$ mistakes was both optimal and trivial 
to obtain. The solution of course is to instead consider the \textit{regret}, which requires choosing a useful benchmark. Recalling the general definition of regret, we therefore must decide on a suitable 
predictor $u$. In the learning from experts setting, a natural choice is to consider loss with respect to the loss of the \textit{best expert}. We therefore consider
\[\text{Regret}_T(x, u) := \frac{1}{T} \sum_{t = 1}^{T} \ell_t(x_t) - \frac{1}{T} \sum_{t = 1}^{T} \ell_t(u_t) = \frac{1}{T} \sum_{t = 1}^{T} \mathbbm{1}\{x_t \neq y_t\} - \min_{1 \leq i \leq N} \frac{1}{T} \sum_{t = 1}^{T} \mathbbm{1}\{u^{(i)}_t \neq y_t\}\]
where $u^{(i)}_t$ denotes the advice from the $i^{\text{th}}$ expert at round $t$. Thus in the first expression $u_t$ is the advice from the best expert at round $t$. Note that we're considering what we previously 
called \textit{average regret} here. I've just dropped the ``avg'' subscript to lighten notation (often when people talk about regret they're talking about this averaged version anyways). 

\subsection{The Weighted Majority Algorithm}

\subsubsection{Simple Case of the Infallible Expert}
To motivate the general algorithm we first consider the simplified case in which we assume that one of the experts is never wrong. A reasonable algorithm for this case is as follows: 
\begin{itemize}
\item Take a majority vote of the current pool of experts. Once the true realization is revealed, discard all of incorrect experts. In the case that your guess $x_t$ was incorrect, then by definition of the majority
vote at least half of them will be discarded). 
\item Proceed by taking a majority vote for the remaining pool of experts. 
\item Terminate either by reaching the final round $T$ or ending with only the single infallible expert (so will incur zero loss beyond this point). 
\end{itemize}
Since you're discarding at least half of the experts each time you are incorrect, this is just like a binary search for the infallible expert. So after the first mistake you are left with at most
$N/2$ experts, then at most $N/4$ after the second mistake, and in general at most $N/2^m$ after $m$ mistakes. To find the maximum number of mistakes needed to find the infallible expert
we solve $\frac{N}{2^m} = 1$, which yields $m \leq \log_2 N$. Accounting for the possibility that we exhaust the rounds before finding the infallible expert: $m \leq \max\{\log_2 N, T\}$. 
Now, suppose $T >> N$. Let's consider the regret. By definition, the regret for the best expert here is $0$. Thus, 
\[\text{Regret}_T = \frac{1}{T}(\text{\# mistakes}) - 0 \leq \frac{\log_2 N}{T} \to 0 \text{ as } T \to \infty \]
In this case the numerator is quite sub-linear; we can bound it with a constant! Under less restrictive assumptions, the results won't be quite as nice. 

\subsubsection{The Iterated Majority Algorithm}
We now drop the assumption that there exists a majority expert, but can still use the previous algorithm as motivation for the general case. A very simple way to generalize the previous algorithm
is perform the same exact procedure (guess via majority vote, discard incorrect experts), but if you run out of experts just ``reset''; that is, start again with all of the experts. At first glance, this algorithm
makes intuitive sense but seems to have some weaknesses; each time you reset the learner is essentially forgetting everything they learned, which seems unnecessarily inefficient. Moreover, dropping
experts after a single mistake seems overly harsh; a quality expert may make a rare mistake, and dropping this expert will hurt subsequent predictions. We will address these concerns with an improved
algorithm in the next section, but for now let's analyze this algorithm as is. Of course, the baseline for comparison is no longer against a perfect expert that never makes mistakes, but more generally 
against the best expert. 
\begin{prop}
Suppose $m^*_T$ is the number of mistakes made by the best expert after $T$ rounds; that is, $m^*_T := \min_{i = 1, \dots, N} \sum_{t = 1}^{T} \mathbbm{1}\{u^{(i)}_t \neq y_t\}$. 
Let $m_T = \sum_{t = 1}^{T} \mathbbm{1}\{x_t \neq y_t\}$ be the total number of mistakes made by the learner after $T$ rounds. Then 
\[m_T \leq (m_T^* + 1)\log_2 N\]
\end{prop}

\begin{proof}
Let's call the stage prior to the first reset the first \textit{epoch}. Then once we reset we're in the second epoch, etc. Within an epoch, the number of mistakes is bounded by $\log_2 N$, which follows 
directly from the previous analysis in the infallible expert case. Now let $\mathcal{E}_T$ be the current epoch at time $T$. Then, 
\[m_T \leq \mathcal{E}_T \log_2 N\]
Now how do we bound $\mathcal{E}_T$? The key thing to note here is that in order to reset (i.e. to advance to the next epoch), then every expert must have make a mistake; in particular, the best expert
made a mistake. For example, if we're in the second epoch then the best expert must have made at least one mistake. Thus, 
\[m^*_T \geq \mathcal{E}_T - 1\]
or 
\[\mathcal{E}_T \leq m^*_T + 1\]
Putting these bounds together, 
\[m_T \leq \mathcal{E}_T \log_2 N \leq (m^*_T + 1) \log_2 N\]
\end{proof}

Note that when $m_T^* = 0$ (that is, there exists an infallible expert) then we recover the result from the above simplified case. 


\subsubsection{The Weighted Majority Algorithm}
We now consider an improvement to the previous algorithm. To motivate this, we first give a different perspective on weighted majority algorithm in the case of the infallible expert.
We might imagine assigning each expert a weight that encodes our degree of confidence in their advice. In the infallible expert case, we start at round $0$ by assigning each expert
$i$ a weight of $w^{(i)}_0 = 1$ and then when we make a mistake we set any incorrect experts' weights to $0$. This unforgiving approach works because we know there exists an expert who is never wrong and
thus we need not worry about discarding this expert. In the general case, there might not be an infallible expert but there might be various experts who are correct very often and so it would be unwise 
to discard them. The natural extension is thus to decrease their weight by a little bit but not all the way to $0$. In particular, we might choose some $\eta \in (0, 1)$ and decrement the weights of the incorrect
experts by: 
\[w^{(i)}_{t} \gets (1 - \eta) w^{(i)}_{t - 1}\]
This leads to the following algorithm. Here I continue to use that notation that $u_t^{(i)}$ is the advice from expert $i$ at round $t$, and $y_t$ is the true realization at time $t$.
\bigskip

 \begin{algorithm}[H]
	\SetAlgoLined
	
	\textbf{Input}: $0 < \eta < 1$, $T \in \mathbb{N}$, \text{ Experts } X = \{1, \dots, N\} \\
	
	\bigskip
	
	Set $w^{(i)} := 1$ \text{ for all } i = 1, \dots, N \\
	
	\For{$t = 0, \dots, T$} {
		$\phi := \sum_{i \in X} w^{(i)}$ \\
		\If{$\sum_{i: u_t^{(i)} = A} \frac{w_t^{(i)}}{\phi} \geq \frac{1}{2}$} {
			$x_t := A$
		} \Else {
			$x_t := B$
		}
		
		\For{$i = 1, \dots, N$} {
			\If{$u_t^{(i)} \neq y_t$} {
					$w^{(i)} := (1 - \eta)w^{(i)}$ \\
			}
		}
	}			
\caption{Weighted Majority Algorithm}
\end{algorithm}

\bigskip

Notice that the weights are decremented for the incorrect experts regardless of whether $x_t$ was correct or not; we don't let the wrong experts get bailed out by their correct colleagues!
To analyze this algorithm, we apply similar arguments to the infallible expert case. Clearly we will need some sort of assumption on the quality of expert advice. This was unnecessary in the infallible expert
case as we could bound the regret entirely based on the existence of the infallible expert; the quality of all of the remaining experts did not factor into that bound. Without that assumption, the experts could 
be arbitrarily bad, so our bound will be a function of $m_T^{(i)}$, the number of mistakes made by expert $i$ up until time $T$. Moreover, let $m_T$ be the number of mistakes made by the algorithm up until
time $T$ (i.e. number of times that $x_t \neq y_t$). 

\begin{prop}
For any $i \in \{1, 2, \dots, N\}$ and $\eta \in (0, 1/2)$, 
\[m_T \leq 2(1 + \eta)m_T^{(i)} + \frac{\log N}{\eta}\]
In particular, 
\[m_T \leq 2(1 + \eta)m_T^{*} + \frac{\log N}{\eta}\]
where $m_T^{*} := \argmin_{i} m_T^{(i)}$ is the number of mistakes of the best expert. 
\end{prop}
Before diving into the proof it is worth comparing this to the infallible expert result. In that case we found that (for $T >> N$) that $m_T \leq \log_2 N$. We see that the second term in the above 
bound is the analog of this result, but scaled by $\frac{1}{\eta}$. A larger value of $\eta$ will punish the incorrect experts more severely and thus serve to reduce the weight of bad experts
more quickly. However, increasing $\eta$ will \textit{increase} the first term, which captures the notion that enacting this stricter punishment will also have the adverse effect of rapidly decreasing the 
rate of good experts who just happened to be wrong once or twice. Thus the optimal $\eta$ will balance these two factors. On a final note, consider that when $T >> N$ then the first term will 
dominate and thus we conclude for the ``big $T$, small $N$ regime'' that the number of mistakes is bounded by approximately twice the number of mistakes made by the best expert. 

\begin{proof}
Recall that the proof in the infallible expert case proceeded by bounded how many experts where left (i.e. how many with non-zero weight) as a function of how many mistakes were made.  Since the 
weights in this case were either $0$ or $1$ then this can be viewed as bounding the sum of the weights of the experts. We proceed with the same strategy here, but in this case the weights can be any 
real number in $[0, 1]$. To this end, we define $\phi_t := \sum_{i = 1}^{N} w_t^{(i)}$, the sum of the weights at round $t$. $\phi_t$ can be thought of as a potential function of sorts, although it doesn't 
fit into the exact form of potential functions described earlier in this document. Recall that in that section the primary goal was to bound $\Phi_{t} - \Phi_{t - 1}$. In this case, instead of this additive bound 
we will establish a multiplicative bound $\phi_{t} \leq \left(1 - \frac{\eta}{2}\right)\phi_{t - 1}$ for all $t$ when a mistake is made. 
Note that this sharpens the obvious bound $\phi_T \leq \phi_{T - 1}$, which follows from the fact that the weights can only decrease. To this end, suppose a mistake is made at time $t$. This means that 
the greater than $\frac{1}{2}$ of the total weight contributed to a guess that ended up being incorrect. Let $\rho$ denote the proportion of the weight that guessed \textit{correctly}, which implies that 
$\rho < \frac{1}{2}$. Intuitively, at least half of the total weight will suffer the $1 - \eta$ penalty, so we can upper bound $\phi_{t + 1}$ by considering that the smallest possible amount of this weight 
was penalized, which would be exactly one-half of it. Formally, 
\begin{align*}
\phi_{t + 1} &= \rho \phi_t + (1 - \eta)(1 - \rho) \phi_t \\
		 &= \phi_t \left(\rho + (1 - \rho)(1 - \eta)\right) \\
		 &= \phi_t \left(1 - \eta + \eta \rho \right) \\
		 &\leq \phi_t \left(1 - \eta + \frac{1}{2}\eta \right) && \rho < \frac{1}{2} \\
		 &= \phi_t \left(1 - \frac{\eta}{2}\right)
\end{align*}

Now we recurse using this relation, 
\begin{align*}
\phi_T &\leq \left(1 - \frac{\eta}{2}\right)\phi_{T - 1} \\
	  &\leq \left(1 - \frac{\eta}{2}\right)^2\phi_{T - 2} \\
	  &\vdots \\
	  &\leq \left(1 - \frac{\eta}{2}\right)^{m_T}\phi_{0} \\
	  &= \left(1 - \frac{\eta}{2}\right)^{m_T} N \\
\end{align*}
The final line uses the fact that all weights are initialized to $1$, which implies that the sum of weights is
just equal to the number of experts, $N$. Next consider
\[\phi_T = \sum_{i = 1}^{N} w_T^{(i)} > w_T^{(i)} = (1 - \eta)^{m_T^{(i)}}\]
which follows from the fact that the weight for expert $i$ suffers a $1 - \eta$ multiplicative penalty for each mistake. Putting these two bounds together, 
\[(1 - \eta)^{m_T^{(i)}} < \phi_T \leq \left(1 - \frac{\eta}{2}\right)^{m_T}N\]
The result then follows by taking logs, applying the approximation 
\[-x - x^2 < \log(1 - x) < -x \text{ for } x \in \left(0, \frac{1}{2}\right)\]
and then rearranging. 
\end{proof}

One final note on this result: notice that we have not assumed anything about the sequence of realizations nor about the expert advice. It is this generality that makes this algorithm 
so useful. In particular, this bound holds for any arbitrary correlation structure the realizations might have. It even holds when the realizations are chosen by adversary, and for this 
reason this algorithm is quite popular in game theory. 


\subsubsection{A Lower Bound}
We've seen three algorithms that seek to minimize regret in this setting, and the results have been pretty nice. For $T >> N$ then in the above bound we derived, the first term will dominate. We thus have 
something like $\frac{m_T}{m_T^*} \leq 2(1 + \eta)$. In other words, the weighted majority algorithm makes a little over twice as many mistakes as the best expert.
The natural question is, can we do better? It turns out that, if we insist on using a deterministic algorithm, then the answer is no. 
\begin{prop}
Suppose $m_T^* \leq \frac{T}{2}$. Then in the worst-case, $m_T \geq 2m_T^*$ for any deterministic algorithm. 
\end{prop} 

\begin{proof}
This is proved by a very simple scenario. Suppose we have two experts, one of which always advises $A$ and the other always advises $B$. For any fixed deterministic algorithm employed by the 
learner, consider that the adversary chooses the realizations $y_t$ such that $x_t \neq y_t$ for all $t$; that is, the learner always guesses wrong. This is possible due to the fact that the algorithm
is \textit{deterministic}. This is the one subtle point of this proof, so let's look at it a bit more closely. The adversary knows the advice that the experts will offer. The learner's deterministic algorithm 
can be thought of as a mapping $\mathcal{A}: (u_1^{(1)}, \dots, u_T^{(1)}) \times (u_1^{(2)}, \dots, u_T^{(2)}) \to (x_1, \dots, x_T)$; that is, given the advice from the experts the learner will 
always make the same choices. Therefore, we can construct the worst-case scenario that these deterministic choices are never correct. If the algorithm were randomized this wouldn't be possible,
as one realization of choices might be all wrong, but then a different realization could have any number of correct guesses. 
Thus, the learner makes $T$ mistakes but by definition one of the experts much be correct at least half the time. That is, the learner makes at least twice as many mistakes as the best expert.    
\end{proof}

\subsubsection{Introducing Randomness: The Randomized Weighted Majority Algorithm}
With the previous lower bound result, it might seem that we've done as best we can on this problem. But that result holds only for \textit{deterministic} algorithms. If we introduce randomness, 
can we do better (on average)? The randomized weighted majority algorithm (also known as the polynomial weights algorithm) answers this question. This algorithm is almost surprisingly 
similar to its deterministic counterpart: instead of taking a weighted average of the experts as the guess, simply sample one expert at random proportional to their weights, and then heed
this experts advice. Everything else is the same, that's the only difference. 

 \begin{algorithm}[H]
	\SetAlgoLined
	
	\textbf{Input}: $0 < \eta < 1$, $T \in \mathbb{N}$, \text{ Experts } X = \{1, \dots, N\} \\
	
	\bigskip
	
	Set $w^{(i)} := 1$ \text{ for all } i = 1, \dots, N \\
	
	\For{$t = 0, \dots, T$} {
		$\phi := \sum_{i \in X} w^{(i)}$ \\
		$p_i := \frac{w^{(i)}}{\phi}$ \\
		$\text{Sample } j \sim \text{Cat}(N, (p_1, \dots, p_N))$ \\
		$x_t := u_t^{(j)}$ \\
		
		\For{$i = 1, \dots, N$} {
			\If{$u_t^{(i)} \neq y_t$} {
					$w^{(i)} := (1 - \eta)w^{(i)}$ \\
			}
		}
	}			
\caption{Weighted Majority Algorithm}
\end{algorithm}

\bigskip

Note that $\text{Cat}(N, (p_1, \dots, p_N))$ denotes the categorical distribution supported on $\{1, \dots, N\}$, where the integers have associated probabilities $p_1, \dots, p_N$. 
We now show that, on average, this algorithm (when $T >> N$) makes about half the number of mistakes as its deterministic counterpart; that is, the number of mistakes is on par
with that of the best expert, instead of being about twice as many as the best expert. Note that this is on average though, which is a much weaker statement than the deterministic 
guarantees proved above. But in another sense this is a very powerful result; the expected behavior of the algorithm is superior to the previous algorithms we considered and its 
randomized nature makes it suitable for adversarial settings. 

\begin{thm}
For any $i \in \{1, \dots, N\}$, then the expected number of mistakes made my the randomized weighted majority algorithm after $T$ rounds is bounded as follows.
\[\E[m_T] \leq (1 + \eta)m_T^{(i)} + \frac{\log N}{\eta}\]
In particular this bound holds relative to the best expert. 
\[\E[m_T] \leq (1 + \eta)m_T^{*} + \frac{\log N}{\eta}\]
\end{thm}

\begin{proof}
\textbf{TODO} (both Harvard and UPenn notes and the Arora paper have this proof)
\end{proof}

Recall that the primary objective we seek to minimize in the online learning setting is \textit{regret}. The above bound directly gives us a bound on the regret. 
\[\text{Regret}_T = \E[m_T] - m_T^* \leq (1 + \eta)m_T^{*} + \frac{\log N}{\eta} - m_T^* = \eta m_T^* + \frac{\log N}{\eta}\]
We haven't yet considered choosing $\eta$. Clearly, we want to optimize this parameter to balance the opposing effects of the two terms. Recall that 
we discussed similar tradeoffs in the deterministic case; the first term increasing in $\eta$ could be thought of as the cost of ``over-penalizing'' good experts
while the second term decreasing in $\eta$ was interpreted as quantifying the benefits of increasing $\eta$ in that the bad experts may be more 
readily ignored. In optimizing $\epsilon$ note that we want to make sure the value is not a function of $m_T^*$ as that quantity is unknown to the learner. 
So while we could simply differentiate the regret with respect to $\eta$ (the function is differentiable for fixed $T$) then set to $0$ to optimize, this would lead
to $\eta$ as a function of $m_T^*$ which we don't want. Therefore, we first use the trivial bound $m_T^* \leq T$ and only \textit{then} take the derivative. 
\[\eta m_T^* + \frac{\log N}{\eta} \leq \eta T + \frac{\log N}{\eta}\]
Taking the derivative with respect to $\eta$, setting equal to $0$, and solving for $\eta$ we obtain the optimum:
\[\eta = \sqrt{\frac{\log N}{T}}\]
Plugging this back in to the bound on $\text{Regret}_T$ yields
\[\text{Regret}_T \leq  \sqrt{\frac{\log N}{T}} m_T^* +  \sqrt{\frac{T}{\log N}}\log N = \sqrt{T \log N} + \sqrt{T \log N} = 2\sqrt{T \log N}\]
If we think all the way back to the section introducing the concept of regret, we noted that the goal is to have regret grow sub-linearly in $T$, which corresponded 
to the quality of our guesses improving over time. We see that we have accomplished this goal here! This also implies that the average regret converges to $0$
at rate $\frac{1}{\sqrt{T}}$. 
\[\overline{\text{Regret}}_T \leq \frac{1}{T} \cdot 2\sqrt{T \log N} = 2\sqrt{\frac{\log N}{T}}\]

\subsubsection{A lower bound for randomized algorithms}
Recall that we established a lower bound of $m_T \geq 2m_T^*$ for any deterministic algorithm. In other words, $\overline{\text{Regret}}_T \geq \frac{m_T^*}{T} = O\left(\frac{1}{T}\right)$.
We just showed that in expectation this can be beaten by 
a randomized algorithm, which achieved $\overline{\text{Regret}}_T = O\left(\frac{1}{\sqrt{T}}\right)$. The natural question is whether this algorithm is optimal among all randomized 
algorithms. 
\textbf{TODO}: from homework problem 

% Section: Applications of the Randomized Weighted Majority Algorithm
\section{Applications of the Randomized Weighted Majority Algorithm}


\subsection{Online Learning and Prediction}
In the previous section we considered the problem of learning from expert advice. Our goal was to combine the advice from the experts in order to construct a series of choices that would
minimize our regret. We now consider a slight modification to this setup: instead of combining expert advice, we consider \textit{expert} a single expert each round. In this case, we can think 
of the experts as \textit{options} or \textit{strategies} that we must choose. This is a very natural setting in game theory; we might imagine we're playing a sequential game against and adversary, 
and we pick a strategy each round to try to minimize our losses in the game. What I've just described is essentially a two-player game with \textit{pure strategies}. We will instead consider the 
\textit{mixed strategy} setting in which during each round the player picks a probability distribution over the set of strategies, and then the strategy for that round is determined by a random sample from this 
distribution. It should not be surprising that we want to consider such a randomized setting; recall in the previous section that we were able to improve performance on average by randomizing the algorithm. 
We will also generalize from considering zero-one loss to allowing any real-valued loss in $[0, 1]$. The setup is summarized as follows. \\
For each round $t = 1, \dots, T$:
\begin{itemize}
\item Learner pick a probability distribution $p^{(t)}$ supported on the set of $n$ possible options. 
\item The costs of \textit{all} options are now revealed. The losses are denoted by $m^{(t)} \in [0, 1]^n$, where $m^{(t)}_i \in [0, 1]$ is the loss for option $i$. 
\item The loss of the learner for round $t$ is defined as the expected loss: $\E_{i \sim p^{(t)}}[m^{(t)}_i] = \langle p^{(t)}, m^{(t)}\rangle$
\end{itemize} 
As usual, we want to minimize the cumulative loss relative to the best decision in hindsight. 
\[\textit{Regret}_T = \sum_{t = 1}^{T} \langle p^{(t)}, m^{(t)}\rangle - \min_i \sum_{t = 1}^{T} m_i^{(t)}\]

\subsection{Using Randomized Weighted Majority to Prove the Minimax Theorem}
\textbf{TODO: Lecture 6 or Aaron Roth's notes}
A constructive version of LP duality. 


% Section: Online Convex Optimization
\section{Online Convex Optimization}






\section{Resources}
\begin{itemize}
\item Potential Function Proofs for Gradient Methods (Bansal and Gupta) 
\item Francesco Orabana (Boston University) lecture notes on online optimization
\item Alina Ene (Boston University) lecture notes from Advanced Optimization Algorithms Spring 2022 course
\item Aaron Singer (Harvard University) lecture notes from Advanced Optimization
\item Aaron Roth (UPenn) lecture notes from Algorithmic Game Theory (lectures 4 and 5)
\item The Multiplicative Weights Update Algorithm: A Meta-Algorithm and Applications (Arora)
\end{itemize}



\end{document}

