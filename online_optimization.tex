\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bbm} % For mathbbm for indicator function
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

\begin{document}

\begin{center}
\Large
Optimization Under Uncertainty/Online Optimization
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

\section{Introduction}
This text contains a brief introduction to the field of online optimization. ``Online'' here refers to settings in which one has limited or no knowledge about the future; thus, we might 
also call this subject ``optimization under uncertainty''. This is opposed to the ``offline'' setting dealt with in classical optimization theory. Take linear programming for instance: here we
assume complete knowledge of the problem in the form of the objective function and relevant constraints. This complete information assumption may be unrealistic in certain applications. 
As an example, let's consider two-player games from game theory. In the classical setup, it is assumed that both players have perfect knowledge of the others strategies (or mixed strategies). 
A natural extension that falls under the umbrella of online learning is to consider the situation in which the adversary's strategies are unknown. In these kind of applications the decision problem 
may be sequential in nature; that is, a player is required to make sequential decisions responding to those of the adversary. However, online learning also includes problems in which a decision is 
made only once. 

\section{General Setting and Regret Minimization}
Consider the following game repeated over $T$ rounds: 
\begin{itemize}
\item An adversary picks $y_t \in [0, 1]$
\item You pick $x_t \in [0, 1]$, trying to guess the adversary's number
\item The adversary's number is revealed and you face an $\ell_2$ penalty: $(x_t - y_t)^2$
\end{itemize}
The question is: how should you guess in order to incur minimal cumulative loss? Currently, under these very light assumptions we can essentially say nothing. For example, suppose the adversary draws 
$y_t$ randomly from a completely different distribution (unknown to you) at each round $t$. This game seems somewhat hopeless for the player. However, what if we add more structure? Suppose we know 
in advance that the adversary always draws $y_t \overset{iid}{\sim} \mathcal{U}[0, 1]$. Let $Y \sim \mathcal{U}[0, 1]. $In this case, since we know that the mean minimizes $\ell_2$ loss we should guess 
$x_t := \E Y= \frac{1}{2}$. Letting $x = (x_1, \dots, x_T)^T$ and $y = (y_1, \dots, y_T)^T$ the cumulative loss incurred would thus be
\[\text{loss}(x, y) := \norm{x - y}_2^2 = \sum_{t = 1}^{T} (y_t - x_t)^2 = \sum_{t = 1}^{T} (Y_t - \E Y_t)^2 \]
\[\E_Y[\text{loss}(x, y)] =  \sum_{t = 1}^{T} \E(Y_t - \E Y_t)^2 = \sum_{t = 1}^{T} \Var(Y) = T\Var(Y) = \frac{T}{12}\]
where we use the fact that the $Y_t$ are iid and the final step recalls the variance of a uniform random variable. With respect to $\ell_2$ loss, this is the best we can do. Note that there was nothing special 
about the uniform distribution here; the key assumption was that the adversary picked numbers iid from a distribution that we knew in advance. In general, if this distribution has variance $\sigma^2$ then 
by playing our optimal choice of the mean of the distribution we will incur cumulative loss $\sigma^2 T$. 

The assumption that we know the distribution is quite strong, and seems unreasonable to hold in real applications. So what can we say about the situation where the distribution is unknown in advance?
We're still assuming that the adversary picks a fixed distribution at the beginning and samples iid from that distribution each round. Intuitively, we would imagine that our guesses will likely be pretty bad at first, 
but that if $T$ is sufficiently large we can, over time, start to get a sense of the underlying distribution and tune our guesses accordingly. We certainly can't hope to compete with the minimal loss incurred in the 
known distribution setting. Thus, $\sigma^2 T$ seems to be a good benchmark - we can measure our performance relative to this idealized setting. 
\[\text{loss}(x, y) := \E_Y \norm{X - Y}_2^2 - \sigma^2 T\]
Note that the expectation is taken with respect to the distribution chosen by the adversary, which is unknown the the player. 
It is also useful to divide through by $T$ and consider the average loss instead. 
\[\text{loss}_{\text{avg}}(x, y) := \frac{1}{T} \E_Y \norm{X - Y}_2^2 - \sigma^2\]
What can we hope for? Recall our intuition that over time we hope to be able to \textit{learn} the underlying distribution and thus hope that our estimates will tend towards the estimates we would make
when the distribution is known. That is, we hope to be able to show that the average loss tends to $0$ in the limit as $T \to \infty$. What does this imply about $\text{loss}(x, y)$? Well, first of all we should 
not expect this to tend to $0$ since this measures the difference in the \textit{cumulative} loss with respect to the cumulative loss of the idealized setting. Since at first we expect our guesses to be significantly
worse than in the idealized case, then the large loss accumulated for these early guesses will not go away - the cumulative loss will just keep adding to this. However, since we expect our guesses to get better
in time then we expect the \textit{rate} of accumulation to decrease over time. For example, if $\text{loss}(x, y)$ grew linearly then this would imply a constant rate of accumulation, which would mean our guesses
are not improving over time. Thus, our hope is that $\text{loss}(x, y)$ grows \textit{sub-linearly} (could be logarithmically, square root, etc.). This makes sense because when we divide through by $T$ in order 
to consider the average growth relative to the baseline then in order for this average distance to tend to $0$ then the denominator (which is growing linearly) must grow faster than the numerator (hence the 
numerator must grow \textit{sub-linearly}). 

With this intuition established, we proceed with the last couple steps in order to arrive at our final performance metric. I begin by re-writing $\text{loss}(x, y)$ in an equivalent way
\[\text{loss}(x, y) = \E_Y \norm{X - Y}_2^2 - \min_{x \in [0, 1]} \E_Y \norm{X - Y}_2^2 \]
Recall that this follows from the fact that $\sigma^2 T$ was the loss when the optimal $x \equiv x_t$ was chosen to minimize the $\ell_2$ distance. This above expression just explicitly 
writes out this optimization perspective. Now for another generalization: we have thus far been making the restrictive assumption that the adversary chooses a fixed distribution at the start of the game. We want to 
define a general performance metric to applies in cases without any sort of distributional assumption. We can thus simply take the previous expression and remove all randomness from the expression, 
just treating the $x_t$ and $y_t$ as sequences of real numbers. The resulting expression is known as \textit{regret}, which captures the notion that once the true realizations are revealed you regret 
the choices you made  and would have rather chosen the true values. In other words, it expresses what you would have chosen \textit{in hindsight}. 
\[\text{Regret}_T(x, y) = \norm{x - y}_2^2 - \min_{x \in [0, 1]} \norm{x - y}_2^2 \]
We will consider that we've ``won'' the game if $\text{Regret}_T(x, y)$ grows sub-linearly in $T$: 
\[\lim_{T \to \infty} \frac{\text{Regret}_T(x, y)}{T} = 0\]
which we've seen is another way of saying that the average regret tends to $0$. We have now arrived at the final definition of loss that we will consider. This definition will generalize a few restricting assumptions
we have been making: 
\begin{itemize}
\item Instead of considering $\ell_2$ loss we will instead consider an arbitrary loss function $\ell$. We need not even explicitly consider $y$ anymore; all we need to be able to say is that 
a guess $x_t$ generalizes some loss $\ell(x_t)$. 
\item We will now allow for different loss functions at each round of the game, $\ell_t$, $t = 1, \dots, T$. 
\item We drop the assumption that the values must be in $[0, 1]$, instead considering subsets of $d$-dimensional Euclidean space.  
\item Instead of parameterizing the regret with respect to the idealized setting we have been considering, we allow the framework to be flexible and just parameterize it with respect to 
some guess $u \in \R^d$ (in the above examples, $u$ was the mean of the distribution). 
\end{itemize}
\begin{definition}
Let $x = (x_1, \dots, x_T)^T$, where $x_t \in V \subset \R^d$ be the vectors output by an algorithm at each of $T$ rounds. Suppose the values incur loss $\ell_t(x_t)$ at round $t$, where $\ell_t: \R^d \to \R$
for $t = 1, \dots, T$. Also let $u \in V$ be a ``predictor'' which the loss will be measured with respect to. We then define the \textit{regret} as: 
\[\text{Regret}_T(x, u) := \sum_{t = 1}^{T} \ell_t(x_t) - \sum_{t = 1}^{T} \ell_t(u) \]
\end{definition}
As we will see, the generality of this definition will be powerful in allowing it to apply in a diverse array of settings. Note that there are many different assumptions we could tweak in this framework; for example, 
we can suppose that the loss functions $\ell_t$ are known to the player in advance, or are they chosen by the adversary and only revealed once the player has made their guess $x_t$? The latter assumption, along 
with the assumption that the $\ell_t$ are convex, form the general \textit{online convex optimization} framework. We can also generalize further to allow the baseline $u$ be a sequence $u_1, \dots, u_T$ which 
we measure loss with respect to. 

\section{Regret Minimization: A Simple Example}
\textbf{TODO: include the example from Prof Orabana's first week of notes}

\section{A Note on Potential Function Proofs}
This section provides a very brief introduction to a common proof technique commonly utilized in proofs of convergence for iterative optimization algorithms. For an in-depth discussion of this topic see 
Banal and Gupta's paper mentioned in the references. The potential function framework (not to be confused with potential functions from vector calculus/physics) is utilized in the proofs of many optimization algorithms, 
including in the online setting. 

First recall the general goal of these algorithms: supposing $f$ is convex, we seek to approximate its minimum $f(x^*)$. It is common to try to bound the additive error $f(x_T) - f(x^*) < \epsilon$, where $x_T$
is the estimate of the true minimizer $x^*$ produced by an algorithm after $T$ iterations. The goal is to construct an upper bound on this additive error as a function of $T$. 

Potential functions are a unifying technique that helps in constructing such a bound. Note that above we are considering the distance in function values $f(x_T) - f(x^*)$. We might also consider the distance 
between the minimizers $d(x_T, x^*)$, where I intentionally keep the distance metric unspecified for now (could be $\ell_2$, etc.). If $f$ is continuous then we can easily convert between bounds on the function
value distance and bounds on the minimizing vector distance. The potential function is defined to take into account both of these distances: 
\[\Phi_t := a_t [f(x_t) - f(x^*)] + b_t \cdot d(x_t, x^*)\]
where $a_t, b_t$ are non-negative multipliers that can be tailored to the needs of different proofs. The justification for the above definition is that bounding the potential function will induce a bound on 
$f(x_T) - f(x^*)$. Indeed, suppose we obtain the bound: 
\[\Phi_{t + 1} - \Phi_t \leq B_t\]
Then this implies
\[\sum_{t = 1}^{T - 1} [\Phi_{t + 1} - \Phi_t] \leq  \sum_{t = 1}^{T - 1} B_t\]
The lefthand side is a telescoping sum and therefore we're left with 
\[\Phi_{T} - \Phi_0 \leq  \sum_{t = 1}^{T - 1} B_t\]
Moving $\Phi_0$ to the other side and expanding $\Phi_{T}$, 
\[ a_T [f(x_T) - f(x^*)] + b_T \cdot d(x_T, x^*) \leq \Phi_0 + \sum_{t = 1}^{T - 1} B_t\]
Thus, 
\[ f(x_t) - f(x^*) \leq \frac{1}{a_T}\left[\Phi_0 - b_T \cdot d(x_T, x^*) + \sum_{t = 1}^{T - 1} B_t\right] \leq \frac{\Phi_0 + \sum_{t = 1}^{T - 1} B_t}{a_T}\]
A couple notes: first, in the last step we simplify the bound by dropping $b_T \cdot d(x_T, x^*)$. In general, we're considering $T$ large enough such that $d(x_T, x^*)$ is small and thus
this final inequality shouldn't give up too much in the bound. Next, we observe that $\Phi_0$ encodes the error given by the initialization $x_0$ of the algorithm (the ``initial guess''). This potential
function framework provides a nice organization for many convergence proofs; you'll see many convergence proofs of gradient descent, etc. that feature this telescoping sum step without explicitly 
considering a potential function, but I find that explicitly defining a potential function helps me draw parallels between similar proofs. 

\subsection{TODO: include example of a potential function proof from the Banal and Gupta paper}


% Section: Learning From Experts
\section{Learning from Experts}
One important online optimization problem is that of learning from expert advice. The problem is typically defined as follows: a decision-maker must make a decision at each of $T$ rounds. Here let's 
consider a simple set of possible realizations $V = \{A, B\}$ so that this can be thought of as a repeated binary classification problem. We consider the zero-one loss 
$\ell(x_t, y_t) = \mathbbm{1}(x_t \neq y_t)$. Typically in binary classification we base the classification on information 
given in the form of \textit{features} or \textit{predictors}. In this setting we refer to the analogous concept as \textit{experts}; in particular, we consider a set of $N$ experts that each recommend a decision
in $\{A, B\}$ each time step. Your goal is thus to use this expert advice in order to make the best possible decisions. Note that one's first approach in solving this problem would likely be to develop an 
algorithm to minimize the number of mistakes: 
\[\min_{x_t} \sum_{t = 1}^{T} \ell(x_t, y_t)\]
However, this is not a good baseline to evaluate our algorithms. Indeed, it can be shown that no randomized algorithm can make fewer than $\frac{T}{2}$ mistakes in expectation. Moreover, the trivial algorithm 
of simply flipping a coin at each step $x_t \overset{iid}{\sim} \text{Bern}(1/2)$ achieves this lower bound. So this turned out to be very uninteresting: $\frac{T}{2}$ mistakes was both optimal and trivial 
to obtain. The solution of course is to instead consider the \textit{regret}, which requires choosing a useful benchmark. Recalling the general definition of regret, we therefore must decide on a suitable 
predictor $u$. In the learning from experts setting, a natural choice is to consider loss with respect to the loss of the \textit{best expert}. We therefore consider
\[\text{Regret}_T(x, u) := \frac{1}{T} \sum_{t = 1}^{T} \ell_t(x_t) - \frac{1}{T} \sum_{t = 1}^{T} \ell_t(u_t) = \frac{1}{T} \sum_{t = 1}^{T} \mathbbm{1}\{x_t \neq y_t\} - \min_{1 \leq i \leq N} \frac{1}{T} \sum_{t = 1}^{T} \mathbbm{1}\{u^{(i)}_t \neq y_t\}\]
where $u^{(i)}_t$ denotes the advice from the $i^{\text{th}}$ expert at round $t$. Thus in the first expression $u_t$ is the advice from the best expert at round $t$. Note that we're considering what we previously 
called \textit{average regret} here. I've just dropped the ``avg'' subscript to lighten notation (often when people talk about regret they're talking about this averaged version anyways). 

\subsection{The Weighted Majority Algorithm}

\subsubsection{Simple Case of the Infallible Expert}
To motivate the general algorithm we first consider the simplified case in which we assume that one of the experts is never wrong. A reasonable algorithm for this case is as follows: 
\begin{itemize}
\item Take a majority vote of the current pool of experts. Once the true realization is revealed, discard all of incorrect experts. In the case that your guess $x_t$ was incorrect, then by definition of the majority
vote at least half of them will be discarded). 
\item Proceed by taking a majority vote for the remaining pool of experts. 
\item Terminate either by reaching the final round $T$ or ending with only the single infallible expert (so will incur zero loss beyond this point). 
\end{itemize}
Since you're discarding at least half of the experts each time you are incorrect, this is just like a binary search for the infallible expert. So after the first mistake you are left with at most
$N/2$ experts, then at most $N/4$ after the second mistake, and in general at most $N/2^m$ after $m$ mistakes. To find the maximum number of mistakes needed to find the infallible expert
we solve $\frac{N}{2^m} = 1$, which yields $m \leq \log_2 N$. Accounting for the possibility that we exhaust the rounds before finding the infallible expert: $m \leq \max\{\log_2 N, T\}$. 
Now, suppose $T >> N$. Let's consider the regret. By definition, the regret for the best expert here is $0$. Thus, 
\[\text{Regret}_T = \frac{1}{T}(\text{\# mistakes}) - 0 \leq \frac{\log_2 N}{T} \to 0 \text{ as } T \to \infty \]
In this case the numerator is quite sub-linear; we can bound it with a constant! Under less restrictive assumptions, the results won't be quite as nice. 

\subsubsection{The Iterated Majority Algorithm}
We now drop the assumption that there exists a majority expert, but can still use the previous algorithm as motivation for the general case. A very simple way to generalize the previous algorithm
is perform the same exact procedure (guess via majority vote, discard incorrect experts), but if you run out of experts just ``reset''; that is, start again with all of the experts. At first glance, this algorithm
makes intuitive sense but seems to have some weaknesses; each time you reset the learner is essentially forgetting everything they learned, which seems unnecessarily inefficient. Moreover, dropping
experts after a single mistake seems overly harsh; a quality expert may make a rare mistake, and dropping this expert will hurt subsequent predictions. We will address these concerns with an improved
algorithm in the next section, but for now let's analyze this algorithm as is. Of course, the baseline for comparison is no longer against a perfect expert that never makes mistakes, but more generally 
against the best expert. 
\begin{prop}
Suppose $m^*_T$ is the number of mistakes made by the best expert after $T$ rounds; that is, $m^*_T := \min_{i = 1, \dots, N} \sum_{t = 1}^{T} \mathbbm{1}\{u^{(i)}_t \neq y_t\}$. 
Let $m_T = \sum_{t = 1}^{T} \mathbbm{1}\{x_t \neq y_t\}$ be the total number of mistakes made by the learner after $T$ rounds. Then 
\[m_T \leq (m_T^* + 1)\log_2 N\]
\end{prop}

\begin{proof}
Let's call the stage prior to the first reset the first \textit{epoch}. Then once we reset we're in the second epoch, etc. Within an epoch, the number of mistakes is bounded by $\log_2 N$, which follows 
directly from the previous analysis in the infallible expert case. Now let $\mathcal{E}_T$ be the current epoch at time $T$. Then, 
\[m_T \leq \mathcal{E}_T \log_2 N\]
Now how do we bound $\mathcal{E}_T$? The key thing to note here is that in order to reset (i.e. to advance to the next epoch), then every expert must have make a mistake; in particular, the best expert
made a mistake. For example, if we're in the second epoch then the best expert must have made at least one mistake. Thus, 
\[m^*_T \geq \mathcal{E}_T - 1\]
or 
\[\mathcal{E}_T \leq m^*_T + 1\]
Putting these bounds together, 
\[m_T \leq \mathcal{E}_T \log_2 N \leq (m^*_T + 1) \log_2 N\]
\end{proof}

Note that when $m_T^* = 0$ (that is, there exists an infallible expert) then we recover the result from the above simplified case. 


\subsubsection{The Weighted Majority Algorithm}
We now consider an improvement to the previous algorithm. To motivate this, we first give a different perspective on weighted majority algorithm in the case of the infallible expert.
We might imagine assigning each expert a weight that encodes our degree of confidence in their advice. In the infallible expert case, we start at round $0$ by assigning each expert
$i$ a weight of $w^{(i)}_0 = 1$ and then when we make a mistake we set any incorrect experts' weights to $0$. This unforgiving approach works because we know there exists an expert who is never wrong and
thus we need not worry about discarding this expert. In the general case, there might not be an infallible expert but there might be various experts who are correct very often and so it would be unwise 
to discard them. The natural extension is thus to decrease their weight by a little bit but not all the way to $0$. In particular, we might choose some $\eta \in (0, 1)$ and decrement the weights of the incorrect
experts by: 
\[w^{(i)}_{t} \gets (1 - \eta) w^{(i)}_{t - 1}\]
This leads to the following algorithm. Here I continue to use that notation that $u_t^{(i)}$ is the advice from expert $i$ at round $t$, and $y_t$ is the true realization at time $t$.
\bigskip

 \begin{algorithm}[H]
	\SetAlgoLined
	
	\textbf{Input}: $0 < \eta < 1$, $T \in \mathbb{N}$, \text{ Experts } X = \{1, \dots, N\} \\
	
	\bigskip
	
	Set $w^{(i)} := 1$ \text{ for all } i = 1, \dots, N \\
	
	\For{$t = 0, \dots, T$} {
		$\phi := \sum_{i \in X} w^{(i)}$ \\
		\If{$\sum_{i: u_t^{(i)} = A} \frac{w_t^{(i)}}{\phi} \geq \frac{1}{2}$} {
			$x_t := A$
		} \Else {
			$x_t := B$
		}
		
		\For{$i = 1, \dots, N$} {
			\If{$u_t^{(i)} \neq y_t$} {
					$w^{(i)} := (1 - \eta)w^{(i)}$ \\
			}
		}
	}			
\caption{Weighted Majority Algorithm}
\end{algorithm}

\bigskip

Notice that the weights are decremented for the incorrect experts regardless of whether $x_t$ was correct or not; we don't let the wrong experts get bailed out by their correct colleagues!
To analyze this algorithm, we apply similar arguments to the infallible expert case. Clearly we will need some sort of assumption on the quality of expert advice. This was unnecessary in the infallible expert
case as we could bound the regret entirely based on the existence of the infallible expert; the quality of all of the remaining experts did not factor into that bound. Without that assumption, the experts could 
be arbitrarily bad, so our bound will be a function of $m_T^{(i)}$, the number of mistakes made by expert $i$ up until time $T$. Moreover, let $m_T$ be the number of mistakes made by the algorithm up until
time $T$ (i.e. number of times that $x_t \neq y_t$). 

\begin{prop}
For any $i \in \{1, 2, \dots, N\}$ and $\eta \in (0, 1/2)$, 
\[m_T \leq 2(1 + \eta)m_T^{(i)} + \frac{\log N}{\eta}\]
In particular, 
\[m_T \leq 2(1 + \eta)m_T^{*} + \frac{\log N}{\eta}\]
where $m_T^{*} := \argmin_{i} m_T^{(i)}$ is the number of mistakes of the best expert. 
\end{prop}
Before diving into the proof it is worth comparing this to the infallible expert result. In that case we found that (for $T >> N$) that $m_T \leq \log_2 N$. We see that the second term in the above 
bound is the analog of this result, but scaled by $\frac{1}{\eta}$. A larger value of $\eta$ will punish the incorrect experts more severely and thus serve to reduce the weight of bad experts
more quickly. However, increasing $\eta$ will \textit{increase} the first term, which captures the notion that enacting this stricter punishment will also have the adverse effect of rapidly decreasing the 
rate of good experts who just happened to be wrong once or twice. Thus the optimal $\eta$ will balance these two factors. On a final note, consider that when $T >> N$ then the first term will 
dominate and thus we conclude for the ``big $T$, small $N$ regime'' that the number of mistakes is bounded by approximately twice the number of mistakes made by the best expert. 

\begin{proof}
Recall that the proof in the infallible expert case proceeded by bounded how many experts where left (i.e. how many with non-zero weight) as a function of how many mistakes were made.  Since the 
weights in this case were either $0$ or $1$ then this can be viewed as bounding the sum of the weights of the experts. We proceed with the same strategy here, but in this case the weights can be any 
real number in $[0, 1]$. To this end, we define $\phi_t := \sum_{i = 1}^{N} w_t^{(i)}$, the sum of the weights at round $t$. $\phi_t$ can be thought of as a potential function of sorts, although it doesn't 
fit into the exact form of potential functions described earlier in this document. Recall that in that section the primary goal was to bound $\Phi_{t} - \Phi_{t - 1}$. In this case, instead of this additive bound 
we will establish a multiplicative bound $\phi_{t} \leq \left(1 - \frac{\eta}{2}\right)\phi_{t - 1}$ for all $t$ when a mistake is made. 
Note that this sharpens the obvious bound $\phi_T \leq \phi_{T - 1}$, which follows from the fact that the weights can only decrease. To this end, suppose a mistake is made at time $t$. This means that 
the greater than $\frac{1}{2}$ of the total weight contributed to a guess that ended up being incorrect. Let $\rho$ denote the proportion of the weight that guessed \textit{correctly}, which implies that 
$\rho < \frac{1}{2}$. Intuitively, at least half of the total weight will suffer the $1 - \eta$ penalty, so we can upper bound $\phi_{t + 1}$ by considering that the smallest possible amount of this weight 
was penalized, which would be exactly one-half of it. Formally, 
\begin{align*}
\phi_{t + 1} &= \rho \phi_t + (1 - \eta)(1 - \rho) \phi_t \\
		 &= \phi_t \left(\rho + (1 - \rho)(1 - \eta)\right) \\
		 &= \phi_t \left(1 - \eta + \eta \rho \right) \\
		 &\leq \phi_t \left(1 - \eta + \frac{1}{2}\eta \right) && \rho < \frac{1}{2} \\
		 &= \phi_t \left(1 - \frac{\eta}{2}\right)
\end{align*}

Now we recurse using this relation, 
\begin{align*}
\phi_T &\leq \left(1 - \frac{\eta}{2}\right)\phi_{T - 1} \\
	  &\leq \left(1 - \frac{\eta}{2}\right)^2\phi_{T - 2} \\
	  &\vdots \\
	  &\leq \left(1 - \frac{\eta}{2}\right)^{m_T}\phi_{0} \\
	  &= \left(1 - \frac{\eta}{2}\right)^{m_T} N \\
\end{align*}
The final line uses the fact that all weights are initialized to $1$, which implies that the sum of weights is
just equal to the number of experts, $N$. Next consider
\[\phi_T = \sum_{i = 1}^{N} w_T^{(i)} > w_T^{(i)} = (1 - \eta)^{m_T^{(i)}}\]
which follows from the fact that the weight for expert $i$ suffers a $1 - \eta$ multiplicative penalty for each mistake. Putting these two bounds together, 
\[(1 - \eta)^{m_T^{(i)}} < \phi_T \leq \left(1 - \frac{\eta}{2}\right)^{m_T}N\]
The result then follows by taking logs, applying the approximation 
\[-x - x^2 < \log(1 - x) < -x \text{ for } x \in \left(0, \frac{1}{2}\right)\]
and then rearranging. 
\end{proof}

One final note on this result: notice that we have not assumed anything about the sequence of realizations nor about the expert advice. It is this generality that makes this algorithm 
so useful. In particular, this bound holds for any arbitrary correlation structure the realizations might have. It even holds when the realizations are chosen by adversary, and for this 
reason this algorithm is quite popular in game theory. 


\subsubsection{A Lower Bound}
We've seen three algorithms that seek to minimize regret in this setting, and the results have been pretty nice. For $T >> N$ then in the above bound we derived, the first term will dominate. We thus have 
something like $\frac{m_T}{m_T^*} \leq 2(1 + \eta)$. In other words, the weighted majority algorithm makes a little over twice as many mistakes as the best expert.
The natural question is, can we do better? It turns out that, if we insist on using a deterministic algorithm, then the answer is no. 
\begin{prop}
Suppose $m_T^* \leq \frac{T}{2}$. Then in the worst-case, $m_T \geq 2m_T^*$ for any deterministic algorithm. 
\end{prop} 

\begin{proof}
This is proved by a very simple scenario. Suppose we have two experts, one of which always advises $A$ and the other always advises $B$. For any fixed deterministic algorithm employed by the 
learner, consider that the adversary chooses the realizations $y_t$ such that $x_t \neq y_t$ for all $t$; that is, the learner always guesses wrong. This is possible due to the fact that the algorithm
is \textit{deterministic}. This is the one subtle point of this proof, so let's look at it a bit more closely. The adversary knows the advice that the experts will offer. The learner's deterministic algorithm 
can be thought of as a mapping $\mathcal{A}: (u_1^{(1)}, \dots, u_T^{(1)}) \times (u_1^{(2)}, \dots, u_T^{(2)}) \to (x_1, \dots, x_T)$; that is, given the advice from the experts the learner will 
always make the same choices. Therefore, we can construct the worst-case scenario that these deterministic choices are never correct. If the algorithm were randomized this wouldn't be possible,
as one realization of choices might be all wrong, but then a different realization could have any number of correct guesses. 
Thus, the learner makes $T$ mistakes but by definition one of the experts much be correct at least half the time. That is, the learner makes at least twice as many mistakes as the best expert.    
\end{proof}

\subsubsection{Introducing Randomness: The Randomized Weighted Majority Algorithm}
With the previous lower bound result, it might seem that we've done as best we can on this problem. But that result holds only for \textit{deterministic} algorithms. If we introduce randomness, 
can we do better (on average)? The randomized weighted majority algorithm (also known as the polynomial weights algorithm) answers this question. This algorithm is almost surprisingly 
similar to its deterministic counterpart: instead of taking a weighted average of the experts as the guess, simply sample one expert at random proportional to their weights, and then heed
this experts advice. Everything else is the same, that's the only difference. 

 \begin{algorithm}[H]
	\SetAlgoLined
	
	\textbf{Input}: $0 < \eta < 1$, $T \in \mathbb{N}$, \text{ Experts } X = \{1, \dots, N\} \\
	
	\bigskip
	
	Set $w^{(i)} := 1$ \text{ for all } i = 1, \dots, N \\
	
	\For{$t = 0, \dots, T$} {
		$\phi := \sum_{i \in X} w^{(i)}$ \\
		$p_i := \frac{w^{(i)}}{\phi}$ \\
		$\text{Sample } j \sim \text{Cat}(N, (p_1, \dots, p_N))$ \\
		$x_t := u_t^{(j)}$ \\
		
		\For{$i = 1, \dots, N$} {
			\If{$u_t^{(i)} \neq y_t$} {
					$w^{(i)} := (1 - \eta)w^{(i)}$ \\
			}
		}
	}			
\caption{Randomized Weighted Majority Algorithm}
\end{algorithm}

\bigskip

Note that $\text{Cat}(N, (p_1, \dots, p_N))$ denotes the categorical distribution supported on $\{1, \dots, N\}$, where the integers have associated probabilities $p_1, \dots, p_N$. 
We now show that, on average, this algorithm (when $T >> N$) makes about half the number of mistakes as its deterministic counterpart; that is, the number of mistakes is on par
with that of the best expert, instead of being about twice as many as the best expert. Note that this is on average though, which is a much weaker statement than the deterministic 
guarantees proved above. But in another sense this is a very powerful result; the expected behavior of the algorithm is superior to the previous algorithms we considered and its 
randomized nature makes it suitable for adversarial settings. 

\begin{thm}
For any $i \in \{1, \dots, N\}$, then the expected number of mistakes made my the randomized weighted majority algorithm after $T$ rounds is bounded as follows.
\[\E[m_T] \leq (1 + \eta)m_T^{(i)} + \frac{\log N}{\eta}\]
In particular this bound holds relative to the best expert. 
\[\E[m_T] \leq (1 + \eta)m_T^{*} + \frac{\log N}{\eta}\]
\end{thm}

\begin{proof}
\textbf{TODO} (both Harvard and UPenn notes and the Arora paper have this proof)
\end{proof}

Recall that the primary objective we seek to minimize in the online learning setting is \textit{regret}. The above bound directly gives us a bound on the regret. 
\[\text{Regret}_T = \E[m_T] - m_T^* \leq (1 + \eta)m_T^{*} + \frac{\log N}{\eta} - m_T^* = \eta m_T^* + \frac{\log N}{\eta}\]
We haven't yet considered choosing $\eta$. Clearly, we want to optimize this parameter to balance the opposing effects of the two terms. Recall that 
we discussed similar tradeoffs in the deterministic case; the first term increasing in $\eta$ could be thought of as the cost of ``over-penalizing'' good experts
while the second term decreasing in $\eta$ was interpreted as quantifying the benefits of increasing $\eta$ in that the bad experts may be more 
readily ignored. In optimizing $\epsilon$ note that we want to make sure the value is not a function of $m_T^*$ as that quantity is unknown to the learner. 
So while we could simply differentiate the regret with respect to $\eta$ (the function is differentiable for fixed $T$) then set to $0$ to optimize, this would lead
to $\eta$ as a function of $m_T^*$ which we don't want. Therefore, we first use the trivial bound $m_T^* \leq T$ and only \textit{then} take the derivative. 
\[\eta m_T^* + \frac{\log N}{\eta} \leq \eta T + \frac{\log N}{\eta}\]
Taking the derivative with respect to $\eta$, setting equal to $0$, and solving for $\eta$ we obtain the optimum:
\[\eta = \sqrt{\frac{\log N}{T}}\]
Plugging this back in to the bound on $\text{Regret}_T$ yields
\[\text{Regret}_T \leq  \sqrt{\frac{\log N}{T}} m_T^* +  \sqrt{\frac{T}{\log N}}\log N = \sqrt{T \log N} + \sqrt{T \log N} = 2\sqrt{T \log N}\]
If we think all the way back to the section introducing the concept of regret, we noted that the goal is to have regret grow sub-linearly in $T$, which corresponded 
to the quality of our guesses improving over time. We see that we have accomplished this goal here! This also implies that the average regret converges to $0$
at rate $\frac{1}{\sqrt{T}}$. 
\[\overline{\text{Regret}}_T \leq \frac{1}{T} \cdot 2\sqrt{T \log N} = 2\sqrt{\frac{\log N}{T}}\]

\subsubsection{A lower bound for randomized algorithms}
Recall that we established a lower bound of $m_T \geq 2m_T^*$ for any deterministic algorithm. In other words, $\overline{\text{Regret}}_T \geq \frac{m_T^*}{T} = O\left(\frac{1}{T}\right)$.
We just showed that in expectation this can be beaten by 
a randomized algorithm, which achieved $\overline{\text{Regret}}_T = O\left(\frac{1}{\sqrt{T}}\right)$. The natural question is whether this algorithm is optimal among all randomized 
algorithms. 
\textbf{TODO}: from homework problem 

\subsubsection{Using Randomized Weighted Majority to Prove the Minimax Theorem}
\textbf{TODO: Lecture 6 or Aaron Roth's notes}
A constructive version of LP duality. 


% Section: The Multiplicative Weights Update Algorithm
\section{The Multiplicative Weights Update Algorithm}

\subsection{Online Learning and Prediction: Extension of Randomized Weighted Majority Algorithm}
In the previous section we considered the problem of learning from expert advice. Our goal was to combine the advice from the experts in order to construct a series of choices that would
minimize our regret. We now consider a slight modification to this setup: instead of combining expert advice, we consider \textit{expert} a single expert each round. In this case, we can think 
of the experts as \textit{options} or \textit{strategies} that we must choose. This is a very natural setting in game theory; we might imagine we're playing a sequential game against and adversary, 
and we pick a strategy each round to try to minimize our losses in the game. What I've just described is essentially a two-player game with \textit{pure strategies}. We will instead consider the 
\textit{mixed strategy} setting in which during each round the player picks a probability distribution over the set of strategies, and then the strategy for that round is determined by a random sample from this 
distribution. It should not be surprising that we want to consider such a randomized setting; recall in the previous section that we were able to improve performance on average by randomizing the algorithm. 
We will also generalize from considering zero-one loss to allowing any real-valued loss in $[0, 1]$. The setup is summarized as follows. \\
For each round $t = 1, \dots, T$:
\begin{itemize}
\item Learner pick a probability distribution $p^{(t)}$ supported on the set of $n$ possible options. ``Choosing a probability distribution'' may sound a little weird, but just think of this as 
assigning weights to each expert as in the algorithms from the previous section. 
\item The costs of \textit{all} options are now revealed. The losses are denoted by $\ell^{(t)} \in [0, 1]^N$, where $\ell^{(t)}_i \in [0, 1]$ is the loss for option $i$. 
\item The loss of the learner for round $t$ is defined as the expected loss: $\E_{i \sim p^{(t)}}[\ell^{(t)}_i] = \langle p^{(t)}, \ell^{(t)}\rangle$
\end{itemize} 
As usual, we want to minimize the cumulative loss relative to the best decision in hindsight. 
\[\textit{Regret}_T = \sum_{t = 1}^{T} \langle p^{(t)}, \ell^{(t)}\rangle - \min_i \sum_{t = 1}^{T} \ell_i^{(t)}\]
Really the only difference from the setup in the previous section is that we're considering losses in $[0, 1]$ instead of $\{0, 1\}$. In the latter case we interpreted the problem as a binary decision-making
problem using the advice of experts. The interpretation of the current case is that we choose a strategy/option/decision that incurs a loss in $[0, 1]$. The goal will be to generalize the weighted majority 
algorithm to work in this setting. To this end, first consider that the weight update rule for expert $i$ for that algorithm can be written:
\[w_i^{(t + 1)} \gets (1 - \eta \ell_i^{(t)})w_i^{(t)}\]
where $\ell_i^{(t)} \in \{0, 1\}$; $0$ if the expert was correct and $1$ if the expert made a mistake. Thus, the weight was unaffected for correct experts and was decremented by factor $1 - \eta$ for 
incorrect ones. Thus, a natural extension is to simply keep this same exact formula, but now $ \ell_i^{(t)} \in [0, 1]$. So a strategy that incurs $0$ loss will leave the weight of the expert unaffected, while 
the maximal loss of $1$ will lead to a $1 - \eta$ multiplicative penalty. The penalty for all losses in between are then linearly interpolated between these extremes. So this can simply be viewed as a 
``smoothed'' version of the previous algorithm. We can even consider a further generalization if we allow losses in $[-1, 1]$. In this case the update factor assumes values in $[1 - \eta, 1 + \eta]$
which implies that strategies with negative losses are rewarded by boosting their weights. So in this framework weighted can be incremented as well as decremented. The algorithm described above 
is known as the \textit{multiplicative weights update algorithm}, which is really the crown jewel of all the algorithms we've presented in the last couple sections. Given that we've built things up 
very incrementally (majority algorithm when there is an infallible expert, weighted majority algorithm, randomized weighted majority algorithm, multiplicative weights updates algorithm), then this 
final product should feel quite natural. 

\bigskip

 \begin{algorithm}[H]
	\SetAlgoLined
	
	\textbf{Input}: $0 < \eta < 1$, $T \in \mathbb{N}$, \text{ Strategies } X = \{1, \dots, N\} \\
	
	\bigskip
	
	Set $w^{(i)} := 1$ \text{ for all } i = 1, \dots, N \\
	
	\For{$t = 0, \dots, T$} {
		Choose probability distribution $p^{(t)}_i := \frac{w_i}{\norm{w}_1}$, $i = 1, \dots, N$ \\
		$\text{Sample strategy } j \sim \text{Cat}(N, (p_1, \dots, p_N))$ \\
		Observe losses $\ell^{(t)} \in [-1, 1]^N$ \\
		
		\For{$i = 1, \dots, N$} {
			$w^{(i)} := (1 - \eta \ell_i^{(t)})w^{(i)}$ \\
		}
	}			
\caption{Multiplicative Weights Update Algorithm}
\end{algorithm}

\bigskip

\textbf{TODO: include analysis/proof for this algorithm}

\subsection{Applications of the Multiplicative Weights Update Algorithm}
\subsubsection{The Winnow Algorithm}
In this section we see one of the many applications of the multiplicative weights update (MWU) algorithm; namely, its use in learning linear classifiers. We consider a typical classification setup
with data
\[(x_i, y_i), x_i \in X \subset \R^N, y_i \in \{-1, +1\}, i = 1, \dots, m \]
which we assume to be linearly separable; that is, there exists $a \in \R^n$ s.t. $f(x) = \text{sgn}(\langle a, x\rangle)$ correctly classifies all the points. A correct linear classifier must satisfy
\[y_i \langle a, x\rangle \geq 0 \text{ for all } i = 1, \dots, m\]
Note that we're not including an intercept term here and thus are implicitly assuming that the data is centered at the origin, but this can easily be generalized. For some context, we note that 
there are two well-known algorithms that solve this problem: the perceptron algorithm and Support Vector Machines (SVM). These algorithms are not pre-requisites for understanding the Winnow 
algorithm, but they are useful as a basis for comparison. The perceptron algorithm finds a linear classifier by finding the normal vector $a$ to the separating hyperplane, such that $\norm{a}_2 = 1$. 
Considering normal vectors of unit Euclidean norm was very natural in this setting; the length of this vector had no affect on the span of the hyperplane and thus did not affect the classifications. 
The SVM extends this idea by trying to find the ``best'' separating hyperplane in the sense that it minimizes the margin $\min_{i} y_i \langle a, x \rangle$ between the hyperplane and the nearest points. 
\[\max_a \min_i \langle a, x_i \rangle \text{ s.t. } \langle a, x_i \rangle \geq 0 \ \forall i \text{ and } \norm{a}_2 = 1\]
So the SVM also focuses on optimizing $a$ such that $\norm{a}_2 = 1$. We now consider a different perspective, viewing $a$ as a 
\textit{probability distribution} over the coordinates of $\R^N$. Note that the resulting $a$ will still determine the separating hyperplane, but the way we now construct this vector is very different than before. 
Before diving into the algorithm, let's think about how we might interpret a classifier obtained by optimizing subject to the $1$-norm constraint. The sign of 
\[\langle a, x\rangle = \sum_{j = 1}^{N} a_j x_j\]
determines the classification of the point $x$. Since $a$ is a probability distribution then this linear combination can be viewed as an average. In this way we can view the entries of $a$ as indicating the relative
importance of the coordinates of $\R^N$. This can offer nice interpretations of results, and brings to mind some of the benefits enjoyed by methods such as the non-negative matrix factorization. One final note 
before proceeding: there is no reason we have to normalize so that the $a_j$ sum to $1$; the important thing is that the entries are non-negative so that they can be considered weights. It's simply convenient 
to normalize them to form a distribution. 

This leads to the following optimization problem. 
\begin{align*}
&y_i\langle a, x_i \rangle \geq 0 \ \forall i = 1, \dots, m\\
&\; \qquad \norm{a}_1 = 1 \\
&\; \qquad a_i \geq 0 \ \forall i \\
\end{align*}
Since $y_i\langle a, x_i \rangle = \sum_{j = 1}^{N} (y_j x_{ij})a_j$ where $y_j x_{ij}$ is a constant then we see that the first constraint is a linear inequality constraint. Similarly, the second constraint 
can be written $\sum_{j = 1}^{N} a_j = 1$, a linear equality constraint. So this is just a linear program! It might look weird as there is no explicit objective function, but we can just consider there is 
an objective $c^T a$, where $c = 0$. Basically, we're just telling the LP to find any $a$ that satisfies the constraints; that is, any linear classifier that separates the data. So we could just plug this thing
into an LP solver and call it a day. However, LP solvers are relatively slow so if there is structure to be exploited in a specific problem, more tailored algorithms can often lead to faster solvers. This will
be the case here.

To this end, let's see if we can find a faster algorithm. While this problem, as stated, appears to be a standard offline LP, we can actually view it in the online MWU framework. Indeed, let's treat each
coordinate of $\R^N$ as an ``expert'', in which case $a$ is then a probability distribution over these experts. To find $a$ we will use the typical MWU scheme; the trick will be in how we define the losses 
of the experts $\ell^{(t)} = (\ell^{(t)}_1, \dots, \ell^{(t)}_N)$ for each round $t$. 

\bigskip

 \begin{algorithm}[H]
	\SetAlgoLined
	
	\textbf{Input}: $0 < \eta < 1$, $T \in \mathbb{N}$ \\
	
	\bigskip
	
	Set $w_i := 1$ \text{ for all } i = 1, \dots, N \\
	
	\For{$t = 0, \dots, T$} {
		Choose probability distribution $a^{(t)}_i := \frac{w_i}{\norm{w}_1}$, $i = 1, \dots, N$ \\
		\If{$y_i \langle a^{(t)}, x_i \rangle \geq 0 \ \forall i = 1, \dots, N$} {
			\Return $a^{(t)}$
		} \Else {
			There exists misclassified example k: $y_k \langle a^{(t)}, x_k \rangle < 0$ \\
			Define losses $\ell^{(t)} := -\frac{y_k}{\rho} x_k$, where $\rho := \max_{1 \leq i \leq m} \norm{y_i x_i}_\infty$
		}
		 
		
		
		\For{$i = 1, \dots, N$} {
			$w_i := (1 - \eta \ell_i^{(t)})w_i$ \\
		}
	}			
\caption{The Winnow Algorithm}
\end{algorithm}



% Section: Online Convex Optimization
\section{Online Convex Optimization}
In the spirit of mathematics, let's continue generalizing further. The settings we've considered so far (learning from experts, online prediction) can be viewed 
in a more general framework, that of online convex optimization. The generalized setting is as follows. \\
For each round $t = 1, \dots, T$:
\begin{itemize}
\item Pick ``action'' $x^{(t)} \in X$, where $X \subset \R^N$ is convex.
\item Receive a convex loss function $f_t: \R^N \to \R$. 
\item Suffer loss $f_t(x^{(t)})$
\end{itemize}
As before, this generalization shouldn't be too shocking. We're basically considering a continuous version of the previous problem we considered: instead of a finite 
set of strategies, we consider a continuum and similarly with the losses. Both the feasible set of actions $X$ and the loss functions $f_t$ are assumed to be convex, just 
as in typical offline convex optimization. The difference here is that we're considering a sequential scenario in which decisions are made and losses observed one at a time. 
As usual our goal will be to minimize regret; that is, how well we compete with the single action with the greatest loss. 
\[\text{Regret}_T = \sum_{t = 1}^{T} f_t(x^{(t)}) - \min_{x \in X} \sum_{t = 1}^{T} f_t(x)\]

\subsection{Examples}

\subsubsection{Online Linear Regression}
We consider the typical linear regression problem, but now suppose that the observations arrive one at a time in an online fashion, as well as the realizations of the observed response. 
In this scenario the feature vector for each observation is like expert advice, and we use it to help choose an ``action'' each round, the action being the linear model itself (i.e. the vector 
of regression coefficients). 
For each round $t = 1, \dots, T$:
\begin{itemize}
\item Receive feature vector $v^{(t)} \in \R^N$ for current observation.
\item Choose action (vector of regression coefficients) $x^{(t)}$
\item Observe true response $y^{(t)}$ and suffer loss $\abs{\langle x^{(t)}, v^{(t)} \rangle - y^{(t)}}$
\end{itemize}
Note that the set of actions $X = \R^N$ and loss functions $f_t(x) = \abs{\langle x, v^{(t)} \rangle - y^{(t)}}$ are both convex. 

\subsubsection{Learning from Expert Advice}
We now re-cast the expert advice setting as an online convex optimization problem. In particular, we consider the randomized setting in which the learner gives weights to the 
experts and then uses these weights to sample an expert at random (i.e. the learner defines a probability distribution over the set of experts). 
For each round $t = 1, \dots, T$:
\begin{itemize}
\item Learner picks probability distribution $p^{(t)}$ over set of $N$ experts.
\item Expert is sampled $j \sim \text{Cat}(N, (p_1, \dots, p_N))$.
\item Loss for each expert is revealed: $\ell^{(t)} \in [-1, 1]^N$. 
\item Learner suffers loss: $\E_{i \sim p^{(t)}} [\ell^{(t)}_i] = \langle p^{(t)}, \ell^{(t)} \rangle$.
\end{itemize}
The set of actions here is $X = \Delta_N$, where $\Delta_N = \{p \in \R^N: p_i \in [0, 1], \norm{p}_1 = 1\}$ is the \textit{probability simplex} over $\{1, \dots, N\}$ (the set of all probability distributions over this set). 
So while the set $\{1, \dots, N\}$ is certainly not convex, the set $\Delta_N$ can easily be shown to be convex. The loss function $f_t(p) = \langle p, \ell^{(t)} \rangle$ is linear and thus also 
convex. One thing to consider is that we have previously considered a concept of ``regret'' in the learning from expert advice problem. Did this definition agree with the general definition we give for regret in the
online convex optimization framework? Let's check. Previously, we defined regret for the expert problem relative to the advice of the best expert: 
\[\text{Regret}_T = \sum_{t = 1}^{T} \langle p^{(t)}, \ell^{(t)} \rangle - \min_{i = 1, \dots, N} \sum_{t = 1}^{T} \ell_i^{(t)}\]
As noted above, the summand of the first term is the convex loss function $f_t(p^{(t)}) = \langle p^{(t)}, \ell^{(t)} \rangle$. Moreover, we can re-write the second term as 
\[\min_{i = 1, \dots, N} \sum_{t = 1}^{T} \ell_i^{(t)} = \min_{p \in \Delta_N} \sum_{t = 1}^{T} \langle p, \ell^{(t)}\rangle =  \min_{p \in \Delta_N} \sum_{t = 1}^{T} f(p)\]
This follows from the fact that the minimizing probability distribution will be the one that puts all of the mass on the best expert. Therefore, we have shown
\[ \sum_{t = 1}^{T} \langle p^{(t)}, \ell^{(t)} \rangle - \min_{i = 1, \dots, N} \sum_{t = 1}^{T} \ell_i^{(t)} = \sum_{t = 1}^{T} f_t(p^{(t)}) - \min_{p \in \Delta_N} \sum_{t = 1}^{T} f_t(p)\]
The righthand side is precisely the general definition we gave for regret in the online convex optimization setup. 

\subsection{Online Linear Optimization}
Just as linear programming is a special case of (offline) convex optimization, it is natural to consider online linear programming within the online convex optimization framework. In this setup 
the loss functions are restricted to be linear.
\[f_t(x) = \langle x, z^{(t)}\rangle\]
for some vector $z^{(t)} \in \R^N$. It turns out that given a online convex optimization problem we can reduce it to a linear one; so in this sense online linear optimization is the ``hardest'' problem 
in this setting. The reduction comes from the well-known characterization of convexity that states that a (differentiable) function is convex if and only if the first-order Taylor polynomial at every point
forms a global lower bound on the function. 
\begin{prop}
A differentiable function $f: \R^n \to \R$ is convex if and only if for all $x, y \in \R^n$
\[f(y) \geq f(x) + \langle \nabla f(x), y - x \rangle\]
\end{prop}

\subsubsection{Solving Online Convex Optimization using Online Linear Optimization}
Since our goal is to solve the online convex optimization (OCO) problem, then the idea is to upper bound the OCO regret by the regret of a related online linear optimization (OLO) problem. 
Suppose $x^{(t)}$ is the action chosen by an OLO algorithm. Then by the above proposition, for any $u \in \R^N$, 
\[f_t(u) \geq f_t(x^{(t)}) + \langle \nabla f_t(x^{(t)}), u - x^{(t)}\rangle\]
Rearranging, 
\[f_t(x^{(t)}) - f_t(u) \leq \langle \nabla f_t(x^{(t)}), x^{(t)}\rangle - \langle \nabla f_t(x^{(t)}), u \rangle\]
If we define the linear loss $\ell_t(x) := \langle x, \nabla f_t(x^{(t)})\rangle$ then the lefthand side is just the difference of the linear loss function evaluated at $x^{(t)}$ and $u$. Take note of 
what we're doing here: we're using the gradient of the convex loss function (the loss we're really interested in) to define a linear loss function. This idea would even work if $f$ were not 
differentiable, we would simply replace the gradient with a sub-gradient. Okay, given this definition we now have
\[f_t(x^{(t)}) - f_t(u) \leq \ell_t(x^{(t)}) - \ell_t(u)\]
Recalling our goal to obtain a bound on the OCO regret, let's sum over $t$. 
\[\sum_{t = 1}^{T} f_t(x^{(t)}) - \sum_{t = 1}^{T} f_t(u) \leq \sum_{t = 1}^{T} \ell_t(x^{(t)}) - \sum_{t = 1}^{T} \ell_t(u)\]
In particular, this holds for $u := x^* \in \argmin_{x \in X} \sum_{t = 1}^{T} f_t(x)$; that is, we're setting $u$ to be the minimizer of the convex loss.
\begin{align*} 
\sum_{t = 1}^{T} f_t(x^{(t)}) - \sum_{t = 1}^{T} f_t(x^*) &\leq \sum_{t = 1}^{T} \ell_t(x^{(t)}) - \sum_{t = 1}^{T} \ell_t(x^*) \\
									  	&\leq \sum_{t = 1}^{T} \ell_t(x^{(t)}) - \min_{x \in X} \sum_{t = 1}^{T} \ell_t(x)
\end{align*}
We have thus found,
\[\sum_{t = 1}^{T} f_t(x^{(t)}) - \min_{x \in X} \sum_{t = 1}^{T} f_t(x) \leq \sum_{t = 1}^{T} \ell_t(x^{(t)}) - \min_{x \in X} \sum_{t = 1}^{T} \ell_t(x)\]
That is, 
\[\text{Regret}_{T, \text{OCO}} \leq \text{Regret}_{T, \text{OLO}}\]
Thus, solving the OLO problem with linear loss defined using the gradient of the convex loss will yield a solution that is an upper bound on the loss of the convex problem. Of course, if this 
upper bound is nowhere near tight then this is not too helpful. Let's clearly state the algorithm we've arrived at via the above analysis. 
For each round $t = 1, \dots, T$:
\begin{itemize}
\item Call OLO algorithm and receive action $x^{(t)} \in X$ (note: optimizing with respect to previous loss function $\ell_{t - 1}(x) = \langle x, \nabla f(x^{(t - 1)}) \rangle $). 
\item Receive new convex loss function $f_t$
\item Define new linear loss function $\ell_t(x) = \langle x, \nabla f_t(x^{(t)})\rangle$
\end{itemize}

\subsubsection{Solving Offline Convex Optimization using Online Linear Optimization}
These online techniques can even be leveraged to solve offline problems! Consider the standard convex optimization setup:
\[\min_{x \in X} f(x)\]
where $f$ and $X$ are convex. We consider a procedure that leverages OLO quite similar to the previous example; really the only difference here is that the convex loss function 
is constant: $f_t \equiv f$. \\
For each round $t = 1, \dots, T$:
\begin{itemize}
\item Call OLO algorithm and receive action $x^{(t)} \in X$ (note: optimizing with respect to previous loss function $f_{t - 1}(x) = \langle x, \nabla f(x^{(t - 1)}) \rangle $). 
\item Define new loss function $f_t(x) = \langle x, \nabla f(x^{(t)})\rangle$
\end{itemize}
At the end we'll now return an average of the guesses $\frac{1}{T}\sum_{t = 1}^{T} x^{(t)}$. The following theorem shows that the resulting approximation to the solution of the convex problem
is dependent on the quality of the OLO algorithm. 
\begin{thm}
\[f\left(\frac{1}{T}\sum_{t = 1}^{T} x^{(t)}\right) - \min_{x \in X} f(x) \leq \frac{\text{Regret}_{T, \text{OLO}}}{T}\]
\end{thm}

\begin{proof}
\textbf{TODO} (proof at end of Advanced Optimization Algorithms lecture 24)
\end{proof}





\section{Resources}
\begin{itemize}
\item Potential Function Proofs for Gradient Methods (Bansal and Gupta) 
\item Francesco Orabana (Boston University) lecture notes on online optimization
\item Alina Ene (Boston University) lecture notes from Advanced Optimization Algorithms Spring 2022 course
\item Aaron Singer (Harvard University) lecture notes from Advanced Optimization
\item Aaron Roth (UPenn) lecture notes from Algorithmic Game Theory (lectures 4 and 5)
\item The Multiplicative Weights Update Algorithm: A Meta-Algorithm and Applications (Arora)
\end{itemize}



\end{document}

