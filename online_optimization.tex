\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bbm} % For mathbbm for indicator function
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

\begin{document}

\begin{center}
\Large
Optimization Under Uncertainty/Online Optimization
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

\section{Introduction}
This text contains a brief introduction to the field of online optimization. ``Online'' here refers to settings in which one has limited or no knowledge about the future; thus, we might 
also call this subject ``optimization under uncertainty''. This is opposed to the ``offline'' setting dealt with in classical optimization theory. Take linear programming for instance: here we
assume complete knowledge of the problem in the form of the objective function and relevant constraints. This complete information assumption may be unrealistic in certain applications. 
As an example, let's consider two-player games from game theory. In the classical setup, it is assumed that both players have perfect knowledge of the others strategies (or mixed strategies). 
A natural extension that falls under the umbrella of online learning is to consider the situation in which the adversary's strategies are unknown. In these kind of applications the decision problem 
may be sequential in nature; that is, a player is required to make sequential decisions responding to those of the adversary. However, online learning also includes problems in which a decision is 
made only once. 

\section{General Setting and Regret Minimization}
Consider the following game repeated over $T$ rounds: 
\begin{itemize}
\item An adversary picks $y_t \in [0, 1]$
\item You pick $x_t \in [0, 1]$, trying to guess the adversary's number
\item The adversary's number is revealed and you face an $\ell_2$ penalty: $(x_t - y_t)^2$
\end{itemize}
The question is: how should you guess in order to incur minimal cumulative loss? Currently, under these very light assumptions we can essentially say nothing. For example, suppose the adversary draws 
$y_t$ randomly from a completely different distribution (unknown to you) at each round $t$. This game seems somewhat hopeless for the player. However, what if we add more structure? Suppose we know 
in advance that the adversary always draws $y_t \overset{iid}{\sim} \mathcal{U}[0, 1]$. Let $Y \sim \mathcal{U}[0, 1]. $In this case, since we know that the mean minimizes $\ell_2$ loss we should guess 
$x_t := \E Y= \frac{1}{2}$. Letting $x = (x_1, \dots, x_T)^T$ and $y = (y_1, \dots, y_T)^T$ the cumulative loss incurred would thus be
\[\text{loss}(x, y) := \norm{x - y}_2^2 = \sum_{t = 1}^{T} (y_t - x_t)^2 = \sum_{t = 1}^{T} (Y_t - \E Y_t)^2 \]
\[\E_Y[\text{loss}(x, y)] =  \sum_{t = 1}^{T} \E(Y_t - \E Y_t)^2 = \sum_{t = 1}^{T} \Var(Y) = T\Var(Y) = \frac{T}{12}\]
where we use the fact that the $Y_t$ are iid and the final step recalls the variance of a uniform random variable. With respect to $\ell_2$ loss, this is the best we can do. Note that there was nothing special 
about the uniform distribution here; the key assumption was that the adversary picked numbers iid from a distribution that we knew in advance. In general, if this distribution has variance $\sigma^2$ then 
by playing our optimal choice of the mean of the distribution we will incur cumulative loss $\sigma^2 T$. 

The assumption that we know the distribution is quite strong, and seems unreasonable to hold in real applications. So what can we say about the situation where the distribution is unknown in advance?
We're still assuming that the adversary picks a fixed distribution at the beginning and samples iid from that distribution each round. Intuitively, we would imagine that our guesses will likely be pretty bad at first, 
but that if $T$ is sufficiently large we can, over time, start to get a sense of the underlying distribution and tune our guesses accordingly. We certainly can't hope to compete with the minimal loss incurred in the 
known distribution setting. Thus, $\sigma^2 T$ seems to be a good benchmark - we can measure our performance relative to this idealized setting. 
\[\text{loss}(x, y) := \E_Y \norm{X - Y}_2^2 - \sigma^2 T\]
Note that the expectation is taken with respect to the distribution chosen by the adversary, which is unknown the the player. 
It is also useful to divide through by $T$ and consider the average loss instead. 
\[\text{loss}_{\text{avg}}(x, y) := \frac{1}{T} \E_Y \norm{X - Y}_2^2 - \sigma^2\]
What can we hope for? Recall our intuition that over time we hope to be able to \textit{learn} the underlying distribution and thus hope that our estimates will tend towards the estimates we would make
when the distribution is known. That is, we hope to be able to show that the average loss tends to $0$ in the limit as $T \to \infty$. What does this imply about $\text{loss}(x, y)$? Well, first of all we should 
not expect this to tend to $0$ since this measures the difference in the \textit{cumulative} loss with respect to the cumulative loss of the idealized setting. Since at first we expect our guesses to be significantly
worse than in the idealized case, then the large loss accumulated for these early guesses will not go away - the cumulative loss will just keep adding to this. However, since we expect our guesses to get better
in time then we expect the \textit{rate} of accumulation to decrease over time. For example, if $\text{loss}(x, y)$ grew linearly then this would imply a constant rate of accumulation, which would mean our guesses
are not improving over time. Thus, our hope is that $\text{loss}(x, y)$ grows \textit{sub-linearly} (could be logarithmically, square root, etc.). This makes sense because when we divide through by $T$ in order 
to consider the average growth relative to the baseline then in order for this average distance to tend to $0$ then the denominator (which is growing linearly) must grow faster than the numerator (hence the 
numerator must grow \textit{sub-linearly}). 

With this intuition established, we proceed with the last couple steps in order to arrive at our final performance metric. I begin by re-writing $\text{loss}(x, y)$ in an equivalent way
\[\text{loss}(x, y) = \E_Y \norm{X - Y}_2^2 - \min_{x \in [0, 1]} \E_Y \norm{X - Y}_2^2 \]
Recall that this follows from the fact that $\sigma^2 T$ was the loss when the optimal $x \equiv x_t$ was chosen to minimize the $\ell_2$ distance. This above expression just explicitly 
writes out this optimization perspective. Now for another generalization: we have thus far been making the restrictive assumption that the adversary chooses a fixed distribution at the start of the game. We want to 
define a general performance metric to applies in cases without any sort of distributional assumption. We can thus simply take the previous expression and remove all randomness from the expression, 
just treating the $x_t$ and $y_t$ as sequences of real numbers. The resulting expression is known as \textit{regret}, which captures the notion that once the true realizations are revealed you regret 
the choices you made  and would have rather chosen the true values. In other words, it expresses what you would have chosen \textit{in hindsight}. 
\[\text{Regret}_T(x, y) = \norm{x - y}_2^2 - \min_{x \in [0, 1]} \norm{x - y}_2^2 \]
We will consider that we've ``won'' the game if $\text{Regret}_T(x, y)$ grows sub-linearly in $T$: 
\[\lim_{T \to \infty} \frac{\text{Regret}_T(x, y)}{T} = 0\]
which we've seen is another way of saying that the average regret tends to $0$. We have now arrived at the final definition of loss that we will consider. This definition will generalize a few restricting assumptions
we have been making: 
\begin{itemize}
\item Instead of considering $\ell_2$ loss we will instead consider an arbitrary loss function $\ell$. We need not even explicitly consider $y$ anymore; all we need to be able to say is that 
a guess $x_t$ generalizes some loss $\ell(x_t)$. 
\item We will now allow for different loss functions at each round of the game, $\ell_t$, $t = 1, \dots, T$. 
\item We drop the assumption that the values must be in $[0, 1]$, instead considering subsets of $d$-dimensional Euclidean space.  
\item Instead of parameterizing the regret with respect to the idealized setting we have been considering, we allow the framework to be flexible and just parameterize it with respect to 
some guess $u \in \R^d$ (in the above examples, $u$ was the mean of the distribution). 
\end{itemize}
\begin{definition}
Let $x = (x_1, \dots, x_T)^T$, where $x_t \in V \subset \R^d$ be the vectors output by an algorithm at each of $T$ rounds. Suppose the values incur loss $\ell_t(x_t)$ at round $t$, where $\ell_t: \R^d \to \R$
for $t = 1, \dots, T$. Also let $u \in V$ be a ``predictor'' which the loss will be measured with respect to. We then define the \textit{regret} as: 
\[\text{Regret}_T(x, u) := \sum_{t = 1}^{T} \ell_t(x_t) - \sum_{t = 1}^{T} \ell_t(u) \]
\end{definition}
As we will see, the generality of this definition will be powerful in allowing it to apply in a diverse array of settings. Note that there are many different assumptions we could tweak in this framework; for example, 
we can suppose that the loss functions $\ell_t$ are known to the player in advance, or are they chosen by the adversary and only revealed once the player has made their guess $x_t$? The latter assumption, along 
with the assumption that the $\ell_t$ are convex, form the general \textit{online convex optimization} framework. We can also generalize further to allow the baseline $u$ be a sequence $u_1, \dots, u_T$ which 
we measure loss with respect to. 

\section{Regret Minimization: A Simple Example}
\textbf{TODO: include the example from Prof Orabana's first week of notes}


\section{Learning from Experts}
One important online optimization problem is that of learning from expert advice. The problem is typically defined as follows: a decision-maker must make a decision at each of $T$ rounds. Here let's 
consider a simple set of possible realizations $V = \{A, B\}$ so that this can be thought of as a repeated binary classification problem. We consider the zero-one loss 
$\ell(x_t, y_t) = \mathbbm{1}(x_t \neq y_t)$. Typically in binary classification we base the classification on information 
given in the form of \textit{features} or \textit{predictors}. In this setting we refer to the analogous concept as \textit{experts}; in particular, we consider a set of $N$ experts that each recommend a decision
in $\{A, B\}$ each time step. Your goal is thus to use this expert advice in order to make the best possible decisions. Note that one's first approach in solving this problem would likely be to develop an 
algorithm to minimize the number of mistakes: 
\[\min_{x_t} \sum_{t = 1}^{T} \ell(x_t, y_t)\]
However, this is not a good baseline to evaluate our algorithms. Indeed, it can be shown that no randomized algorithm can make fewer than $\frac{T}{2}$ in expectation. Moreover, the trivial algorithm 
of simply flipping a coin at each step $x_t \overset{iid}{\sim} \text{Bern}(1/2)$ achieves this lower bound. So this turned out to be very uninteresting: $\frac{T}{2}$ mistakes was both optimal and trivial 
to obtain. The solution of course is to instead consider the \textit{regret}, which requires choosing a useful benchmark. Recalling the general definition of regret, we therefore must decide on a suitable 
predictor $u$. In the learning from experts setting, a natural choice is to consider loss with respect to the loss of the \textit{best expert}. We therefore consider
\[\text{Regret}_T(x, u) := \sum_{t = 1}^{T} \ell_t(x_t) - \sum_{t = 1}^{T} \ell_t(u) = \sum_{t = 1}^{T} \mathbbm{1}\{x_t \neq y_t\} - \min_{1 \leq i \leq N} \sum_{t = 1}^{T} \mathbbm{1}\{u^{(i)}_t \neq y_t\}\]
where $u^{(i)}_t$ denotes the advice from the $i^{\text{th}}$ expert at round $t$. 

\subsection{The Weighted Majority Algorithm}



\end{document}

