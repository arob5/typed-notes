\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{bbm} % For mathbbm for indicator function
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

\begin{document}

\begin{center}
\Large
Optimization Under Uncertainty/Online Optimization
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

\section{Introduction}
This text contains a brief introduction to the field of online optimization. ``Online'' here refers to settings in which one has limited or no knowledge about the future; thus, we might 
also call this subject ``optimization under uncertainty''. This is opposed to the ``offline'' setting dealt with in classical optimization theory. Take linear programming for instance: here we
assume complete knowledge of the problem in the form of the objective function and relevant constraints. This complete information assumption may be unrealistic in certain applications. 
As an example, let's consider two-player games from game theory. In the classical setup, it is assumed that both players have perfect knowledge of the others strategies (or mixed strategies). 
A natural extension that falls under the umbrella of online learning is to consider the situation in which the adversary's strategies are unknown. In these kind of applications the decision problem 
may be sequential in nature; that is, a player is required to make sequential decisions responding to those of the adversary. However, online learning also includes problems in which a decision is 
made only once. 

\section{General Setting and Regret Minimization}
Consider the following game repeated over $T$ rounds: 
\begin{itemize}
\item An adversary picks $y_t \in [0, 1]$
\item You pick $x_t \in [0, 1]$, trying to guess the adversary's number
\item The adversary's number is revealed and you face an $\ell_2$ penalty: $(x_t - y_t)^2$
\end{itemize}
The question is: how should you guess in order to incur minimal cumulative loss? Currently, under these very light assumptions we can essentially say nothing. For example, suppose the adversary draws 
$y_t$ randomly from a completely different distribution (unknown to you) at each round $t$. This game seems somewhat hopeless for the player. However, what if we add more structure? Suppose we know 
in advance that the adversary always draws $y_t \overset{iid}{\sim} \mathcal{U}[0, 1]$. Let $Y \sim \mathcal{U}[0, 1]. $In this case, since we know that the mean minimizes $\ell_2$ loss we should guess 
$x_t := \E Y= \frac{1}{2}$. Letting $x = (x_1, \dots, x_T)^T$ and $y = (y_1, \dots, y_T)^T$ the cumulative loss incurred would thus be
\[\text{loss}(x, y) := \norm{x - y}_2^2 = \sum_{t = 1}^{T} (y_t - x_t)^2 = \sum_{t = 1}^{T} (Y_t - \E Y_t)^2 \]
\[\E_Y[\text{loss}(x, y)] =  \sum_{t = 1}^{T} \E(Y_t - \E Y_t)^2 = \sum_{t = 1}^{T} \Var(Y) = T\Var(Y) = \frac{T}{12}\]
where we use the fact that the $Y_t$ are iid and the final step recalls the variance of a uniform random variable. With respect to $\ell_2$ loss, this is the best we can do. Note that there was nothing special 
about the uniform distribution here; the key assumption was that the adversary picked numbers iid from a distribution that we knew in advance. In general, if this distribution has variance $\sigma^2$ then 
by playing our optimal choice of the mean of the distribution we will incur cumulative loss $\sigma^2 T$. 

The assumption that we know the distribution is quite strong, and seems unreasonable to hold in real applications. So what can we say about the situation where the distribution is unknown in advance?
We're still assuming that the adversary picks a fixed distribution at the beginning and samples iid from that distribution each round. Intuitively, we would imagine that our guesses will likely be pretty bad at first, 
but that if $T$ is sufficiently large we can, over time, start to get a sense of the underlying distribution and tune our guesses accordingly. We certainly can't hope to compete with the minimal loss incurred in the 
known distribution setting. Thus, $\sigma^2 T$ seems to be a good benchmark - we can measure our performance relative to this idealized setting. 
\[\text{loss}(x, y) := \E_Y \norm{X - Y}_2^2 - \sigma^2 T\]
Note that the expectation is taken with respect to the distribution chosen by the adversary, which is unknown the the player. 
It is also useful to divide through by $T$ and consider the average loss instead. 
\[\text{loss}_{\text{avg}}(x, y) := \frac{1}{T} \E_Y \norm{X - Y}_2^2 - \sigma^2\]
What can we hope for? Recall our intuition that over time we hope to be able to \textit{learn} the underlying distribution and thus hope that our estimates will tend towards the estimates we would make
when the distribution is known. That is, we hope to be able to show that the average loss tends to $0$ in the limit as $T \to \infty$. What does this imply about $\text{loss}(x, y)$? Well, first of all we should 
not expect this to tend to $0$ since this measures the difference in the \textit{cumulative} loss with respect to the cumulative loss of the idealized setting. Since at first we expect our guesses to be significantly
worse than in the idealized case, then the large loss accumulated for these early guesses will not go away - the cumulative loss will just keep adding to this. However, since we expect our guesses to get better
in time then we expect the \textit{rate} of accumulation to decrease over time. For example, if $\text{loss}(x, y)$ grew linearly then this would imply a constant rate of accumulation, which would mean our guesses
are not improving over time. Thus, our hope is that $\text{loss}(x, y)$ grows \textit{sub-linearly} (could be logarithmically, square root, etc.). This makes sense because when we divide through by $T$ in order 
to consider the average growth relative to the baseline then in order for this average distance to tend to $0$ then the denominator (which is growing linearly) must grow faster than the numerator (hence the 
numerator must grow \textit{sub-linearly}). 

With this intuition established, we proceed with the last couple steps in order to arrive at our final performance metric. I begin by re-writing $\text{loss}(x, y)$ in an equivalent way
\[\text{loss}(x, y) = \E_Y \norm{X - Y}_2^2 - \min_{x \in [0, 1]} \E_Y \norm{X - Y}_2^2 \]
Recall that this follows from the fact that $\sigma^2 T$ was the loss when the optimal $x \equiv x_t$ was chosen to minimize the $\ell_2$ distance. This above expression just explicitly 
writes out this optimization perspective. Now for another generalization: we have thus far been making the restrictive assumption that the adversary chooses a fixed distribution at the start of the game. We want to 
define a general performance metric to applies in cases without any sort of distributional assumption. We can thus simply take the previous expression and remove all randomness from the expression, 
just treating the $x_t$ and $y_t$ as sequences of real numbers. The resulting expression is known as \textit{regret}, which captures the notion that once the true realizations are revealed you regret 
the choices you made  and would have rather chosen the true values. In other words, it expresses what you would have chosen \textit{in hindsight}. 
\[\text{Regret}_T(x, y) = \norm{x - y}_2^2 - \min_{x \in [0, 1]} \norm{x - y}_2^2 \]
We will consider that we've ``won'' the game if $\text{Regret}_T(x, y)$ grows sub-linearly in $T$: 
\[\lim_{T \to \infty} \frac{\text{Regret}_T(x, y)}{T} = 0\]
which we've seen is another way of saying that the average regret tends to $0$. We have now arrived at the final definition of loss that we will consider. This definition will generalize a few restricting assumptions
we have been making: 
\begin{itemize}
\item Instead of considering $\ell_2$ loss we will instead consider an arbitrary loss function $\ell$. We need not even explicitly consider $y$ anymore; all we need to be able to say is that 
a guess $x_t$ generalizes some loss $\ell(x_t)$. 
\item We will now allow for different loss functions at each round of the game, $\ell_t$, $t = 1, \dots, T$. 
\item We drop the assumption that the values must be in $[0, 1]$, instead considering subsets of $d$-dimensional Euclidean space.  
\item Instead of parameterizing the regret with respect to the idealized setting we have been considering, we allow the framework to be flexible and just parameterize it with respect to 
some guess $u \in \R^d$ (in the above examples, $u$ was the mean of the distribution). 
\end{itemize}
\begin{definition}
Let $x = (x_1, \dots, x_T)^T$, where $x_t \in V \subset \R^d$ be the vectors output by an algorithm at each of $T$ rounds. Suppose the values incur loss $\ell_t(x_t)$ at round $t$, where $\ell_t: \R^d \to \R$
for $t = 1, \dots, T$. Also let $u \in V$ be a ``predictor'' which the loss will be measured with respect to. We then define the \textit{regret} as: 
\[\text{Regret}_T(x, u) := \sum_{t = 1}^{T} \ell_t(x_t) - \sum_{t = 1}^{T} \ell_t(u) \]
\end{definition}
As we will see, the generality of this definition will be powerful in allowing it to apply in a diverse array of settings. Note that there are many different assumptions we could tweak in this framework; for example, 
we can suppose that the loss functions $\ell_t$ are known to the player in advance, or are they chosen by the adversary and only revealed once the player has made their guess $x_t$? The latter assumption, along 
with the assumption that the $\ell_t$ are convex, form the general \textit{online convex optimization} framework. We can also generalize further to allow the baseline $u$ be a sequence $u_1, \dots, u_T$ which 
we measure loss with respect to. 

\section{Regret Minimization: A Simple Example}
\textbf{TODO: include the example from Prof Orabana's first week of notes}

\section{A Note on Potential Function Proofs}
This section provides a very brief introduction to a common proof technique commonly utilized in proofs of convergence for iterative optimization algorithms. For an in-depth discussion of this topic see 
Banal and Gupta's paper mentioned in the references. The potential function framework (not to be confused with potential functions from vector calculus/physics) is utilized in the proofs of many optimization algorithms, 
including in the online setting. 

First recall the general goal of these algorithms: supposing $f$ is convex, we seek to approximate its minimum $f(x^*)$. It is common to try to bound the additive error $f(x_T) - f(x^*) < \epsilon$, where $x_T$
is the estimate of the true minimizer $x^*$ produced by an algorithm after $T$ iterations. The goal is to construct an upper bound on this additive error as a function of $T$. 

Potential functions are a unifying technique that helps in constructing such a bound. Note that above we are considering the distance in function values $f(x_T) - f(x^*)$. We might also consider the distance 
between the minimizers $d(x_T, x^*)$, where I intentionally keep the distance metric unspecified for now (could be $\ell_2$, etc.). If $f$ is continuous then we can easily convert between bounds on the function
value distance and bounds on the minimizing vector distance. The potential function is defined to take into account both of these distances: 
\[\Phi_t := a_t [f(x_t) - f(x^*)] + b_t \cdot d(x_t, x^*)\]
where $a_t, b_t$ are non-negative multipliers that can be tailored to the needs of different proofs. The justification for the above definition is that bounding the potential function will induce a bound on 
$f(x_T) - f(x^*)$. Indeed, suppose we obtain the bound: 
\[\Phi_{t + 1} - \Phi_t \leq B_t\]
Then this implies
\[\sum_{t = 1}^{T - 1} [\Phi_{t + 1} - \Phi_t] \leq  \sum_{t = 1}^{T - 1} B_t\]
The lefthand side is a telescoping sum and therefore we're left with 
\[\Phi_{T} - \Phi_0 \leq  \sum_{t = 1}^{T - 1} B_t\]
Moving $\Phi_0$ to the other side and expanding $\Phi_{T}$, 
\[ a_T [f(x_T) - f(x^*)] + b_T \cdot d(x_T, x^*) \leq \Phi_0 + \sum_{t = 1}^{T - 1} B_t\]
Thus, 
\[ f(x_t) - f(x^*) \leq \frac{1}{a_T}\left[\Phi_0 - b_T \cdot d(x_T, x^*) + \sum_{t = 1}^{T - 1} B_t\right] \leq \frac{\Phi_0 + \sum_{t = 1}^{T - 1} B_t}{a_T}\]
A couple notes: first, in the last step we simplify the bound by dropping $b_T \cdot d(x_T, x^*)$. In general, we're considering $T$ large enough such that $d(x_T, x^*)$ is small and thus
this final inequality shouldn't give up too much in the bound. Next, we observe that $\Phi_0$ encodes the error given by the initialization $x_0$ of the algorithm (the ``initial guess''). This potential
function framework provides a nice organization for many convergence proofs; you'll see many convergence proofs of gradient descent, etc. that feature this telescoping sum step without explicitly 
considering a potential function, but I find that explicitly defining a potential function helps me draw parallels between similar proofs. 

\subsection{TODO: include example of a potential function proof from the Banal and Gupta paper}



\section{Learning from Experts}
One important online optimization problem is that of learning from expert advice. The problem is typically defined as follows: a decision-maker must make a decision at each of $T$ rounds. Here let's 
consider a simple set of possible realizations $V = \{A, B\}$ so that this can be thought of as a repeated binary classification problem. We consider the zero-one loss 
$\ell(x_t, y_t) = \mathbbm{1}(x_t \neq y_t)$. Typically in binary classification we base the classification on information 
given in the form of \textit{features} or \textit{predictors}. In this setting we refer to the analogous concept as \textit{experts}; in particular, we consider a set of $N$ experts that each recommend a decision
in $\{A, B\}$ each time step. Your goal is thus to use this expert advice in order to make the best possible decisions. Note that one's first approach in solving this problem would likely be to develop an 
algorithm to minimize the number of mistakes: 
\[\min_{x_t} \sum_{t = 1}^{T} \ell(x_t, y_t)\]
However, this is not a good baseline to evaluate our algorithms. Indeed, it can be shown that no randomized algorithm can make fewer than $\frac{T}{2}$ in expectation. Moreover, the trivial algorithm 
of simply flipping a coin at each step $x_t \overset{iid}{\sim} \text{Bern}(1/2)$ achieves this lower bound. So this turned out to be very uninteresting: $\frac{T}{2}$ mistakes was both optimal and trivial 
to obtain. The solution of course is to instead consider the \textit{regret}, which requires choosing a useful benchmark. Recalling the general definition of regret, we therefore must decide on a suitable 
predictor $u$. In the learning from experts setting, a natural choice is to consider loss with respect to the loss of the \textit{best expert}. We therefore consider
\[\text{Regret}_T(x, u) := \frac{1}{T} \sum_{t = 1}^{T} \ell_t(x_t) - \frac{1}{T} \sum_{t = 1}^{T} \ell_t(u_t) = \frac{1}{T} \sum_{t = 1}^{T} \mathbbm{1}\{x_t \neq y_t\} - \min_{1 \leq i \leq N} \frac{1}{T} \sum_{t = 1}^{T} \mathbbm{1}\{u^{(i)}_t \neq y_t\}\]
where $u^{(i)}_t$ denotes the advice from the $i^{\text{th}}$ expert at round $t$. Thus in the first expression $u_t$ is the advice from the best expert at round $t$. Note that we're considering what we previously 
called \textit{average regret} here. I've just dropped the ``avg'' subscript to lighten notation (often when people talk about regret they're talking about this averaged version anyways). 

\subsection{The Weighted Majority Algorithm}

\subsubsection{Simple Case of the Infallible Expert}
To motivate the general algorithm we first consider the simplified case in which we assume that one of the experts is never wrong. A reasonable algorithm for this case is as follows: 
\begin{itemize}
\item Take a majority vote of the current pool of experts. Once the true realization is revealed, if your guess was incorrect then discard all of incorrect experts (which by definition of the majority
vote will be at least half of them). 
\item Proceed by taking a majority vote for the remaining pool of experts. 
\item Terminate either by reaching the final round $T$ or ending with only the single infallible expert (so will incur zero loss beyond this point). 
\end{itemize}
Since you're discarding at least half of the experts each time you are incorrect, this is just like a binary search for the infallible expert. So after the first mistake you are left with at most
$N/2$ experts, then at most $N/4$ after the second mistake, and in general at most $N/2^m$ after $m$ mistakes. To find the maximum number of mistakes needed to find the infallible expert
we solve $\frac{N}{2^m} = 1$, which yields $m \leq \log_2 N$. Accounting for the possibility that we exhaust the rounds before finding the infallible expert: $m \leq \max\{\log_2 N, T\}$. 
Now, suppose $T >> N$. Let's consider the regret. By definition, the regret for the best expert here is $0$. Thus, 
\[\text{Regret}_T = \frac{1}{T}(\text{\# mistakes}) - 0 \leq \frac{\log_2 N}{T} \to 0 \text{ as } T \to \infty \]
In this case the numerator is quite sub-linear; we can bound it with a constant! Under less restrictive assumptions, the results won't be quite as nice. 

\subsubsection{The General Case}
We now drop the assumption that there exists a majority expert, but can still use the previous algorithm as motivation for the general case. To start, let's consider a different interpretation for 
the previous algorithm. We might imagine assigning each expert a weight that encodes our degree of confidence in their advice. In the infallible expert case, we start at round $0$ by assigning each expert
$i$ a weight of $w^{(i)}_0 = 1$ and then when we make a mistake we set any incorrect experts' weights to $0$. This unforgiving approach works because we know there exists an expert who is never wrong and
thus we need not worry about discarding this expert. In the general case, there might not be an infallible expert but there might be various experts who are correct very often and so it would be unwise 
to discard them. The natural extension is thus to decrease their weight by a little bit but not all the way to $0$. In particular, we might choose some $\epsilon \in (0, 1)$ and decrement the weights of the incorrect
experts by: 
\[w^{(i)}_{t} \gets (1 - \epsilon) w^{(i)}_{t - 1}\]
This leads to the following algorithm. Here I continue to use that notation that $u_t^{(i)}$ is the advice from expert $i$ at round $t$, and $y_t$ is the true realization at time $t$. Also, to simplify notation I encode 
event $A$ as $0$ and $B$ as $1$. 
\bigskip

 \begin{algorithm}[H]
	\SetAlgoLined
	
	\textbf{Input}: $0 < \epsilon < 1$, $T \in \mathbb{N}$, \text{ Experts } X = \{1, \dots, N\} \\
	
	\bigskip
	
	Set $w^{(i)} := 1$ \text{ for all } i = 1, \dots, N \\
	
	\For{$t = 0, \dots, T$} {
		$\phi := \sum_{i \in X} w^{(i)}$ \\
		$x_t := \mathbbm{1}\left\{\sum_{i \in X} \frac{w^{(i)}}{\phi} u_t^{(i)} \geq \frac{1}{2} \right\}$ \\
		
		\If{$x_t \neq y_t$} {
			\For{$i = 1, \dots, N$} {
				\If{$u_t^{(i)} \neq y_t$} {
					$w^{(i)} := \max\left\{0, (1 - \epsilon)w^{(i)}\right\}$ \\
				}
			}
		}		
	}	
\caption{Weighted Majority Algorithm}
\end{algorithm}

\bigskip

To analyze this algorithm, we apply similar arguments to the infallible expert case. Clearly we will need some sort of assumption on the quality of expert advice. This was unnecessary in the infallible expert
case as we could bound the regret entirely based on the existence of the infallible expert; the quality of all of the remaining experts did not factor into that bound. Without that assumption, the experts could 
be arbitrarily bad, so our bound will be a function of $m_T^{(i)}$, the number of mistakes made by expert $i$ up until time $T$. Moreover, let $m_T$ be the number of mistakes made by the algorithm up until
time time (i.e. number of times that $x_t \neq y_t$). 

\begin{prop}
For any $i \in \{1, 2, \dots, N\}$, 
\[m_T \leq 2(1 + \epsilon)m_T^{(i)} + \frac{\log N}{\epsilon}\]
In particular, 
\[m_T \leq 2(1 + \epsilon)m_T^{*} + \frac{\log N}{\epsilon}\]
where $m_T^{*} $
\end{prop}
Before diving into the proof it is worth comparing this to the infallible expert result. In that case we found that (for $T >> N$) that $m_T \leq \log_2 N$. We see that the second term in the above 
bound is the analog of this result, but scaled by $\frac{1}{\epsilon}$. A larger value of $\epsilon$ will punish the incorrect experts more severely and thus serve to reduce the weight of bad experts
more quickly. However, increasing $\epsilon$ will \textit{increase} the first term, which captures the notion that enacting this stricter punishment will also have the adverse effect of rapidly decreasing the 
rate of good experts who just happened to be wrong once or twice. Thus the optimal $\epsilon$ will balance these two factors. On a final note, consider that when $T >> N$ then the first term will 
dominate and thus we conclude for the ``big $T$, small $N$ regime'' that the number of mistakes is bounded by approximately twice the number of mistakes made by the best expert. 

\begin{proof}

\end{proof}


\section{Resources}
\begin{itemize}
\item Potential Function Proofs for Gradient Methods (Bansal and Gupta) 
\item Francesco Orabana (Boston University) lecture notes on online optimization
\item Alina Ene (Boston University) lecture notes from Advanced Optimization Algorithms Spring 2022 course
\item Aaron Singer (Harvard University) lecture notes from Advanced Optimization
\end{itemize}



\end{document}

