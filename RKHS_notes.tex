\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Exp}[1]{\exp\mathopen{}\left\{#1\right\}\mathclose{}}
\newcommand{\Log}[1]{\log\mathopen{}\left\{#1\right\}\mathclose{}}


\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% RKHS commands
\newcommand{\featureMap}{\phi}
\newcommand{\RKHS}{\mathcal{H}}
\newcommand{\Ker}{k}
\newcommand{\dataSpace}{\mathcal{X}}
\newcommand{\dimSpace}{D}
\newcommand{\meanEmb}[1]{\mu_{#1}}
\newcommand{\MMD}{\text{MMD}}
\newcommand{\MMDSpace}{\mathcal{F}}
\newcommand{\rVec}{\mathbf{X}}
\newcommand{\rVecTwo}{\mathbf{Y}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

% Title and author
\title{An Introduction to Reproducing Kernel Hilbert Spaces in Statistics and Machine Learning}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}
There are many ways to motivate Reproducing Kernel Hilbert Spaces (RKHS). They are of pure mathematical interest in addition to providing utility in myriad applied fields. I will motivate RKHS as 
representing the codomain of a feature map in machine learning applications, which will lead us to the celebrated ? theorem, which provides a very useful interpretation of kernels. I will then proceed to 
explore other interpretations and uses of RKHS, emphasizing their power to study classes of functions with certain degrees of smoothness. Mapping features and controlling smoothness of functions do not 
initially appear to be related, but the theory of RKHS will shed light on deep connections between these two ideas. 

\section{Hilbert Space Review}

\section{Machine Learning Motivation: Feature Maps and Kernels}
\subsection{The Big Picture}
Let $\mathcal{X}$ be an arbitrary, non-empty set. We can think of this as a standard input or feature space in a machine learning setting. Note that we have not assumed any structure on $\mathcal{X}$; no notion of 
distance, geometry, etc. It is ideal to be able to develop this theory on a completely arbitrary input set given that in applications input data can come in many forms, without any sort of obvious structure. The general 
machine learning setup is to suppose we have observed training data pairs $\{(x_i, y_i)\}_{i = 1}^{N}$ such that $x_i \in \mathcal{X}$. We could then proceed to fit a model directly on the observed features $x_i$; however, 
it is often advantageous to first derive more informative features $\phi(x_i)$ by applying some feature map $\phi: \mathcal{X} \to \mathcal{H}$. The model is then fit in the transformed space, using the derived training set 
$\{(\phi(x_i), y_i)\}_{i = 1}^{N}$. As an example, suppose we are considering a linear regression model with $\mathcal{X} = \R$. Training a linear model on $\{(x_i, y_i)\}_{i = 1}^{N}$ will only allow us to fit straight lines to the 
dataset. However, if the relationship exhibits non-linearity we may choose to also regress on derived features consisting of polynomial terms: $\phi(x_i) = [1, x_i, x_i^2, \dots, x_i^p]^T$. In this case, the co-domain $\mathcal{H}$
of the feature map is the space of polynomials up to order $p$. Running a regression on $\{(\phi(x_i), y_i)\}_{i = 1}^{N}$ now yields a much more flexible class of models, though also note that the additional flexibility does come 
at a computational cost since the dimensionality of the input space has increased from $1$ to $p$. By increasing $p$ we achieve the ability to represent more complex functions, but the computational expense will also increase. 
\textbf{TODO: } work through the linear model formulas to better motivate the kernel. See how it appears in the equation for $\hat{\beta}$. 

Kernels provide an elegant solution to the issue of increasing computational cost. A key observation is that the additional computational cost of working in the derived feature space often manifests in the form of having to evaluate 
inner products in $\mathcal{H}$, e.g. $\langle \phi(x_i), \phi(x_j) \rangle_{\mathcal{H}} =  \phi(x_i)^T  \phi(x_j)$. Now suppose we knew of a function $k: \mathcal{X} \times \mathcal{X} \to \R$ that satisfied 
\[k(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle_{\mathcal{H}}\]
This would be great since it would allow us to indirectly work in the higher-dimensional space $\mathcal{H}$ while only requiring computations that look like $k(x_i, x_j)$, which take place in the lower-dimensional space. Without providing 
formal definitions yet, we call the function $k(\cdot, \cdot)$ a \textit{kernel}. We can thus view a kernel as providing a ``short-cut'' to operate in higher-dimensional spaces without having to explicitly perform the expensive inner product 
computations in those spaces. Up to this point, we have been considering fixing a feature map $\phi$ and then wondering if there exists a kernel $k$ that can capture the relevant properties of $\phi$. However, it is natural to flip this around 
and start by fixing a kernel, then ask if the choice of kernel implies the existence of some interesting feature space $\mathcal{H}$. If we can find certain kernels that imply rich feature spaces, then we never really even have to think about the 
feature space, but instead just focus on the kernel itself. In fact, we will see that certain choices of the kernel actually imply that $\mathcal{H}$ is infinite-dimensional. This means that it would actually be impossible to perform the operations 
using the associated feature map with finite computational resources. Therefore, kernel methods are incredibly powerful and allow us to fit models that are implicitly using infinite-dimensional derived features, circumventing the need to actually 
do computation in the infinite-dimensional space. As we will see, the derived feature space $\mathcal{H}$ is what we will call the RKHS associated with the kernel $k$ and feature map $\phi: \mathcal{X} \to \mathcal{H}$. While we introduced RKHS from a slightly more theoretical perspective below, it is helpful to keep in mind this idea of feature maps to help provide intuition for the definitions. 

\subsection{Defining RKHS via a Smoothness Property}
There are multiple ways to formally define RKHS. The one we choose here seems quite disconnected from the discussion on feature spaces, but we will establish the connection shortly. We like this definition because it is general and captures 
a smoothness property of RKHS. We are thus taking the first steps to connect the ideas of feature maps and controlling smoothness of functions. 

\begin{definition} 
Let $\mathcal{X}$ be a non-empty set and $\mathcal{H}$ a Hilbert space of functions of the form $f: \mathcal{X} \to \R$. Define the \textit{evaluation functional} $\delta_x: \mathcal{H} \to \R$ associated with a fixed $x \in \mathcal{X}$ by 
\[\delta_x(f) = f(x), \ f \in \mathcal{H}\]
Then $\mathcal{H}$ is called a Reproducing Kernel Hilbert Space (RKHS) if $\delta_x$ is continuous for all $x \in \mathcal{X}$; that is, if all evaluations functionals are continuous. 
\end{definition}
From this definition, we see that RKHS are, first an foremost, a Hilbert space, meaning we have notions such as the length of vectors, distance between them, as well as the concept of orthogonality. Moreover, it is a specific Hilbert space consisting of very well-behaved functions. The following proposition is quite helpful in understanding why the continuous evaluation functional property leads to a space of such nice functions. 
\begin{prop}
Let $\mathcal{H}$ be a RKHS with norm denoted by $\norm{\cdot}_{\mathcal{H}}$ and $f, f_n \in \mathcal{H}$ for $n \in \mathbb{N}$. If $\norm{f_n - f}_{\mathcal{H}} \to 0$ then $f_n \to f$ pointwise. 
\end{prop}
\begin{proof} 
Let $x \in \mathcal{X}$. Then 
\begin{align*}
\abs{f_n(x) - f(x)} &= \abs{\delta_x(f_n) - \delta_x(f)} \\
			   &= \abs{\delta_x(f_n - f)} \\
			   &\leq \norm{\delta_x}\norm{f_n - f}_{\mathcal{H}} \to 0
\end{align*}
We have used the fact that evaluation functionals are continuous, and that the operator norm satisfies $\norm{\delta_x} < \infty$. This latter fact is by definition of the RKHS, seeing that $\delta_x$ is a linear function and hence the continuity 
of this functional is equivalent to boundedness. 
\end{proof}
This property is quite nice. Loosely speaking, it states that functions that are close in RKHS norm must also be close pointwise. In machine learning and statistical applications, it can be very helpful to have a guarantee that functions are close 
on a pointwise basis. While $L_2$ spaces are very useful for discussing things like convergence in an average sense, they provide no such pointwise guarantee. In fact, in $L_2$ functions are only defined up to a set of measure zero, so individual pointwise evaluations are really not the focus at all in these spaces. In general, it is possible to have a convergent sequence in the $L_2$ sense, that does not converge pointwise. 

\subsubsection{Reproducing Kernels}
It perhaps seems a bit odd to motivate RKHS using kernels and feature maps, then define RKHS in such a way that doesn't even mention kernels. We now introduce the notion of a \textit{reproducing kernel} and begin to work our way towards an equivalent definition of RKHS defined in terms of such kernels. In fact, we will shortly prove that the notion of \textit{kernel} as a positive definite function typically used in machine learning settings is actually equivalent to the notion of reproducing kernel defined below. 
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space of real-valued functions defined on non-empty set $\mathcal{X}$. A function $k: \mathcal{X} \times \mathcal{X} \to \R$ is called a reproducing kernel if it satisfies the following properties. 
\begin{enumerate}
\item $\forall x \in \mathcal{X}, k(\cdot, x) \in \mathcal{H}$
\item $\forall x \in \mathcal{X}, \forall f \in \mathcal{H}, \langle f, k(\cdot, x)\rangle_{\mathcal{H}} = f(x)$
\end{enumerate}
\end{definition} 
The first property says that if we fix one of the entries of $k$, then viewed as a function of the other entry the resulting function must be in $\mathcal{H}$. Recall that we might think of $\mathcal{H}$ as a space of infinite-dimensional features derived from the original features $\mathcal{X}$ (this connection will be made precise below). Requiring the function $k(\cdot, x)$ to belong to $\mathcal{H}$ thus means that $k(\cdot, x)$ can be thought of as a feature as well. We can intuitively think of this feature as encoding the ``similarity'' between $x$ and other values $x^\prime \in \mathcal{X}$. The second property is known as the \textit{reproducing property}, and states that the RKHS inner product between any $f \in \mathcal{H}$ and $k(\cdot, x)$ is equivalent to simply evaluating $f$ at $x$. Thus, $k(\cdot, x)$ ``reproduces'' the functions in the RKHS. In other words, in RKHS pointwise function evaluation can be represented as an inner product. Note that it follows immediately from this definition that 
\[\langle k(\cdot, x), k(\cdot, y) \rangle_{\mathcal{H}} = k(x, y) = k(y, x) \]
Thus, there is a sort of duality between $k(x, y)$, which captures the similarity between the finite-dimensional features $x$ and $y$, and 
$\langle k(\cdot, x), k(\cdot, y) \rangle_{\mathcal{H}}$, which captures the similarity between the infinite-dimensional features $k(\cdot, x)$ and $k(\cdot, y)$. 

It turns out that the the reproducing kernel both exists and is unique. In fact, we will show that $\mathcal{H}$ being a RKHS is equivalent $\mathcal{H}$ having a reproducing kernel. This result will establish 
the equivalence of the smoothness property (evaluation functionals are continuous) and the existence of a reproducing kernel. We start by showing uniqueness. 
\begin{prop}
If it exists, the reproducing kernel is unique. 
\end{prop}
\begin{proof} 
Suppose $k_1$ and $k_2$ are both reproducing kernels. Then by definition, for any $f \in \mathcal{H}$ and $x \in \mathcal{X}$, 
$\langle f, k_1(\cdot, x)\rangle_{\mathcal{H}} = f(x) = \langle f, k_2(\cdot, x)\rangle_{\mathcal{H}}$. Then, using the bilinearity of the 
inner product,
\[\langle f, k_1(\cdot, x) - k_2(\cdot, x) \rangle_{\mathcal{H}} = 0\]
Since this holds for all $f \in \mathcal{H}$, and the only vector that is orthogonal to every other vector in the space is the zero vector, then we conclude
\[k_1(\cdot, x) - k_2(\cdot, x) = 0\]
or 
\[k_1(\cdot, x) = k_2(\cdot, x)\]

\end{proof}
 Next, we prove existence, which requires an application of the well-known \textit{Riesz representation theorem}. Importantly, the existence proof tells us that a RKHS can equivalently be defined as
 a Hilbert space that possesses a reproducing kernel. In other words, the existence of a reproducing kernel is equivalent to pointwise function evaluation being a continuous operation. 
 \begin{thm}
 $\mathcal{H}$ is a RKHS if and only if it has a reproducing kernel. 
 \end{thm}
 \begin{proof} 
 $(\impliedby)$ Suppose $\mathcal{H}$ is a Hilbert space with a reproducing kernel $k$. We show that this implies the evaluation functional $\delta_x: \mathcal{H} \to \R$ is continuous, for any 
 $x \in \mathcal{X}$. Equivalently, we can show that $\delta_x$ is bounded in operator norm. To this end, fix $x \in \mathcal{X}$ and consider
 \begin{align*}
 \abs{\delta_x(f)} &= \abs{f(x)} \\
 			  &= \langle f, k(\cdot, x) \rangle_{\mathcal{H}} \\
			  &\leq \norm{f}_{\mathcal{H}} \norm{k(\cdot, x)}_{\mathcal{H}} \\
			  &=: \norm{f}_{\mathcal{H}} C_x
 \end{align*}
 where the second line uses the reproducing property. Note that $C_x := \norm{k(\cdot, x)}_{\mathcal{H}}$ only depends on $x$, not $f$. Thus, for any $f \in \mathcal{H}$, 
 \[\frac{ \abs{\delta_x(f)}}{\norm{f}_{\mathcal{H}}} \leq C_x\]
 which implies $\norm{\delta_x} \leq C_x$; i.e. that the evaluation functional $\delta_x$ is bounded, and hence continuous.
 
 $(\implies)$ Now suppose that for any $x \in \mathcal{X}$, the evaluation functional $\delta_x: \mathcal{H} \to \R$ is a bounded, linear operator (i.e. $\delta_x$ is in the topological dual space 
 of $\mathcal{H}$). We must establish the existence of a reproducing kernel; that is, a mapping $k: \mathcal{X} \times \mathcal{X} \to \R$ satisfying $k(\cdot, x) \in \mathcal{H}$ and the 
 reproducing property. Since $\delta_x$ is linear and bounded, then the Riesz representation theorem asserts the existence of a function $f_x \in \mathcal{H}$ satisfying 
 \[\delta_x(f) = \langle f, f_x \rangle_{\mathcal{H}}, \text{ for all } f \in \mathcal{H}\]
 Define the function $k: \mathcal{X} \times \mathcal{X} \to \R$ by $k(x^\prime, x) := f_x(x^\prime)$ for all $x, x^\prime \in \mathcal{H}$. We now check that $k$ satisfies the two properties of a 
 reproducing kernel. First, we have
 \[f_x \in \mathcal{H} \implies k(\cdot, x) \in \mathcal{H}\]
 so the first property is satisfied. The reproducing property follows from 
 \[\langle f, k(\cdot, x)\rangle_{\mathcal{H}} = \langle f, f_x \rangle_{\mathcal{H}} = \delta_x(f) = f(x)\]
 which completes the proof. 
\end{proof}

 To review, we defined an RKHS as a special type of Hilbert space in which all of the evaluation functionals are continuous. We showed that this property is equivalent to the space possessing 
 a reproducing kernel. As an alternative exposition, we could have defined a RKHS as a Hilbert space $\mathcal{H}$ of functions with domain $\mathcal{X}$ 
 satisfying the property that for any $x \in \mathcal{X}$ there exists a function $f_x \in \mathcal{H}$ with the property that 
 \[f(x) = \langle f, f_x \rangle_{\mathcal{H}}, \text{ for all } f \in \mathcal{H} \]
 With this definition, we see clearly that RKHS are Hilbert spaces in which point evaluation can be represented as an inner product. The continuity of inner products, thus implies that point evaluation 
 is continuous, thus arriving at the property used in the original definition we provided for RKHS. Given the alternative definition for RKHS here, we could then introduce the notation 
 $k(\cdot, x) := f_x(\cdot)$ to complete the connection with our earlier exposition. 
 
\subsection{Connecting back to Feature Maps}
 Up until this point, the concept of a reproducing kernel may seem somewhat cryptic. We have seen that reproducing kernels provide a way to express function evaluation as an inner product; 
 $k(\cdot, x)$ is sometimes referred to as the \textit{representer of evaluation} at $x$. But what really is a reproducing kernel? We now begin make our way towards connecting the concept of a 
 reproducing kernel to the more familiar concept of a kernel often defined in treatments of introductory level machine learning. We begin by establishing one of the most important properties of 
 reproducing kernels: that they are positive-definite functions. In the following section, we will prove a sort of converse which establishes a deep connection between reproducing kernels and 
 feature maps. To begin, we define positive definiteness. 
 \begin{definition}
 A symmetric function $h: \mathcal{X} \times \mathcal{X} \to \R$ is called positive definite if for any $n \in \mathbb{N}$ and all $\mathbf{a} = (a_1, \dots, a_n)^T \in \R^n$ and 
 $x_1, \dots, x_n \in \mathcal{X}$, 
 \[\sum_{i = 1}^{n} \sum_{j = 1}^{n} a_i a_j h(x_i, x_j) \geq 0\]
 By defining the $n \times n$ matrix $\mathbf{H}$ with entries $\mathbf{H}_{ij} = h(x_i, x_j)$ we can also write this condition as 
 \[\mathbf{a}^T \mathbf{H} \mathbf{a} \geq 0\]
 \end{definition}
 Given that this definition is made with respect to a greater-than-or-equal-to sign, it seems like it might be more appropriate to term this condition \textit{positive semidefinite}. Indeed, some authors 
 use this terminology, which coincides with the use in linear algebra. However, the definition given above is the standard in machine learning literature. Consider a vector space $\mathcal{U}$ with inner product 
 $\langle \cdot, \cdot \rangle$. Recall from the definition of inner products, the requirement that for any $u \in \mathcal{U}$, $\langle u, u \rangle > 0$. This property is also typically 
 called \textit{positive definiteness}, though it clearly appears to be distinct from the definition given above for general symmetric functions. It turns out that all inner products are positive definite (in the sense of 
 the above definition), which follows from the positive definiteness (in the latter sense mentioned here). 
 \begin{prop}
 Let $\mathcal{U}$ be a real vector space with inner product $\langle \cdot, \cdot \rangle$. Then 
 $\langle \cdot, \cdot \rangle: \mathcal{U} \times \mathcal{U} \to \R$ is a positive definite function. 
 \end{prop} 
\begin{proof}
 First note that by definition of inner products, $\langle \cdot, \cdot \rangle$ is symmetric. Then for any $n \in \mathbb{N}$, $\mathbf{a} = (a_1, \dots, a_n)^T \in \R^n$ and 
 $u_1, \dots, u_n \in \mathcal{U}$, 
 \begin{align*}
 \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_i a_j \langle u_i, u_j \rangle &=  \sum_{i = 1}^{n} \sum_{j = 1}^{n} \langle a_i u_i, a_j u_j \rangle \\
 										                 &= \langle \sum_{i = 1}^{n}  a_i u_i, \sum_{j = 1}^{n}  a_j u_j \rangle  \\
										                 &= \langle \sum_{i = 1}^{n}  a_i u_i, \sum_{i = 1}^{n}  a_i u_i \rangle \\
										                 &> 0
 \end{align*}
 where the first two lines use the bilinearity of inner products, and the final line uses the positive-definite property of inner products. 
\end{proof}
 So all inner products are symmetric positive definite, bilinear functions. Recall that in the machine learning setup, we start with an arbitrary non-empty set $\mathcal{X}$ with no additional structure. The idea 
 was that we can apply a feature map $\phi$ to points in $\mathcal{X}$ in order to map them to a more structured space (e.g. think of representing images as vectors of pixel values). The following theorem states 
 that a function $h$ that implicitly accesses the inner product between transformed features $\phi(x)$ and $\phi(y)$ is itself a positive definite function. The proof is almost identical to the previous proof that inner 
 products are positive definite. 
 \begin{thm}
 Let $\mathcal{X}$ be a non-empty set, $\mathcal{F}$ an arbitrary real Hilbert space, and $\phi: \mathcal{X} \to \mathcal{F}$. Define the function $h: \mathcal{X} \times \mathcal{X} \to \R$ by 
 $h(x, y) := \langle \phi(x), \phi(y) \rangle_{\mathcal{F}}$. Then $h$ is a positive definite function. 
 \end{thm}
 \begin{proof}
 Note that $h$ is symmetric since $\langle \cdot, \cdot \rangle_{\mathcal{F}}$ is symmetric. Then for any $n \in \mathbb{N}$, $\mathbf{a} = (a_1, \dots, a_n)^T \in \R^n$ and 
 $x_1, \dots, x_n \in \mathcal{X}$, 
 \begin{align*}
 \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_i a_j h(x_i, x_j) &=  \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_i a_j  \left\langle \phi(x_i), \phi(x_j) \right\rangle_{\mathcal{F}} \\
 									      &= \left\langle  \sum_{i = 1}^{n} a_i \phi(x_i), \sum_{i = 1}^{n}  a_i \phi(x_i) \right\rangle_{\mathcal{F}} \\
									      &\geq 0
 \end{align*}
 where the logic follows from properties of the inner product, exactly as in the previous proof. 
 \end{proof}
 An interesting question is: does this go the other way? In particular, if we're given an arbitrary positive definite function, does their exist a feature space and feature map such that the positive definite 
 function can be represented as an inner product in that space? If true, this would yield a very interesting characterization of what it means to be a positive definite function; namely, that positive definite 
 functions are simply functions that implicitly access inner products in some other space. The answer to this question is yes, and the result is known as the \textit{Moore-Aronszajn theorem}. Before getting 
 to this theorem, we prove two simpler intermediate results: 1.) that reproducing kernels are positive-definite functions, and 2.) reproducing kernels obey a Cauchy-Schwarz type inequality.
 
 \begin{lemma} 
 Reproducing kernels are positive definite. 
 \end{lemma}
 \begin{proof} 
 We can simply apply the previous result. In particular, let $\mathcal{H}$ be the Hilbert space of functions $f: \mathcal{X} \to \R$ with reproducing kernel $k: \mathcal{X} \times \mathcal{X} \to \R$. 
 Define $\phi: \mathcal{X} \to \mathcal{H}$ by $\phi(x) = k(\cdot, x)$. Since $\phi(x) \in \mathcal{H}$ we can then exploit the reproducing property to conclude
 \[k(x, x^\prime) = \langle k(\cdot, x), k(\cdot, x^\prime) \rangle_{\mathcal{H}} = \langle \phi(x), \phi(x^\prime) \rangle_{\mathcal{H}}\]
 The setup of the previous theorem is now satisfied, so we can conclude that $k$ is positive definite. 
 \end{proof}
 
 \begin{lemma} 
 If $h: \mathcal{X} \times \mathcal{X} \to \R$ is positive definite, then for $x, x^\prime \in \mathcal{X}$, 
 \[\abs{h(x, x^\prime)}^2 \leq h(x, x)h(x^\prime, x^\prime)\]
 \end{lemma}
 \begin{proof} 
 
 \end{proof}
 So we now know that reproducing kernels are positive definite functions, and also obey a Cauchy-Schwarz type inequality; both facts indicate that they are closely related to inner products. In fact, 
 we have showed that the reproducing kernel can be represented as an inner product $k(x, x^\prime) = \langle \phi(x), \phi(x^\prime) \rangle_{\mathcal{H}}$, where $\phi(x) = k(\cdot, x)$. This construction 
 will be central in the statement and proof of the \textit{Moore-Aronszajn theorem}. 
 
 \section{Examples of RKHS}
 \begin{itemize}
 \item Set of real-valued functions with bounded Fourier transform (i.e. frequencies bounded in some interval $[-a, a]$). 
 \item Sobolev space of order $m$. 
 \end{itemize}


\section{Maximum Mean Discrepancy and Two-Sample Testing}
Here we consider the classical problem of distinguishing between two probability distributions $P$ and $Q$ given iid samples from each. In particular, we want to use data 
$X_1, \dots, X_M \overset{iid}{\sim} P$ and $Y_1, \dots, Y_N \overset{iid}{\sim} Q$ to test whether $P \neq Q$. Classical approaches to this problem typically make distributional 
assumptions about $P$ and $Q$; the approach considered here is a non-parametric one. 

In addition to providing a method of non-parametric two-sample testing, the methods discussed here will yield a metric on probability distributions that is amenable to estimation 
from samples. Since the test statistic in the two-sample test will be defined in terms of this metric, we start with the latter. Lemma 1 of \cite{Gretton1} states that 
$P = Q$ if and only if $\E_P[f(X)] = \E_Q[f(X)]$ for all $f \in C(\mathcal{X})$, the space of bounded continuous functions on a separable metric space 
$\mathcal{X}$. This fact might lead us to define a metric something like 
\begin{align*}
d(P, Q) = \sup_{f \in C(\mathcal{X})} \left(\E_P[f(X)] - \E_Q[f(X)]\right)
\end{align*}
[\textbf{TODO}: why isn't there an absolute value here?]
The above mentioned lemma implies that $P = Q$ if and only if $d(P, Q) = 0$, as required by any metric $d(\cdot, \cdot)$. However, for practical purposes the function space 
$C(\mathcal{X})$ is too ``rich'' to work with. The first task is thus to reduce this to a more tractable function space $\mathcal{F} \subset C(\mathcal{X})$, 
while maintaining enough richness to guarantee $P = Q \iff d(P, Q) = 0$. Such a restriction leads of to the definition of maximum mean discrepancy, defined below. 

\subsection{Maximum Mean Discrepancy}
Restricting the class of test functions to $\mathcal{F} \subset C(\mathcal{X})$ yields the following definition.

\begin{definition}
Let $\mathcal{F}$ be a class of functions $f: \mathcal{X} \to \R$. Then the \textbf{maximum mean discrepancy (MMD)} is defined as 
\begin{align}
\MMD(P, Q; \MMDSpace) := \sup_{f \in \mathcal{F}} \left(\E_P[f(X)] - \E_Q[f(Y)]\right),
\end{align}
which can be empirically estimated as 
\begin{align}
\MMD_{M,N}(P, Q; \MMDSpace) := \sup_{f \in \mathcal{F}} \left(\frac{1}{M} \sum_{i = 1}^{M} f(X_i) - \frac{1}{N} \sum_{i = 1}^{N} f(Y_i)\right)
\end{align}
\end{definition}

Provided that $\mathcal{F}$ is rich enough to retain the property $P = Q \iff \MMD(P, Q; \MMDSpace) = 0$ then this is a theoretically well-grounded metric. However, in its current form 
it appears completely intractable as it requires computing a supremum over sample means taken over infinitely many functions. The key idea to make this 
feasible is to define $\mathcal{F}$ implicitly via a kernel $k(\cdot, \cdot)$. Indeed, \cite{Gretton1} proposes choosing $\mathcal{F}$ to be the unit ball in an RKHS 
$\mathcal{H}$. It is proved in \cite{Gretton2} that this choice retains the required properties to define a metric [\textbf{TODO}: add this proof, it is not long]. 
Before proceeding, we must specify a couple technicalities: the space $\mathcal{X}$ is assumed to be compact, and 
the RKHS $\mathcal{H}$ is assumed to be universal, meaning that it is dense in $C(\mathcal{X})$ with respect to $L_\infty$. To have a concrete example in mind,
Gaussian and Laplace kernels induce universal RKHS. 

The task is now to exploit the RKHS structure to obtain a practically computable estimator for the MMD. To this end, we first establish some notation. 
\begin{notation}
\begin{enumerate}
Let $\Ker(\cdot, \cdot)$ be a kernel with associated RKHS $\RKHS$.
	\item Define the \textbf{feature map} $\featureMap: \dataSpace \to \RKHS$ by $\featureMap(x) = \Ker(x, \cdot)$. 
	\item Define the \textbf{mean embedding} $\meanEmb{P} := \E_{x \sim P}\left[\featureMap(x) \right] = \E_{x \sim P}\left[\Ker(x, \cdot) \right] $
\end{enumerate}
\end{notation}
The feature map maps a feature in the original space $x \in \dataSpace$ to a derived feature $\featureMap(x) \in \RKHS$ in the infinite-dimensional feature space. 
The mean embedding is the expected derived feature under a distribution $P$ on $\dataSpace$. We also recall the \textit{reproducing property} for the kernel; for 
$f \in \RKHS$ and $x \in \dataSpace$, 
\begin{align}
f(x) &= \left\langle \Ker(x, \cdot), f \right\rangle = \left\langle \featureMap(x), f \right\rangle.
\end{align}

We are now ready to prove the following result, which establishes 1.) a useful theoretical interpretation of the MMD as the distance between kernel mean embeddings in 
$\RKHS$, and 2.) a useful practical expression of the MMD written in terms of $\Ker(\cdot, \cdot)$, such that it is amenable to estimation from samples. 

\begin{lemma} 
Let $\MMDSpace = \{f \in \RKHS: \norm{f}_{\RKHS} = 1\}$ where $\RKHS$ is the RKHS induced by a universal kernel $\Ker(\cdot, \cdot)$. Then, 
\begin{align}
\MMD(P, Q; \MMDSpace) = \norm{\meanEmb{P} - \meanEmb{Q}}_{\RKHS} = \norm{\E_{x \sim P}[\featureMap(x)] - \E_{y \sim Q}[\featureMap(y)]}_{\RKHS} \label{MMD_mean_embedding}
\end{align}
and
\begin{align}
\MMD^2(P, Q; \MMDSpace) = \E_{x, x^\prime \sim P}\left[\Ker(x, x^\prime)\right] + \E_{y, y^\prime \sim P}\left[\Ker(y, y^\prime)\right] - 2 \E_{x \sim P}\E_{y \sim Q} \left[\Ker(x, y)\right] \label{MMD_kernel}
\end{align}
\end{lemma}

\begin{proof}
For the first result we have, 
\begin{align*}
\MMD(P, Q; \MMDSpace) 
&= \sup_{f \in \MMDSpace} \left(\E_P[f(x)] - \E_Q[f(y)]\right) \\
&= \sup_{\norm{f}_{\RKHS} = 1} \left(\E_P \langle \featureMap(x), f \rangle - \E_Q \langle \featureMap(y), f \rangle\right) &&\text{reproducing property} \\
&= \sup_{\norm{f}_{\RKHS} = 1} \left(\langle \E_P [\featureMap(x)], f \rangle_{\RKHS} - \langle \E_Q [\featureMap(y)], f \rangle_{\RKHS} \right) &&\text{linearity} \\
&= \sup_{\norm{f}_{\RKHS} = 1} \langle \E_P [\featureMap(x)] - \E_Q [\featureMap(y)], f \rangle_{\RKHS}  &&\text{bilinearity} \\
&= \sup_{\norm{f}_{\RKHS} = 1} \langle \meanEmb{P} - \meanEmb{Q}, f \rangle_{\RKHS}  &&\text{definition of mean embedding} \\
&= \frac{\left\langle \meanEmb{P} - \meanEmb{Q},  \meanEmb{P} - \meanEmb{Q} \right\rangle_{\RKHS}}{\norm{ \meanEmb{P} - \meanEmb{Q}}_{\RKHS}}  &&(\dagger) \\
&= \norm{ \meanEmb{P} - \meanEmb{Q}}_{\RKHS} &&\langle g, g \rangle_{\RKHS} = \norm{g}^2_{\RKHS}
\end{align*}
where $(\dagger)$ follows from the fact that the inner product is maximized when the vectors are aligned. Since $f$ is constrained to have unit norm, the 
maximizer is $f = (\meanEmb{P} - \meanEmb{Q}) / \norm{\meanEmb{P} - \meanEmb{Q}}_{\RKHS}$.

Now for the second result, 
\begin{align*}
\MMD^2(P, Q; \MMDSpace) 
&= \norm{\meanEmb{P} - \meanEmb{Q}}^2_{\RKHS} &&\text{previous result} \\
&= \left\langle \meanEmb{P} - \meanEmb{Q}, \meanEmb{P} - \meanEmb{Q} \right\rangle &&\langle g, g \rangle_{\RKHS} = \norm{g}^2_{\RKHS} \\
&= \left\langle \meanEmb{P}, \meanEmb{P} \right\rangle_{\RKHS} + \left\langle \meanEmb{Q}, \meanEmb{Q} \right\rangle_{\RKHS} - 2 \left\langle \meanEmb{P}, \meanEmb{Q} \right\rangle_{\RKHS} &&\text{bilinearity} \\
&= \left\langle \E_{x \sim P}\left[\Ker(x, \cdot)\right], \E_{x^\prime \sim P}\left[\Ker(x^\prime, \cdot)\right] \right\rangle_{\RKHS} + &&\text{def of mean embedding} \\
	&\qquad \left\langle \E_{y \sim Q}\left[\Ker(y, \cdot)\right], \E_{y \sim Q}\left[\Ker(y, \cdot)\right] \right\rangle_{\RKHS} - \\
	&\qquad 2 \left\langle \E_{x \sim P}\left[\Ker(x, \cdot)\right], \E_{y \sim Q}\left[\Ker(y, \cdot)\right] \right\rangle_{\RKHS} \\
&= \E_{x, x^\prime \sim P} \left[\langle \Ker(x, \cdot), \Ker(x^\prime, \cdot)\rangle \right] + &&\text{linearity} \\
	&\qquad \E_{y, y^\prime \sim Q} \left[\langle \Ker(y, \cdot), \Ker(y^\prime, \cdot)\rangle \right] - \\
	&\qquad \E_{x \sim P} \E_{y \sim Q} \left[\langle \Ker(x, \cdot), \Ker(y, \cdot) \right] \\
&= \E_{x, x^\prime \sim P}\left[\Ker(x, x^\prime) \right] + \E_{y, y^\prime \sim P}\left[\Ker(y, y^\prime) \right] - 2 \E_{x \sim P} \E_{y \sim Q}\left[\Ker(x, y)\right] &&\text{reproducing prop}
\end{align*}
\end{proof}

While the definition of MMD is given as a supremum over the difference in expectations taken over a set of test functions, $\MMDSpace$, the above result gives an alternative definition 
when $\MMDSpace$ is taken as the unit ball in an RKHS; namely, the MMD is the distance between the expected derived features in the feature space $\RKHS$. The feature map 
$\featureMap(x)$ may capture complicated, non-linear structure in $x$; such structure is compared in the space $\RKHS$. The second result writes the MMD in terms of expectations 
of kernels, something we can readily estimate given iid samples from $P$ and $Q$. 

% Estimating MMD
\subsubsection{Estimating MMD}

% Specific MMD Examples
\subsubsection{Specific MMD Examples}
Here we present common choices for the kernel $\Ker(\cdot, \cdot)$, which lead to different instantiations of MMD. 

\bigskip
\noindent
\textbf{Energy Distance.} Letting $\rVec$ and $\rVecTwo$ denote random vectors in $\R^\dimSpace$, we consider a kernel equal to the negative Euclidean distance between the vectors,
\begin{align}
\Ker(\rVec, \rVecTwo) := -\norm{\rVec - \rVecTwo}_2.
\end{align}
Plugging into \ref{MMD_kernel}, we then obtain the MMD 
\begin{align}
\MMD^2(P, Q; \MMDSpace) = 2 \E_{\rVec \sim P}\E_{\rVecTwo \sim Q}\norm{\rVec - \rVecTwo}_2. -  \E_{\rVec, \rVec^\prime \sim P}\norm{\rVec - \rVec^\prime}_2 -  \E_{\rVecTwo, \rVecTwo^\prime \sim P}\norm{\rVecTwo - \rVecTwo^\prime}_2. \label{energy_distance}
\end{align}
This is identical to the \textit{energy distance} defined, for example, in \cite{Mak}. 



\section{References:}
\begin{itemize}
\item What is an RKHS? (Gretton)
\item Statistical Machine Learning (CMU lectures by Larry Wassterman and Ryan Tibshirani); see lecture on two-sample testing. 
\item Look into why KL isn't a good metric for density estimation due to its sensitivities on the tail of the distribution (Noted by Wasserman in lecture on density estimation). Works much better in MLE. 
\item See Larry Wasserman's notes on minimax theory (I believe he talks about Hellinger distance).  
\end{itemize}

% Bibliography
\begin{thebibliography}{20}
\bibitem{Gretton1} Arthur Gretton, Karsten M. Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alexander J. Smola. 2006. A kernel method for the two-sample-problem. In Proceedings of the 19th International Conference on Neural Information Processing Systems (NIPS'06). MIT Press, Cambridge, MA, USA, 513–520.
\bibitem{Gretton2} A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. Smola. A kernel method for the two sample
problem. Technical Report 157, MPI for Biological Cybernetics, 2007.
\bibitem{Muandet} Krikamol Muandet; Kenji Fukumizu; Bharath Sriperumbudur; Bernhard Schölkopf, Kernel Mean Embedding of Distributions: A Review and Beyond , now, 2017.
\bibitem{Hofmann} Thomas Hofmann, Bernhard Schölkopf, Alexander J. Smola. "Kernel methods in machine learning." The Annals of Statistics, 36(3) 1171-1220 June 2008.
\bibitem{Mak} Mak, Simon \& Joseph, V. Roshan. (2017). Support points. Annals of Statistics. 46. 10.1214/17-AOS1629. 
\end{thebibliography}

\end{document}


