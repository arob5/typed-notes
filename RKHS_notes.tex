\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{An Introduction to Reproducing Kernel Hilbert Spaces in Statistics and Machine Learning}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}
There are many ways to motivate Reproducing Kernel Hilbert Spaces (RKHS). They are of pure mathematical interest in addition to providing utility in myriad applied fields. I will motivate RKHS as 
representing the codomain of a feature map in machine learning applications, which will lead us to the celebrated ? theorem, which provides a very useful interpretation of kernels. I will then proceed to 
explore other interpretations and uses of RKHS, emphasizing their power to study classes of functions with certain degrees of smoothness. Mapping features and controlling smoothness of functions do not 
initially appear to be related, but the theory of RKHS will shed light on deep connections between these two ideas. 

\section{Hilbert Space Review}

\section{Machine Learning Motivation: Feature Maps and Kernels}
\subsection{The Big Picture}
Let $\mathcal{X}$ be an arbitrary, non-empty set. We can think of this as a standard input or feature space in a machine learning setting. Note that we have not assumed any structure on $\mathcal{X}$; no notion of 
distance, geometry, etc. It is ideal to be able to develop this theory on a completely arbitrary input set given that in applications input data can come in many forms, without any sort of obvious structure. The general 
machine learning setup is to suppose we have observed training data pairs $\{(x_i, y_i)\}_{i = 1}^{N}$ such that $x_i \in \mathcal{X}$. We could then proceed to fit a model directly on the observed features $x_i$; however, 
it is often advantageous to first derive more informative features $\phi(x_i)$ by applying some feature map $\phi: \mathcal{X} \to \mathcal{H}$. The model is then fit in the transformed space, using the derived training set 
$\{(\phi(x_i), y_i)\}_{i = 1}^{N}$. As an example, suppose we are considering a linear regression model with $\mathcal{X} = \R$. Training a linear model on $\{(x_i, y_i)\}_{i = 1}^{N}$ will only allow us to fit straight lines to the 
dataset. However, if the relationship exhibits non-linearity we may choose to also regress on derived features consisting of polynomial terms: $\phi(x_i) = [1, x_i, x_i^2, \dots, x_i^p]^T$. In this case, the co-domain $\mathcal{H}$
of the feature map is the space of polynomials up to order $p$. Running a regression on $\{(\phi(x_i), y_i)\}_{i = 1}^{N}$ now yields a much more flexible class of models, though also note that the additional flexibility does come 
at a computational cost since the dimensionality of the input space has increased from $1$ to $p$. By increasing $p$ we achieve the ability to represent more complex functions, but the computational expense will also increase. 
\textbf{TODO: } work through the linear model formulas to better motivate the kernel. See how it appears in the equation for $\hat{\beta}$. 

Kernels provide an elegant solution to the issue of increasing computational cost. A key observation is that the additional computational cost of working in the derived feature space often manifests in the form of having to evaluate 
inner products in $\mathcal{H}$, e.g. $\langle \phi(x_i), \phi(x_j) \rangle_{\mathcal{H}} =  \phi(x_i)^T  \phi(x_j)$. Now suppose we knew of a function $k: \mathcal{X} \times \mathcal{X} \to \R$ that satisfied 
\[k(x_i, x_j) = \langle \phi(x_i), \phi(x_j) \rangle_{\mathcal{H}}\]
This would be great since it would allow us to indirectly work in the higher-dimensional space $\mathcal{H}$ while only requiring computations that look like $k(x_i, x_j)$, which take place in the lower-dimensional space. Without providing 
formal definitions yet, we call the function $k(\cdot, \cdot)$ a \textit{kernel}. We can thus view a kernel as providing a ``short-cut'' to operate in higher-dimensional spaces without having to explicitly perform the expensive inner product 
computations in those spaces. Up to this point, we have been considering fixing a feature map $\phi$ and then wondering if there exists a kernel $k$ that can capture the relevant properties of $\phi$. However, it is natural to flip this around 
and start by fixing a kernel, then ask if the choice of kernel implies the existence of some interesting feature space $\mathcal{H}$. If we can find certain kernels that imply rich feature spaces, then we never really even have to think about the 
feature space, but instead just focus on the kernel itself. In fact, we will see that certain choices of the kernel actually imply that $\mathcal{H}$ is infinite-dimensional. This means that it would actually be impossible to perform the operations 
using the associated feature map with finite computational resources. Therefore, kernel methods are incredibly powerful and allow us to fit models that are implicitly using infinite-dimensional derived features, circumventing the need to actually 
do computation in the infinite-dimensional space. As we will see, the derived feature space $\mathcal{H}$ is what we will call the RKHS associated with the kernel $k$ and feature map $\phi: \mathcal{X} \to \mathcal{H}$. While we introduced RKHS from a slightly more theoretical perspective below, it is helpful to keep in mind this idea of feature maps to help provide intuition for the definitions. 

\subsection{Defining RKHS via a Smoothness Property}
There are multiple ways to formally define RKHS. The one we choose here seems quite disconnected from the discussion on feature spaces, but we will establish the connection shortly. We like this definition because it is general and captures 
a smoothness property of RKHS. We are thus taking the first steps to connect the ideas of feature maps and controlling smoothness of functions. 

\begin{definition} 
Let $\mathcal{X}$ be a non-empty set and $\mathcal{H}$ a Hilbert space of functions of the form $f: \mathcal{X} \to \R$. Define the \textit{evaluation functional} $\delta_x: \mathcal{H} \to \R$ associated with a fixed $x \in \mathcal{X}$ by 
\[\delta_x(f) = f(x), \ f \in \mathcal{H}\]
Then $\mathcal{H}$ is called a Reproducing Kernel Hilbert Space (RKHS) if $\delta_x$ is continuous for all $x \in \mathcal{X}$; that is, if all evaluations functionals are continuous. 
\end{definition}
From this definition, we see that RKHS are, first an foremost, a Hilbert space, meaning we have notions such as the length of vectors, distance between them, as well as the concept of orthogonality. Moreover, it is a specific Hilbert space consisting of very well-behaved functions. The following proposition is quite helpful in understanding why the continuous evaluation functional property leads to a space of such nice functions. 
\begin{prop}
Let $\mathcal{H}$ be a RKHS with norm denoted by $\norm{\cdot}_{\mathcal{H}}$ and $f, f_n \in \mathcal{H}$ for $n \in \mathbb{N}$. If $\norm{f_n - f}_{\mathcal{H}} \to 0$ then $f_n \to f$ pointwise. 
\end{prop}
\begin{proof} 
Let $x \in \mathcal{X}$. Then 
\begin{align*}
\abs{f_n(x) - f(x)} &= \abs{\delta_x(f_n) - \delta_x(f)} \\
			   &= \abs{\delta_x(f_n - f)} \\
			   &\leq \norm{\delta_x}\norm{f_n - f}_{\mathcal{H}} \to 0
\end{align*}
We have used the fact that evaluation functionals are continuous, and that the operator norm satisfies $\norm{\delta_x} < \infty$. This latter fact is by definition of the RKHS, seeing that $\delta_x$ is a linear function and hence the continuity 
of this functional is equivalent to boundedness. 
\end{proof}
This property is quite nice. Loosely speaking, it states that functions that are close in RKHS norm must also be close pointwise. In machine learning and statistical applications, it can be very helpful to have a guarantee that functions are close 
on a pointwise basis. While $L_2$ spaces are very useful for discussing things like convergence in an average sense, they provide no such pointwise guarantee. In fact, in $L_2$ functions are only defined up to a set of measure zero, so individual pointwise evaluations are really not the focus at all in these spaces. In general, it is possible to have a convergent sequence in the $L_2$ sense, that does not converge pointwise. 

\subsubsection{Reproducing Kernels}
It perhaps seems a bit odd to motivate RKHS using kernels and feature maps, then define RKHS in such a way that doesn't even mention kernels. We now introduce the notion of a \textit{reproducing kernel} and begin to work our way towards an equivalent definition of RKHS defined in terms of such kernels. In fact, we will shortly prove that the notion of \textit{kernel} as a positive definite function typically used in machine learning settings is actually equivalent to the notion of reproducing kernel defined below. 
\begin{definition}
Let $\mathcal{H}$ be a Hilbert space of real-valued functions defined on non-empty set $\mathcal{X}$. A function $k: \mathcal{X} \times \mathcal{X} \to \R$ is called a reproducing kernel if it satisfies the following properties. 
\begin{enumerate}
\item $\forall x \in \mathcal{X}, k(\cdot, x) \in \mathcal{H}$
\item $\forall x \in \mathcal{X}, \forall f \in \mathcal{H}, \langle f, k(\cdot, x)\rangle_{\mathcal{H}} = f(x)$
\end{enumerate}
\end{definition} 
The first property says that if we fix one of the entries of $k$, then viewed as a function of the other entry the resulting function must be in $\mathcal{H}$. Recall that we might think of $\mathcal{H}$ as a space of infinite-dimensional features derived from the original features $\mathcal{X}$ (this connection will be made precise below). Requiring the function $k(\cdot, x)$ to belong to $\mathcal{H}$ thus means that $k(\cdot, x)$ can be thought of as a feature as well. We can intuitively think of this feature as encoding the ``similarity'' between $x$ and other values $x^\prime \in \mathcal{X}$. The second property is known as the \textit{reproducing property}, and states that the RKHS inner product between any $f \in \mathcal{H}$ and $k(\cdot, x)$ is equivalent to simply evaluating $f$ at $x$. Thus, $k(\cdot, x)$ ``reproduces'' the functions in the RKHS. In other words, in RKHS pointwise function evaluation can be represented as an inner product. Note that it follows immediately from this definition that 
\[\langle k(\cdot, x), k(\cdot, y) \rangle_{\mathcal{H}} = k(x, y) = k(y, x) \]
Thus, there is a sort of duality between $k(x, y)$, which captures the similarity between the finite-dimensional features $x$ and $y$, and 
$\langle k(\cdot, x), k(\cdot, y) \rangle_{\mathcal{H}}$, which captures the similarity between the infinite-dimensional features $k(\cdot, x)$ and $k(\cdot, y)$. 

It turns out that the the reproducing kernel both exists and is unique. In fact, we will show that $\mathcal{H}$ being a RKHS is equivalent $\mathcal{H}$ having a reproducing kernel. This result will establish 
the equivalence of the smoothness property (evaluation functionals are continuous) and the existence of a reproducing kernel. We start by showing uniqueness. 
\begin{prop}
If it exists, the reproducing kernel is unique. 
\end{prop}
\begin{proof} 
Suppose $k_1$ and $k_2$ are both reproducing kernels. Then by definition, for any $f \in \mathcal{H}$ and $x \in \mathcal{X}$, 
$\langle f, k_1(\cdot, x)\rangle_{\mathcal{H}} = f(x) = \langle f, k_2(\cdot, x)\rangle_{\mathcal{H}}$. Then, using the bilinearity of the 
inner product,
\[\langle f, k_1(\cdot, x) - k_2(\cdot, x) \rangle_{\mathcal{H}} = 0\]
Since this holds for all $f \in \mathcal{H}$, and the only vector that is orthogonal to every other vector in the space is the zero vector, then we conclude
\[k_1(\cdot, x) - k_2(\cdot, x) = 0\]
or 
\[k_1(\cdot, x) = k_2(\cdot, x)\]

\end{proof}
 Next, we prove existence, which requires an application of the well-known \textit{Riesz representation theorem}. Importantly, the existence proof tells us that a RKHS can equivalently be defined as
 a Hilbert space that possesses a reproducing kernel. In other words, the existence of a reproducing kernel is equivalent to pointwise function evaluation being a continuous operation. 
 \begin{prop}
 $\mathcal{H}$ is a RKHS if and only if it has a reproducing kernel. 
 \end{prop}
 \begin{proof} 
 $(\impliedby)$ Suppose $\mathcal{H}$ is a Hilbert space with a reproducing kernel $k$. We show that this implies the evaluation functional $\delta_x: \mathcal{H} \to \R$ is continuous, for any 
 $x \in \mathcal{X}$. Equivalently, we can show that $\delta_x$ is bounded in operator norm. To this end, fix $x \in \mathcal{X}$ and consider
 \begin{align*}
 \abs{\delta_x(f)} &= \abs{f(x)} \\
 			  &= \langle f, k(\cdot, x) \rangle_{\mathcal{H}} \\
			  &\leq \norm{f}_{\mathcal{H}} \norm{k(\cdot, x)}_{\mathcal{H}} \\
			  &=: \norm{f}_{\mathcal{H}} C_x
 \end{align*}
 where the second line uses the reproducing property. Note that $C_x := \norm{k(\cdot, x)}_{\mathcal{H}}$ only depends on $x$, not $f$. Thus, for any $f \in \mathcal{H}$, 
 \[\frac{ \abs{\delta_x(f)}}{\norm{f}_{\mathcal{H}}} \leq C_x\]
 which implies $\norm{\delta_x} \leq C_x$; i.e. that the evaluation functional $\delta_x$ is bounded, and hence continuous.
 
 $(\implies)$ Now suppose that for any $x \in \mathcal{X}$, the evaluation functional $\delta_x: \mathcal{H} \to \R$ is a bounded, linear operator (i.e. $\delta_x$ is in the topological dual space 
 of $\mathcal{H}$). We must establish the existence of a reproducing kernel; that is, a mapping $k: \mathcal{X} \times \mathcal{X} \to \R$ satisfying $k(\cdot, x) \in \mathcal{H}$ and the 
 reproducing property. Since $\delta_x$ is linear and bounded, then the Riesz representation theorem asserts the existence of a function $f_x \in \mathcal{H}$ satisfying 
 \[\delta_x(f) = \langle f, f_x \rangle_{\mathcal{H}}, \text{ for all } f \in \mathcal{H}\]
 Define the function $k: \mathcal{X} \times \mathcal{X} \to \R$ by $k(x^\prime, x) := f_x(x^\prime)$ for all $x, x^\prime \in \mathcal{H}$. We now check that $k$ satisfies the two properties of a 
 reproducing kernel. First, we have
 \[f_x \in \mathcal{H} \implies k(\cdot, x) \in \mathcal{H}\]
 so the first property is satisfied. The reproducing property follows from 
 \[\langle f, k(\cdot, x)\rangle_{\mathcal{H}} = \langle f, f_x \rangle_{\mathcal{H}} = \delta_x(f) = f(x)\]
 which completes the proof. 
\end{proof}
 
 To review, we defined an RKHS as a special type of Hilbert space in which all of the evaluation functionals are continuous. We showed that this property is equivalent to the space possessing 
 a reproducing kernel. As an alternative exposition, we could have defined a RKHS as a Hilbert space $\mathcal{H}$ of functions with domain $\mathcal{X}$ 
 satisfying the property that for any $x \in \mathcal{X}$ there exists a function $f_x \in \mathcal{H}$ with the property that 
 \[f(x) = \langle f, f_x \rangle_{\mathcal{H}}, \text{ for all } f \in \mathcal{H} \]
 With this definition, we see clearly that RKHS are Hilbert spaces in which point evaluation can be represented as an inner product. The continuity of inner products, thus implies that point evaluation 
 is continuous, thus arriving at the property used in the original definition we provided for RKHS. Given the alternative definition for RKHS here, we could then introduce the notation 
 $k(\cdot, x) := f_x(\cdot)$ to complete the connection with our earlier exposition. 
 

\subsection{Connecting back to Feature Maps}


\section{Applications}
\subsection{Maximum Mean Discrepancy}
Useful: https://stats.stackexchange.com/questions/276497/maximum-mean-discrepancy-distance-distribution




\end{document}


