\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{TA Notes}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction to Stochastic Processes}

\subsection{From Deterministic to Discrete Dynamical Systems}
Introduce difference equations, then add noise to get to AR(1) processes. Talk about what it means to solve a difference equation. Then show how this 
relates to ODEs/continuous time formulation. Pose question: what does it mean to ``solve'' ODEs with random dynamics? 

\subsection{Two Perspectives on Stochastic Processes}
random function vs. collection of random variables; thinking of $t$ as time and $\omega$ as the result of an experiment, or as a particle. 

\subsection{Random Walks}
Talk about the increments, other basic properties 

\section{Introduction to Markov Chains}

\subsection{Finite State Space}
\textbf{TODO}
\begin{align}
\Prob(X_k = j) = \sum_{i = 1}^{I} \Prob(X_k = j|X_{k - 1} = i)\Prob(X_{k - 1} = i)
\end{align}

Define $P(i, j) = \Prob(X_{k+1} = j|X_k = i)$ and $\mu^{(k)}_j = \Prob(X_k = j)$. This can then be re-written as 
\begin{align}
\mu^{(k)}_j = \sum_{i = 1}^{I} P(i, j) \mu^{(k-1)}_i \label{discrete_transition}
\end{align}

\subsection{General State Space}
If the transition dynamics can be described by probability densities $p(x, \cdot)$ for each $x \in \mathcal{X}$ (where $p(x, y)$ is the density of transitioning from $x$ to $y$) then the 
density $\mu_k$ describing the distribution at time $k$ is given by applying the law of total probability:
\begin{align}
\mu_k(y) = \int p(x, y) \mu_{k-1}(x) dx \label{continuous_transition}
\end{align}
This is the continuous analog of \ref{discrete_transition}. Having to write separate formulas for the discrete and continuous cases is annoying and not amenable to developing a general 
theory. Note that there could also be Markov chains with transitions that are not purely discrete nor continuous, but rather a mixture of the two. The concept of a probability measure allows 
us to write down a general expression that encompasses both \ref{discrete_transition} and \ref{continuous_transition}. 
\begin{align}
\mu_k(A) = \mu_{k - 1}P(A) := \int P(x, A)\mu_{k - 1}(dx) \label{general_transition}
\end{align}
The notation $\mu_{k - 1}P$ is very reminiscent of the left-multiplication of a row vector with the transition matrix in the finite state space case. Matrix multiplication (a linear operation) has 
been replaced with integration (another linear operation), but the basic intuition is the same. 

To be absolutely clear here, let's establish the connection between the general expression \ref{general_transition} and the continuous case densities exist. Recalling that integrating densities over 
a set yields the probability of the set, we have 
\[\Prob(X_k \in A) = \mu_k(A) = \int_A p_k(y) dy\]
The same exact logic applies to the transition probability $P(x, A)$:
\[\Prob(X_k \in A|X_{k - 1} = x) = P(x, A) = \int_A p(x, y) dy \]
Putting these together, \ref{general_transition} becomes,
\[\Prob(X_k \in A) = \int \left[ \int_A p(x, y) dy \right] d\mu_{k-1}(x)\]
Finally we note when densities exist, integrating a function $f(x)$ with respect to the measure $\mu_{k-1}$ can be realized by integrating with respect to the density $p_{k-1}$:
\[\int f(x) \mu_{k-1}(dx) = \int f(x) p_{k-1}(x) dx\]
In our current setting, the function we are integrating is $f(x) = \int_A p(x, y) dy$. Thus, plugging this in we obtain 
\[\Prob(X_k \in A) = \int \left[ \int_A p(x, y) dy \right] d\mu_{k-1}(x) = \int \left[ \int_A p(x, y) dy \right] p_{k-1}(x) dx\]
Note that the outer integral is over the entire state space $\mathcal{X}$. Perhaps a more insightful way to write this is 
\[\Prob(X_k \in A)  = \int_A \int_{\mathcal{X}} p(x, y) p_{k-1}(x) dx dy\]


The concept of an invariant (i.e. stationary) distribution also has a natural generalization. A 
distribution $\mu$ is invariant if $\mu P = \mu$; that is, if the distribution is unchanged one time step in the future (and hence unchanged for all future times). By plugging in 
\ref{general_transition}, we see that $\mu P = \mu$ is really shorthand for 
\begin{align*}
\mu(A) = \int P(x, A) \mu(dx) \text{ for all } A \in \mathcal{F}
\end{align*}
Note that the ``for all $A \in \mathcal{F}$'' is essential; for the two distributions $\mu$ and $\mu P$ to be equal they must assign the same probability to every set $A$. 

\subsection{Product Distribution, Detailed Balance, Reversibility}
We will find it useful to talk about the joint distribution between current and subsequent states, $X_{k-1}$ and $X_k$. In other words, we want to answer questions like: 
what is the probability that $X_{k -1}$ is in $A$ and $X_k$ is in $B$? Realizing this event requires 1.) already being in $A$ at the $k^{\text{th}}$ time step and, 2.) transitioning 
from $A$ to $B$ in the next time step. Let's consider the case where densities exist for a moment. In this case, from introductory probability we know that 
the probability that $X_{k-1}$ is in $A$ and $X_k$ is in $B$ can be found by integrating the \textit{joint density} $p_{X_k, X_{k-1}}(x, y)$ over the set $A \times B$. 
\begin{align*}
\Prob(X_{k-1} \in A, X_k \in B) &= \int_{A \times B} p_{X_k, X_{k-1}}(x, y) dx dy =  \int_B \int _A p_{X_k, X_{k-1}}(x, y) dx dy 
\end{align*}
In this case, the joint density factors as 
\[p_{X_k, X_{k-1}}(x, y) = p(x, y)p_{k-1}(x)\]
which makes perfect sense since $p_{k-1}(x)$ captures starting in $A$ and $p(x, y)$ captures the transition from $A$ to $B$. 

\subsubsection{A note on exercise 3.4}
The result that is proved as exercise 3.4,
\begin{align}
(\mu P_1)P_2 = \mu P_1 P_2 \label{associativity}
\end{align}
is not very surprising. In fact, we specifically defined $\mu P$ and $P_1 P_2$ so that this would work. If we defined these notions and found that associativity did not hold, then 
we would definitely want to reconsider those definitions/notation. It is helpful to think about the finite state space case, in which case $\mu$ is a row vector and $P_1$, $P_2$ are 
matrices. The statement \ref{associativity} is identical to the finite state space analog from a notational standpoint, so what we have done is cleverly define things in the general state 
space setting so that the same notation applies here (i.e. we have overloaded the notation). 

\subsection{Questions}
\begin{itemize}
\item Should the probability in example 3.4 be divided by $\Prob(X_{k-1} = x_{k-1})$? Or rather the density $f_{X_{k-1}}(x_{k-1})$? 
\end{itemize}


% Introduction to MCMC
\section{Introduction to Markov Chain Monte Carlo}

\subsection{Sampling from Probability Distributions: Motivation}
\begin{itemize}
\item Why important?
\item Bayesian statistics (e.g. linear regression)
\item The Ising model 
\item Approximating integrals. A bunch of different ways to write expectations: 
\[\pi(\phi) = \E_{X \sim \pi}\left[\phi(X)\right] = \E_{\pi}\left[\phi(X) \right] = \int \phi(x) \pi(dx) \]
\item Emphasis on the fact that the normalizing constant is unknown; intuitively we should not need the normalizing constant for sampling. 
\end{itemize}

\subsection{Simple Monte Carlo}
\begin{itemize}
\item See Youssef's notes 
\item Why not just use deterministic numerical integration? 
\item Apparent immunity to the curse of dimensionality; and where the curse of dimensionality comes in. 
\item Rejection sampling. 
\end{itemize}

\subsection{Markov Chain Monte Carlo}
\subsubsection{The Big Idea}
Discuss the departure from independent sampling, the idea of using a Markov chain, and invariant/stationary distribution. Introduce notation that $h_\pi$ is the density of $\pi$. 

\subsubsection{Metropolis-Hastings}
Metropolis-Hastings (MH) is a class of MCMC algorithms, and is far and away the most common MCMC method. Recall that designing an MCMC algorithm comes down to 
specifying the structure of the Markov chain used in the algorithm. Since homogenous Markov chains are fully defined by an initial condition and a transition kernel $P$, then 
essentially the only design freedom is in the choice of $P$. The class of MH algorithms is thus defined by choosing a specific form for $P$. In particular, the MH transition operator
is composed of two steps. Suppose the current state is $X_k$. The MH Markov chain determines the next state $X_{k + 1}$ by 
\begin{enumerate}
\item \textit{Proposing} a new state $X_{k + 1}^\prime$ by sampling from a specified \textit{proposal distribution} $Q(X_k, \cdot)$; i.e.,  $X_{k + 1} \sim Q(X_k, \cdot)$. 
\item Either accepting or rejecting the proposal (based on a criterion discussed below). The proposal is accepted with probability $\alpha(X_k, X_{k + 1})$, in which case the 
new state is set to the proposed state $X_{k + 1} := X_{k+1}^\prime$. If rejected, the new state is set to the current state $X_{k + 1} := X_k$. 
\end{enumerate}
Recall that we require the target distribution $\pi$ to be a stationary distribution of the Markov chain. Therefore, the above procedure will only be useful if this is true. It may seem like there 
are now two main design choices in MH algorithms: 1.) the proposal distribution $Q$, and 2.) the criterion for accepting or rejecting the samples. However, the accept-reject criterion is chosen 
precisely so that $\pi$ is a stationary distribution, no matter what proposal distribution $Q$ is chosen. Therefore, at a high level the only design freedom in a MH algorithm is in the proposal distribution. 
Therefore, let us first discuss what the accept-reject criterion \textit{has to be}, given the constraint that $\pi$ must be a stationary distribution of the chain. Recall that verifying stationarity directly is 
a difficult task, but we proved that \textit{detailed balance} (i.e. reversibility)--which is conveniently much easier to check--implies stationarity. Therefore, we will use this sufficient condition to derive the 
MH accept-reject criterion.  

Suppose the proposal distribution $Q(x, \cdot)$ has density $q(x, \cdot)$. Recall that $h_\pi$ is the density of the target distribution $\pi$. We seek to determine the acceptance probability $\alpha(X_k, X_{k + 1})$. 

\subsubsection{Gibbs Sampling}

\subsubsection{Roadmap for MCMC Theory}
\begin{enumerate}
\item \textbf{Does the chain have $\pi$ as a stationary distribution?} \\
We have already answered this question above. For MH algorithms, this holds by definition. 

\item \textbf{Does the chain converge to $\pi$? Does it converge for all starting points/initial conditions?} \\
In other words, does the distribution of $X_k$ approach $\pi$ as $k$ grows large? In what sense does it converge? 

\item \textbf{Can we use samples from the chain to estimate expectations?} \\
In other words, does the Monte Carlo estimate $\hat{\pi}(\phi) = \sum_{\ell = 1}^{k} \phi(X_\ell)$ converge to $\pi(\phi) = \int \phi(x) \pi(dx)$? If so, in what sense does it converge? 

\bigskip
\textit{These three questions represent the bare minimum in addressing our goals. The whole motivation for utilizing MCMC was to develop a scalable scheme to sample from a target distribution 
and use those samples to estimate expectations with respect to the target distribution. Thus, if it turned out that suitable convergence results failed to hold, then MCMC wouldn't really be useful 
at all. However, once we have established these basic results, there remain many important unanswered questions. The above three questions make no mention of the rate of convergence. In practice, 
we of course can only run the chain for a finite number of steps, so it is vital to ensure that the chain converges fast enough to be practically useful
They also fail to give a sense of the influence of the starting distribution. In practice, we typically only simulate a Markov chain from a single (or maybe a few) different initial conditions, so this 
question is also of great practical importance. The below items offer more ``quantitative'' viewpoints that seek to address these questions.}

\item \textbf{Does the chain converge at a uniform geometric rate?} \\
This property is known as \textbf{uniform ergodicity} and means there exist constants $M$ and $\rho > 1$ such that 
\[\norm{P^k(x, \cdot) - \pi(\cdot)}_{\text{TV}} \leq M\rho^{-k}\]
This says that each iteration $k$ gets the distribution a factor of $\rho$ closer to the desired distribution $\pi$. This is a very strong notion of convergence, and is very difficult to guarantee. The reason it 
is such a strong notion is due to the fact that $M$ is not a function of the starting location $x$ (hence, ``uniform''); i.e. this says that the same rate of convergence holds no matter where you start. Another thing to note is that 
in practice we typically do not know $M$. Thus, this is more of an asymptotic result in that it establishes a rate of convergence, but doesn't give any sort of finite-time guarantees. In other words, it does 
not answer questions like: after $k$ time steps how far is the chain from the target distribution? 

\item \textbf{Does the chain converge at a (perhaps not uniform) geometric rate?} \\
\textbf{Geometric ergodicity} relaxes the notion of ergodicity above by allowing $M$ to depend on the initial condition $x$. 
\[\norm{P^k(x, \cdot) - \pi(\cdot)}_{\text{TV}} \leq M(x)\rho^{-k}\]
Thus, geometric ergodicity allows the rate of convergence to depend on the starting location. As opposed to uniform ergodicity, geometric ergodicity is a much easier condition to satisfy, and indeed 
a condition that almost all reasonable MCMC schemes should satisfy. 

\item \textbf{Can we compute a finite-time error bound?} \\
The theoretical ideal would be having the ability to answer the question posed earlier: after $k$ time steps how far is the chain from the target distribution? In other words, having a explicit bound on the error 
as a function of $k$. This very powerful notion is referred to as \textbf{quantitative convergence} and can be written as 
\[\norm{P^k(x, \cdot) - \pi(\cdot)}_{\text{TV}} \leq f(x, k)\]
where $f$ is some \textit{known} function of the initial condition $x$ and the time step $k$. 
\end{enumerate}

% Ergodic Theory for Markov Chains
\section{Ergodic Theory for Markov Chains}
We begin by addressing questions 2 and 3 from the MCMC roadmap in the previous section. Recall that these are really the bare minimum any MCMC algorithm should satisfy. We repeat these two questions 
here, as well as a sketch of the answers that will be produced for each question. 

\bigskip
\noindent
\textbf{Question 2:} Does the chain converge to $\pi$? Does it converge for all starting points/initial conditions? \\
\textbf{Answer:} Under some assumptions, we will show that the chain converges in distribution to $\pi$: 
\[\lim_{k \to \infty} P^k(x, A) = \pi(A)\]
Note that $P^{k}(x, A)$ is the distribution of $X_k$, the state of the chain at step $k$. Thus, if we let $X \sim \pi$ denote a random variable distributed according to the target distribution, then we can re-phrase this
statement as 
\[\lim_{k \to \infty} \Prob(X_k \in A|X_0 = x) = \Prob(X \in A)\]
which perhaps makes it more clear that this is the very familiar notion of convergence in distribution. 
As to the question of whether this holds for all starting points, we will show that this statement holds \textit{almost surely}; that is, the set of $x$ for which it holds has probability $1$.

\bigskip
\noindent
\textbf{Question 3:} Can we use samples from the chain to estimate expectations? \\
\textbf{Answer:} Under the same assumptions as required to answer question 1, we will show that 
\[\lim_{k \to \infty} \frac{1}{k} \sum_{\ell = 1}^{k} \phi(X_\ell) = \pi(\phi) \text{ a.s.}\]
That is, the Monte Carlo estimate converges almost surely to the true expectation $\pi(\phi)$. Note that this statement is an analog to the law of large numbers (LLN) in the simple Monte Carlo setting, which required the 
assumption of independence. Here, the $X_k$ are not independent, but under a few additional assumptions we are still able to establish a LLN. 

\subsection{Irreducibility}

\subsubsection{Finite State Space Example}

\subsubsection{Defining Irreducibility in General State Spaces}
Notice that \textit{small set} is defined with respect to a particular distribution $\mu$. For concreteness, think about this being the MCMC target distribution $\pi$. Now, recall  that $C \in \mathcal{F}$ is a small set 
if there exists $\beta \in (0, 1)$ satisfying
\[P(x, A) \geq \beta \pi(A), \text{ for all } x \in C, A \in \mathcal{F}\]
I find it helpful to rearrange this as 
\[\frac{P(x, A)}{\pi(A)} \geq \beta\]
provided that $\pi(A) > 0$ (note that if $\pi(A) = 0$ then the inequality trivially holds). We now see that in particular, for $C$ to be a small set there cannot exist $x \in C$ such that 
$P(x, A) = 0$ and $\pi(A) > 0$, since then the lefthand side would be $0$. This makes a lot of sense. Since $\pi(A) > 0$ then $A$ is ``important'' with respect to the target distribution, in the sense that it has non-zero 
probability. The above definition ensures that that probability of transitioning from any point in the small set to $A$ is strictly positive. From the perspective of MCMC this is good because we want to make sure we 
are sampling from the set $A$. If the current state of the chain is in the small set $C$, then there is a guaranteed path to visit $A$ in the next step (i.e. there is a positive probability of doing so). Note that the definition 
of small set is actually a little stronger than this though; not only does it guarantee a non-zero transition probability, but also provides a lower bound $\beta$ on the probability, relative to $\pi(A)$. This means that if 
$\pi(A)$ is tiny then $P(x, A)$ could potentially be quite small and still satisfy the bound, but if $\pi(A)$ is quite large the bound will force $P(x, A)$ to be larger.

So the small set intuitively provides a path between $C$ and any other set $A$ with $\pi(A) > 0$. However, this is not enough to guarantee that there are no ``islands'' that the chain won't be able to reach. While there 
are no such islands if the chain is already in $C$ what if the chain never actually reaches $C$? The definition of irreducibility removes this possibility, thus excluding cases where such islands can exist. Indeed, recall 
that the definition of irreducibility is that for all $x \in \mathcal{X}$, there exists $k(x) \in \mathbb{N}$ such that 
\[P^{k(x)}(x, C) > 0\]
In words: for any current state $x$, there is a positive probability of reaching the small set $C$ in a finite number of steps $k(x)$. Note that the number of steps required to reach the small set with a certain probability might 
depend on the current state $x$, hence the notation $k(x)$.   

\section{The Main Result}
Recall that in simple Monte Carlo, we estimated the expectation $\pi(\phi)$ by drawing iid samples from $\pi$ and then simply computing the sample mean. 
\begin{align*}
\hat{\pi}_K(\phi) &= \sum_{k = 1}^{K} \phi(X_k), && X_k \overset{iid}{\sim} \pi
\end{align*}
The justification for this estimator is provided by the strong law of large numbers (SLLN), which guarantees 
\[\hat{\pi}_K(\phi) \overset{a.s.}{\to} \pi(\phi) \text{ as } N \to \infty\]
We know that the samples $\{X_k\}$ returned by an MCMC algorithm are not independent, nor identically distributed. However, we hope that the marginal distribution of 
the state $X_k$ is approaching $\pi$ as $k$ grows large. Therefore, it is not unreasonable to wonder if we are justified in using the same estimator $\hat{\pi}_K(\phi)$ as we 
did in the simple Monte Carlo case and still obtain a good estimate for $\pi(\phi)$ so long as the number of samples $K$ is large enough. In other words, we hope that if we 
run the chain large enough then we can sort of treat the samples $X_k$ as if they were independent, and compute the standard sample mean in order to estimate expectations
of interest. In order to justify doing this, we require an analog of the SLLN 
\[\hat{\pi}^N(\phi) \overset{a.s.}{\to} \pi(\phi) \text{ as } N \to \infty\]
that applies when the $X_k$ are not iid; in other words, we need a ``SLLN for Markov chains''. The below theorem states that, under the relatively mild condition or irreducibility, the 
SLLN does indeed hold for Markov chains. 
\begin{thm} 
Suppose that a Markov chain with transition kernel $P$ has invariant distribution $\pi$. Moreover, assume that the Markov chain is irreducible. Then there is a set $S \subset \mathcal{X}$ 
satisfying $\pi(S) = 1$ such that for all $x \in S$ and all $\phi: \mathcal{X} \to \R$ with $\pi(\phi) < \infty$, the Markov chain $\{X_k\}$ with initial distribution $\delta_x$ and transition kernel $P$ 
satisfies
\[\lim_{K \to \infty} \sum_{k = 1}^{K} \phi(X_k) = \pi(\phi) \text{ a.s.} \]
\end{thm}

\bigskip
\noindent
\textbf{Proof Sketch:} The proof of this result is not at all obvious. Here is the big idea: we want to be able to apply the SLLN to $\{\phi(X_k)\}$, but this only applies to iid random variables. Instead, we will 
cleverly define some new random variables $\{Q_i\}$ that \textit{are} iid. We will then define a sample mean sort of estimator $\tilde{\pi}_N(\phi)$ using the $Y_k$ and apply the SLLN to conclude 
\[\tilde{\pi}_N(\phi) \overset{a.s.}{\to} \pi(\phi)\]
Of course, this isn't the convergence result we want since it involves $\tilde{\pi}_N(\phi)$ rather than $\hat{\pi}_N(\phi)$. However, we will be able to show that in the limit the difference between 
these two estimators becomes negligible. This will imply that $\hat{\pi}_N(\phi)$ (the estimator using non-iid samples) also converges to $\pi(\phi)$, as desired. 

Given this overview, the big question is how to define the $Q_i$? Here is where the irreducibility assumption comes into play. We know that any time the chain enters the small set $C$, then the 
distribution of the next transition looks similar to $\pi$; i.e. this is almost as good as getting an independent sample from $\pi$. This means that the chain ``resets'' in a sense. We will then define 
some random variables $T_0, T_1, T_2, \dots$ to keep track of the times when these resets occur; $T_0$ is the time of the first reset, $T_1$ the time of the second reset, and so on. Note that 
since the process is random, then the times $\{T_i\}$ are also random. We will then define $Q_i$ to be the sum of all $\phi(X_k)$ where $T_{i - 1} \leq k \leq T_i - 1$; i.e. the sum of all 
$\phi(X_k)$ starting at the $(i-1)^{\text{st}}$ reset and taking place before the $i^{\text{th}}$ reset. That is, 
\begin{align*}
Q_i := \sum_{k = T_{i-1}}^{T_i - 1} \phi(X_k)
\end{align*}
We will be able to prove that the $\{Q_i\}$ are in fact iid. While the $\{\phi(X_k)\}$ are certainly not iid, the sum of these random variables within each reset are. At this point, we still 
apply the SLLN directly to the $\{Q_i\}$ since the lower and upper bounds on the sum $ \sum_{k = T_{i-1}}^{T_i - 1} \phi(X_k)$ are random variables. In other words, there are a random 
number of terms in this sum! Thus, we will have to derive a similar sum with non-random bounds in order to apply the SLLN. There are many technical details in order to make all these arguments 
rigorous, but it is most important to keep the big picture in mind. 

\begin{proof} 
\noindent
\textbf{Step 1: extended Markov chain construction.} The first step is to define the formal mechanism by which we will keep track of when ``resets'' occur. To do so, we introduce 
indicator random variables $Y_k \in \{0, 1\}$ such that $Y_k = 1$ indicates a reset has occurred at time $k$. Formally, we extend the current Markov chain $\{X_k\}$ to add the $Y_k$ as an 
additional state variable. Thus, the new extended state space is $\mathcal{X} \times \{0, 1\}$ and the extended Markov chain looks like $\{(X_k, Y_k)\}$. We define the initial condition 
$Y_0 = 0$. n order to complete this definition, we 
must define the transition kernel for the extended Markov chain. The transition dynamics we define will rely on the notion of irreducibility and the small set, in order to capture the notion of 
``resetting''. To this end, first recall that the assumption of irreducibility guaranteed the existence of a measure $\mu$ on $\mathcal{X}$, a set $C \in \mathcal{F}$, and 
$\beta \in (0, 1)$ satisfying 
\[P(x, A) \geq \beta \mu(A) \text{ for all } x \in C, A \in \mathcal{F} \]
It is convenient notationally to extend this inequality to all $x \in \mathcal{X}$ (not just $x \in C$) by defining $s(x) := \beta 1(x \in C)$. Then, 
\[P(x, A) \geq s(x)\mu(A) \text{ for all } x \in \mathcal{X}, A \in \mathcal{F} \]
Indeed, if $x \in C$ then this reduces to the previous inequality. On the other hand, if $x \notin C$ then the righthand side is $0$ so the inequality trivially holds. I now define the transition dynamics 
of the extended chain. 
\begin{align*}
\Prob(X_k \in A, Y_k = 1|X_{k-1} = x, Y_{k-1}) &= s(x)\mu(A) \\
\Prob(X_k \in A, Y_k = 0|X_{k-1} = x, Y_{k-1}) &= \tilde{P}(x, A) := P(x, A) - s(x) \mu(A) 
\end{align*} 
We must verify that these transition probabilities are actually valid, but first we make some observations. Notice that that the transition probabilities do not depend on the current 
state of $Y_{k-1}$, but do depend on the current state of $X_{k-1} = x$. We also observe that if this current state $x$ is not in the small set $C$, then the probability of the next state having 
$Y_{k} = 1$ is zero. Thus, if $Y_{k} = 1$ then we know that $X_{k-1} \in C$; i.e. the previous state $X_{k-1}$ was in the small set. So $Y_{k}$ acts as a ``flag'' indicating that 
$X_{k-1}$ was in $C$; but it is a ``probabilistic flag'', in the sense that if $X_{k-1} \in C$ then $Y_k$ is set to $1$ with probability $\beta$. Indeed, 
\[\Prob(Y_k = 1|X_{k-1} \in C, Y_{k-1}) = \Prob(Y_k = 1, X_k \in \mathcal{X}|X_{k-1} \in C, Y_{k-1}) = \beta \mu(\mathcal{X}) = \beta\]
We now verify that the transition probabilities are valid; i.e. that they are bounded in $[0, 1]$ and sum to $1$. Indeed, note that $s(x) \mu(A) \geq 0$ and 
\[s(x) \mu(A) \leq \beta \mu(A) \leq \beta \mu(\mathcal{X}) = \beta < 1\]
so the first transition probability is indeed in $[0, 1]$. For the second transition probability, we have $\tilde{P}(x, A) = P(x, A) \in [0, 1]$ if $x \notin C$. If $x \in C$ then 
\[\tilde{P}(x, A) = P(x, A) - \beta \mu(A) \geq \beta \mu(A) - \beta \mu(A) = 0\]
where the inequality follows from the definition of the small set. We also clearly have $\tilde{P}(x, A) \leq 1$ so this transition probability is valid as well. The transition probabilities sum to one, 
since 
\[\Prob(X_k \in \mathcal{X}, Y_k \in \{0, 1\}) = s(x)\mu(\mathcal{X}) + \tilde{P}(x, \mathcal{X}) = s(x)\mu(\mathcal{X}) + P(x, \mathcal{X}) - s(x)\mu(\mathcal{X}) =  P(x, \mathcal{X}) = 1\]

We have now established that the transition probabilities are valid. To complete this step of the proof, we derive the marginal transition probabilities. First, we consider the marginal for 
$X_k$. 
\begin{align*}
\Prob(X_k \in A|X_{k - 1} = x) &= \Prob(X_k \in A, Y_k \in \{0, 1\}| X_{k - 1} = x) \\
					     &= \Prob(X_k \in A, Y_k = 1| X_{k - 1} = x) + \Prob(X_k \in A, Y_k = 0| X_{k - 1} = x) \\
					     &= s(x)\mu(A) + \left[P(x, A) - s(x)\mu(A) \right] \\
					     &= P(x, A)
\end{align*}
So the marginal $X$ dynamics agree with those of the original (non-extended) chain. Next we consider the marginal transition probabilities for $Y_k$.
\begin{align*}
\Prob(Y_k = 1|X_{k-1}, Y_{k-1}) &= \Prob(X_k \in \mathcal{X}, Y_k = 1|X_{k-1}, Y_{k-1}) = s(x)\mu(\mathcal{X}) = s(x) \\
\Prob(Y_k = 0|X_{k-1}, Y_{k-1}) &= \Prob(X_k \in \mathcal{X}, Y_k = 0|X_{k-1}, Y_{k-1}) = P(x, \mathcal{X}) - s(x)\mu(\mathcal{X}) = 1- s(x) \\
\end{align*}
Now here is the big idea. If $x \in C$ then 
\begin{align*}
\Prob(X_k \in A|X_{k-1} = x) = P(x, A) \geq \beta \mu(A)
\end{align*}
In general this transition probability is $P(x, A)$, which has dependence on the value of previous state $x$. However, the small set assumption gives a lower bound on the transition 
probability $\beta \mu(A)$ which is independent of the previous state! This is what we mean by ``resetting''. The sample $X_k$ can be treated more like an independent sample (from $\mu$). 

\bigskip
\noindent
\textbf{Step 2: characterizing the regeneration times.} In the proof summary, I introduced the random variables $T_0, T_1, T_2, \dots$ that encode the times of the first, second, third, etc. 
reset (i.e. regeneration). We now rigorously define these random variables recursively as 
\begin{align*}
T_0 &:= \min\{k \in \mathbb{N}: Y_k = 1\} \\
 T_i &:= \min\{k \in \mathbb{N}: k > T_{i - 1}, Y_k = 1\}, \ i = 1, 2, 3, \dots
\end{align*}


\end{proof} 




\subsubsection{To include:}
\begin{itemize}
\item The departure from independent sampling. The cost of using correlated samples. 
\item MCMC tool that Youssef shared. 
\end{itemize}

\subsection{Resources}
\begin{itemize}
\item MCMC tool that Youssef shared. 
\end{itemize}









% SDEs
\section{Continuous Time}
\begin{itemize}
\item Motivate as follows: we can move from non-random discrete dynamical systems (e.g. difference equations) to things like Markov Chains by introducing randomness. We can do the 
same to move from continuous-time dynamical systems (e.g. ODEs) to continuous-time stochastic dynamical systems (SDEs) 
\item Include discussion of random vs stochastic DEs (see Stuart)
\end{itemize}

% Brownian Motion 
\subsection{Brownian Motion}

\subsubsection{Introduction and Historical Context}
\begin{itemize}
\item Brown's observation of pollen grain. 
\item Einstein's 1905 paper. 
\item Weiner's mathematical formulation; abstraction of random walk. 
\end{itemize}



A \textit{Brownian motion} $\{B_t\}_{t \in \R_+}$ is a very special type of continuous time stochastic process. Before diving into the definition, recall that in general a stochastic process 
$\{X_t(\omega)\} = \{X(t, \omega)\}$ can be thought of in two ways. 
\begin{enumerate}
\item A collection of random variables, indexed by $t$: at each time $t$, there is a different random variable $X_t: \Omega \to \R$. 
\item A probability distribution over functions: for a fixed $\omega \in \Omega$ we can consider the function $X_\omega: \R_+ \to \R$. In other words, this function of time is 
a specific path (i.e. trajectory) of the process indexed by $\omega$. We might think of this as the path of a particle $\omega$. 
\end{enumerate}

There are many ways to motivate why Brownian motions are important; while the definition may seem opaque at first, the familiarity and utility of these processes the more you come across them. 
One nice way to think about Brownian motions are as the stochastic process analog of Gaussian random variables. One of the primary reasons the Gaussian distribution is so important is due to 
the CLT, which states that properly scaled iid random variables from any distribution converge to a Gaussian. \textbf{Continue to describe this.}




\subsubsection{Resources:}
\begin{itemize}
\item BROWNIAN MOTION AS THE LIMITING DISTRIBUTION OF RANDOM WALKS (Christian Liu)
\item An Introduction to Stochastic Modeling 4th edition 
\item Applied Stochastic Processes in Science and Engineering (M Scott)
\item Applied Stochastic Differential Equations (Sarkka and Solin)
\item Stochastic Differential Equations: An Introduction with Applications. (Oksendal)
\item STOCHASTIC PROCESSES AND APPLICATIONS (Pavliotis)
\end{itemize}


% Resources
\section{Resources}
\begin{itemize}
\item Applied Stochastic Analysis (NYU)
https://cims.nyu.edu/~holmes/teaching/asa2019.html
Really nice notes on Markov Chains (good notes on detailed balance) and stochastic PDEs
\item Stochastic modeling of scientific data (Washington Stat 516)
\item Markov Chain Monte Carlo Lecture Notes (Charles Geyer)
\item Markov Chain Monte Carlo: Theory and Practical Applications (Douc and Corff)
Also some very nice content on geometric ergodicity, total variation norm, etc.
\item Introduction to Stochastic Processes - Lecture Notes (Gordan Zitkovic)
\end{itemize}



\end{document}




