\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{TA Notes}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction to Stochastic Processes}

\subsection{From Deterministic to Discrete Dynamical Systems}
Introduce difference equations, then add noise to get to AR(1) processes. Talk about what it means to solve a difference equation. Then show how this 
relates to ODEs/continuous time formulation. Pose question: what does it mean to ``solve'' ODEs with random dynamics? 

\subsection{Two Perspectives on Stochastic Processes}
random function vs. collection of random variables; thinking of $t$ as time and $\omega$ as the result of an experiment, or as a particle. 

\subsection{Random Walks}
Talk about the increments, other basic properties 

\section{Introduction to Markov Chains}

\subsection{Finite State Space}
\textbf{TODO}
\begin{align}
\Prob(X_k = j) = \sum_{i = 1}^{I} \Prob(X_k = j|X_{k - 1} = i)\Prob(X_{k - 1} = i)
\end{align}

Define $P(i, j) = \Prob(X_{k+1} = j|X_k = i)$ and $\mu^{(k)}_j = \Prob(X_k = j)$. This can then be re-written as 
\begin{align}
\mu^{(k)}_j = \sum_{i = 1}^{I} P(i, j) \mu^{(k-1)}_i \label{discrete_transition}
\end{align}

\subsection{General State Space}
If the transition dynamics can be described by probability densities $p(x, \cdot)$ for each $x \in \mathcal{X}$ (where $p(x, y)$ is the density of transitioning from $x$ to $y$) then the 
density $\mu_k$ describing the distribution at time $k$ is given by applying the law of total probability:
\begin{align}
\mu_k(y) = \int p(x, y) \mu_{k-1}(x) dx \label{continuous_transition}
\end{align}
This is the continuous analog of \ref{discrete_transition}. Having to write separate formulas for the discrete and continuous cases is annoying and not amenable to developing a general 
theory. Note that there could also be Markov chains with transitions that are not purely discrete nor continuous, but rather a mixture of the two. The concept of a probability measure allows 
us to write down a general expression that encompasses both \ref{discrete_transition} and \ref{continuous_transition}. 
\begin{align}
\mu_k(A) = \mu_{k - 1}P(A) := \int P(x, A)\mu_{k - 1}(dx) \label{general_transition}
\end{align}
The notation $\mu_{k - 1}P$ is very reminiscent of the left-multiplication of a row vector with the transition matrix in the finite state space case. Matrix multiplication (a linear operation) has 
been replaced with integration (another linear operation), but the basic intuition is the same. 

To be absolutely clear here, let's establish the connection between the general expression \ref{general_transition} and the continuous case densities exist. Recalling that integrating densities over 
a set yields the probability of the set, we have 
\[\Prob(X_k \in A) = \mu_k(A) = \int_A p_k(y) dy\]
The same exact logic applies to the transition probability $P(x, A)$:
\[\Prob(X_k \in A|X_{k - 1} = x) = P(x, A) = \int_A p(x, y) dy \]
Putting these together, \ref{general_transition} becomes,
\[\Prob(X_k \in A) = \int \left[ \int_A p(x, y) dy \right] d\mu_{k-1}(x)\]
Finally we note when densities exist, integrating a function $f(x)$ with respect to the measure $\mu_{k-1}$ can be realized by integrating with respect to the density $p_{k-1}$:
\[\int f(x) \mu_{k-1}(dx) = \int f(x) p_{k-1}(x) dx\]
In our current setting, the function we are integrating is $f(x) = \int_A p(x, y) dy$. Thus, plugging this in we obtain 
\[\Prob(X_k \in A) = \int \left[ \int_A p(x, y) dy \right] d\mu_{k-1}(x) = \int \left[ \int_A p(x, y) dy \right] p_{k-1}(x) dx\]
Note that the outer integral is over the entire state space $\mathcal{X}$. Perhaps a more insightful way to write this is 
\[\Prob(X_k \in A)  = \int_A \int_{\mathcal{X}} p(x, y) p_{k-1}(x) dx dy\]


The concept of an invariant (i.e. stationary) distribution also has a natural generalization. A 
distribution $\mu$ is invariant if $\mu P = \mu$; that is, if the distribution is unchanged one time step in the future (and hence unchanged for all future times). By plugging in 
\ref{general_transition}, we see that $\mu P = \mu$ is really shorthand for 
\begin{align*}
\mu(A) = \int P(x, A) \mu(dx) \text{ for all } A \in \mathcal{F}
\end{align*}
Note that the ``for all $A \in \mathcal{F}$'' is essential; for the two distributions $\mu$ and $\mu P$ to be equal they must assign the same probability to every set $A$. 

\subsection{Product Distribution, Detailed Balance, Reversibility}
We will find it useful to talk about the joint distribution between current and subsequent states, $X_{k-1}$ and $X_k$. In other words, we want to answer questions like: 
what is the probability that $X_{k -1}$ is in $A$ and $X_k$ is in $B$? Realizing this event requires 1.) already being in $A$ at the $k^{\text{th}}$ time step and, 2.) transitioning 
from $A$ to $B$ in the next time step. Let's consider the case where densities exist for a moment. In this case, from introductory probability we know that 
the probability that $X_{k-1}$ is in $A$ and $X_k$ is in $B$ can be found by integrating the \textit{joint density} $p_{X_k, X_{k-1}}(x, y)$ over the set $A \times B$. 
\begin{align*}
\Prob(X_{k-1} \in A, X_k \in B) &= \int_{A \times B} p_{X_k, X_{k-1}}(x, y) dx dy =  \int_B \int _A p_{X_k, X_{k-1}}(x, y) dx dy 
\end{align*}
In this case, the joint density factors as 
\[p_{X_k, X_{k-1}}(x, y) = p(x, y)p_{k-1}(x)\]
which makes perfect sense since $p_{k-1}(x)$ captures starting in $A$ and $p(x, y)$ captures the transition from $A$ to $B$. 

\subsubsection{A note on exercise 3.4}
The result that is proved as exercise 3.4,
\begin{align}
(\mu P_1)P_2 = \mu P_1 P_2 \label{associativity}
\end{align}
is not very surprising. In fact, we specifically defined $\mu P$ and $P_1 P_2$ so that this would work. If we defined these notions and found that associativity did not hold, then 
we would definitely want to reconsider those definitions/notation. It is helpful to think about the finite state space case, in which case $\mu$ is a row vector and $P_1$, $P_2$ are 
matrices. The statement \ref{associativity} is identical to the finite state space analog from a notational standpoint, so what we have done is cleverly define things in the general state 
space setting so that the same notation applies here (i.e. we have overloaded the notation). 

\subsection{Questions}
\begin{itemize}
\item Should the probability in example 3.4 be divided by $\Prob(X_{k-1} = x_{k-1})$? Or rather the density $f_{X_{k-1}}(x_{k-1})$? 
\end{itemize}


% Introduction to MCMC
\section{Introduction to Markov Chain Monte Carlo}

\subsection{Sampling from Probability Distributions: Motivation}
\begin{itemize}
\item Why important?
\item Bayesian statistics (e.g. linear regression)
\item The Ising model 
\item Approximating integrals. A bunch of different ways to write expectations: 
\[\pi(\phi) = \E_{X \sim \pi}\left[\phi(X)\right] = \E_{\pi}\left[\phi(X) \right] = \int \phi(x) \pi(dx) \]
\item Emphasis on the fact that the normalizing constant is unknown; intuitively we should not need the normalizing constant for sampling. 
\end{itemize}

\subsection{Simple Monte Carlo}
\begin{itemize}
\item See Youssef's notes 
\item Why not just use deterministic numerical integration? 
\item Apparent immunity to the curse of dimensionality; and where the curse of dimensionality comes in. 
\item Rejection sampling. 
\end{itemize}

\subsection{Markov Chain Monte Carlo}
\subsubsection{The Big Idea}
Discuss the departure from independent sampling, the idea of using a Markov chain, and invariant/stationary distribution. Introduce notation that $h_\pi$ is the density of $\pi$. 

\subsubsection{Metropolis-Hastings}
Metropolis-Hastings (MH) is a class of MCMC algorithms, and is far and away the most common MCMC method. Recall that designing an MCMC algorithm comes down to 
specifying the structure of the Markov chain used in the algorithm. Since homogenous Markov chains are fully defined by an initial condition and a transition kernel $P$, then 
essentially the only design freedom is in the choice of $P$. The class of MH algorithms is thus defined by choosing a specific form for $P$. In particular, the MH transition operator
is composed of two steps. Suppose the current state is $X_k$. The MH Markov chain determines the next state $X_{k + 1}$ by 
\begin{enumerate}
\item \textit{Proposing} a new state $X_{k + 1}^\prime$ by sampling from a specified \textit{proposal distribution} $Q(X_k, \cdot)$; i.e.,  $X_{k + 1} \sim Q(X_k, \cdot)$. 
\item Either accepting or rejecting the proposal (based on a criterion discussed below). The proposal is accepted with probability $\alpha(X_k, X_{k + 1})$, in which case the 
new state is set to the proposed state $X_{k + 1} := X_{k+1}^\prime$. If rejected, the new state is set to the current state $X_{k + 1} := X_k$. 
\end{enumerate}
Recall that we require the target distribution $\pi$ to be a stationary distribution of the Markov chain. Therefore, the above procedure will only be useful if this is true. It may seem like there 
are now two main design choices in MH algorithms: 1.) the proposal distribution $Q$, and 2.) the criterion for accepting or rejecting the samples. However, the accept-reject criterion is chosen 
precisely so that $\pi$ is a stationary distribution, no matter what proposal distribution $Q$ is chosen. Therefore, at a high level the only design freedom in a MH algorithm is in the proposal distribution. 
Therefore, let us first discuss what the accept-reject criterion \textit{has to be}, given the constraint that $\pi$ must be a stationary distribution of the chain. Recall that verifying stationarity directly is 
a difficult task, but we proved that \textit{detailed balance} (i.e. reversibility)--which is conveniently much easier to check--implies stationarity. Therefore, we will use this sufficient condition to derive the 
MH accept-reject criterion.  

Suppose the proposal distribution $Q(x, \cdot)$ has density $q(x, \cdot)$. Recall that $h_\pi$ is the density of the target distribution $\pi$. We seek to determine the acceptance probability $\alpha(X_k, X_{k + 1})$. 

\subsubsection{Gibbs Sampling}

\subsubsection{Roadmap for MCMC Theory}
\begin{enumerate}
\item \textbf{Does the chain have $\pi$ as a stationary distribution?} \\
We have already answered this question above. For MH algorithms, this holds by definition. 

\item \textbf{Does the chain converge to $\pi$? Does it converge for all starting points/initial conditions?} \\
In other words, does the distribution of $X_k$ approach $\pi$ as $k$ grows large? In what sense does it converge? 

\item \textbf{Can we use samples from the chain to estimate expectations?} \\
In other words, does the Monte Carlo estimate $\hat{\pi}(\phi) = \sum_{\ell = 1}^{k} \phi(X_\ell)$ converge to $\pi(\phi) = \int \phi(x) \pi(dx)$? If so, in what sense does it converge? 

\bigskip
\textit{These three questions represent the bare minimum in addressing our goals. The whole motivation for utilizing MCMC was to develop a scalable scheme to sample from a target distribution 
and use those samples to estimate expectations with respect to the target distribution. Thus, if it turned out that suitable convergence results failed to hold, then MCMC wouldn't really be useful 
at all. However, once we have established these basic results, there remain many important unanswered questions. The above three questions make no mention of the rate of convergence. In practice, 
we of course can only run the chain for a finite number of steps, so it is vital to ensure that the chain converges fast enough to be practically useful
They also fail to give a sense of the influence of the starting distribution. In practice, we typically only simulate a Markov chain from a single (or maybe a few) different initial conditions, so this 
question is also of great practical importance. The below items offer more ``quantitative'' viewpoints that seek to address these questions.}

\item \textbf{Does the chain converge at a uniform geometric rate?} \\
This property is known as \textbf{uniform ergodicity} and means there exist constants $M$ and $\rho > 1$ such that 
\[\norm{P^k(x, \cdot) - \pi(\cdot)}_{\text{TV}} \leq M\rho^{-k}\]
This says that each iteration $k$ gets the distribution a factor of $\rho$ closer to the desired distribution $\pi$. This is a very strong notion of convergence, and is very difficult to guarantee. The reason it 
is such a strong notion is due to the fact that $M$ is not a function of the starting location $x$ (hence, ``uniform''); i.e. this says that the same rate of convergence holds no matter where you start. Another thing to note is that 
in practice we typically do not know $M$. Thus, this is more of an asymptotic result in that it establishes a rate of convergence, but doesn't give any sort of finite-time guarantees. In other words, it does 
not answer questions like: after $k$ time steps how far is the chain from the target distribution? 

\item \textbf{Does the chain converge at a (perhaps not uniform) geometric rate?} \\
\textbf{Geometric ergodicity} relaxes the notion of ergodicity above by allowing $M$ to depend on the initial condition $x$. 
\[\norm{P^k(x, \cdot) - \pi(\cdot)}_{\text{TV}} \leq M(x)\rho^{-k}\]
Thus, geometric ergodicity allows the rate of convergence to depend on the starting location. As opposed to uniform ergodicity, geometric ergodicity is a much easier condition to satisfy, and indeed 
a condition that almost all reasonable MCMC schemes should satisfy. 

\item \textbf{Can we compute a finite-time error bound?} \\
The theoretical ideal would be having the ability to answer the question posed earlier: after $k$ time steps how far is the chain from the target distribution? In other words, having a explicit bound on the error 
as a function of $k$. This very powerful notion is referred to as \textbf{quantitative convergence} and can be written as 
\[\norm{P^k(x, \cdot) - \pi(\cdot)}_{\text{TV}} \leq f(x, k)\]
where $f$ is some \textit{known} function of the initial condition $x$ and the time step $k$. 
\end{enumerate}

% Ergodic Theory for Markov Chains
\section{Ergodic Theory for Markov Chains}
We begin by addressing questions 2 and 3 from the MCMC roadmap in the previous section. Recall that these are really the bare minimum any MCMC algorithm should satisfy. We repeat these two questions 
here, as well as a sketch of the answers that will be produced for each question. 

\bigskip
\noindent
\textbf{Question 2:} Does the chain converge to $\pi$? Does it converge for all starting points/initial conditions? \\
\textbf{Answer:} Under some assumptions, we will show that the chain converges in distribution to $\pi$: 
\[\lim_{k \to \infty} P^k(x, A) = \pi(A)\]
Note that $P^{k}(x, A)$ is the distribution of $X_k$, the state of the chain at step $k$. Thus, if we let $X \sim \pi$ denote a random variable distributed according to the target distribution, then we can re-phrase this
statement as 
\[\lim_{k \to \infty} \Prob(X_k \in A|X_0 = x) = \Prob(X \in A)\]
which perhaps makes it more clear that this is the very familiar notion of convergence in distribution. 
As to the question of whether this holds for all starting points, we will show that this statement holds \textit{almost surely}; that is, the set of $x$ for which it holds has probability $1$.

\bigskip
\noindent
\textbf{Question 3:} Can we use samples from the chain to estimate expectations? \\
\textbf{Answer:} Under the same assumptions as required to answer question 1, we will show that 
\[\lim_{k \to \infty} \frac{1}{k} \sum_{\ell = 1}^{k} \phi(X_\ell) = \pi(\phi) \text{ a.s.}\]
That is, the Monte Carlo estimate converges almost surely to the true expectation $\pi(\phi)$. Note that this statement is an analog to the law of large numbers (LLN) in the simple Monte Carlo setting, which required the 
assumption of independence. Here, the $X_k$ are not independent, but under a few additional assumptions we are still able to establish a LLN. 

\subsection{Irreducibility}

\subsubsection{Finite State Space Example}

\subsubsection{Defining Irreducibility in General State Spaces}
Notice that \textit{small set} is defined with respect to a particular distribution $\mu$. For concreteness, think about this being the MCMC target distribution $\pi$. Now, recall  that $C \in \mathcal{F}$ is a small set 
if there exists $\beta \in (0, 1)$ satisfying
\[P(x, A) \geq \beta \pi(A), \text{ for all } x \in C, A \in \mathcal{F}\]
I find it helpful to rearrange this as 
\[\frac{P(x, A)}{\pi(A)} \geq \beta\]
provided that $\pi(A) > 0$ (note that if $\pi(A) = 0$ then the inequality trivially holds). We now see that in particular, for $C$ to be a small set there cannot exist $x \in C$ such that 
$P(x, A) = 0$ and $\pi(A) > 0$, since then the lefthand side would be $0$. This makes a lot of sense. Since $\pi(A) > 0$ then $A$ is ``important'' with respect to the target distribution, in the sense that it has non-zero 
probability. The above definition ensures that that probability of transitioning from any point in the small set to $A$ is strictly positive. From the perspective of MCMC this is good because we want to make sure we 
are sampling from the set $A$. If the current state of the chain is in the small set $C$, then there is a guaranteed path to visit $A$ in the next step (i.e. there is a positive probability of doing so). Note that the definition 
of small set is actually a little stronger than this though; not only does it guarantee a non-zero transition probability, but also provides a lower bound $\beta$ on the probability, relative to $\pi(A)$. This means that if 
$\pi(A)$ is tiny then $P(x, A)$ could potentially be quite small and still satisfy the bound, but if $\pi(A)$ is quite large the bound will force $P(x, A)$ to be larger.

So the small set intuitively provides a path between $C$ and any other set $A$ with $\pi(A) > 0$. However, this is not enough to guarantee that there are no ``islands'' that the chain won't be able to reach. While there 
are no such islands if the chain is already in $C$ what if the chain never actually reaches $C$? The definition of irreducibility removes this possibility, thus excluding cases where such islands can exist. Indeed, recall 
that the definition of irreducibility is that for all $x \in \mathcal{X}$, there exists $k(x) \in \mathbb{N}$ such that 
\[P^{k(x)}(x, C) > 0\]
In words: for any current state $x$, there is a positive probability of reaching the small set $C$ in a finite number of steps $k(x)$. Note that the number of steps required to reach the small set with a certain probability might 
depend on the current state $x$, hence the notation $k(x)$.   



\subsubsection{To include:}
\begin{itemize}
\item The departure from independent sampling. The cost of using correlated samples. 
\item MCMC tool that Youssef shared. 
\end{itemize}

\subsection{Resources}
\begin{itemize}
\item MCMC tool that Youssef shared. 
\end{itemize}

% Ergodic theory for Markov Chains 
\section{Ergodic Theory for Markov Chains}


% SDEs
\section{Continuous Time}
\begin{itemize}
\item Motivate as follows: we can move from non-random discrete dynamical systems (e.g. difference equations) to things like Markov Chains by introducing randomness. We can do the 
same to move from continuous-time dynamical systems (e.g. ODEs) to continuous-time stochastic dynamical systems (SDEs) 
\item Include discussion of random vs stochastic DEs (see Stuart)
\end{itemize}

% Brownian Motion 
\subsection{Brownian Motion}

\subsubsection{Introduction and Historical Context}
\begin{itemize}
\item Brown's observation of pollen grain. 
\item Einstein's 1905 paper. 
\item Weiner's mathematical formulation; abstraction of random walk. 
\end{itemize}



A \textit{Brownian motion} $\{B_t\}_{t \in \R_+}$ is a very special type of continuous time stochastic process. Before diving into the definition, recall that in general a stochastic process 
$\{X_t(\omega)\} = \{X(t, \omega)\}$ can be thought of in two ways. 
\begin{enumerate}
\item A collection of random variables, indexed by $t$: at each time $t$, there is a different random variable $X_t: \Omega \to \R$. 
\item A probability distribution over functions: for a fixed $\omega \in \Omega$ we can consider the function $X_\omega: \R_+ \to \R$. In other words, this function of time is 
a specific path (i.e. trajectory) of the process indexed by $\omega$. We might think of this as the path of a particle $\omega$. 
\end{enumerate}

There are many ways to motivate why Brownian motions are important; while the definition may seem opaque at first, the familiarity and utility of these processes the more you come across them. 
One nice way to think about Brownian motions are as the stochastic process analog of Gaussian random variables. One of the primary reasons the Gaussian distribution is so important is due to 
the CLT, which states that properly scaled iid random variables from any distribution converge to a Gaussian. \textbf{Continue to describe this.}




\subsubsection{Resources:}
\begin{itemize}
\item BROWNIAN MOTION AS THE LIMITING DISTRIBUTION OF RANDOM WALKS (Christian Liu)
\item An Introduction to Stochastic Modeling 4th edition 
\item Applied Stochastic Processes in Science and Engineering (M Scott)
\item Applied Stochastic Differential Equations (Sarkka and Solin)
\item Stochastic Differential Equations: An Introduction with Applications. (Oksendal)
\item STOCHASTIC PROCESSES AND APPLICATIONS (Pavliotis)
\end{itemize}


% Resources
\section{Resources}
\begin{itemize}
\item Applied Stochastic Analysis (NYU)
https://cims.nyu.edu/~holmes/teaching/asa2019.html
Really nice notes on Markov Chains (good notes on detailed balance) and stochastic PDEs
\item Stochastic modeling of scientific data (Washington Stat 516)
\item Markov Chain Monte Carlo Lecture Notes (Charles Geyer)
\item Markov Chain Monte Carlo: Theory and Practical Applications (Douc and Corff)
Also some very nice content on geometric ergodicity, total variation norm, etc.
\item Introduction to Stochastic Processes - Lecture Notes (Gordan Zitkovic)
\end{itemize}



\end{document}




