\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{tikz}
\usetikzlibrary{positioning}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Basics of Neural Networks 
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

\section{Introduction}
The neural network era has expanded many of the ideas from the Generalized Linear Models (GLMs) that preceded it. As the amount and complexity and data has continued 
to grow, the desire for more ``expressive'' models has also increased; that is, the ability to model more complex relationships with a broader class of functional forms. GLMs are 
quite flexible and still widely used, but neural networks are able to tackle even more complex problems. In fact, the first major theorem of neural networks is that they are 
``universal function approximators''; informally, given a large enough neural network one can approximate any function arbitrarily well. I hope to offer a brief introduction to neural 
networks here from the perspective of someone coming from a statistical modeling (linear models, GLMs, etc.) perspective in order to remove some of the mystery of this class 
of models. Indeed, we will see that neural networks simply re-package and expand many of the core ideas of GLMs. The resulting class of models is certainly more complex, but the 
core ideas remain largely the same. 

\section{The Basic Setup}
The basic learning-theoretic setup is as follows. We consider an input space $\mathcal{X}$ (features/attributes/predictors) and an output space $\mathcal{Y}$ (responses/labels)
and we seek to learn a mapping $h: \mathcal{X} \to \mathcal{Y}$. In the statistical setting, the motivation for learning this mapping is often to be able to understand the relationship
between these spaces, to gain an understanding of how the explanatory variables are related to the response. As we will see, neural networks are a ``black box'' model in the sense 
that it is very difficult to gain an understanding of the relationships encoded by the model. Thus, the main motivation for neural networks is \textit{prediction}, to predict the value of the 
response given a feature vector. 

\subsection{Hypothesis Class}
Now, to learn a mapping between these spaces we of course need some data. Therefore, our setup assumes that we have access to training data
\[\mathcal{D} = \{(x_i, y_i) \in \mathcal{X} \times \mathcal{Y}: i = 1, \dots, n\}\]
Given this training data, trying to find the ``best'' mapping out of all possible functions is not very helpful. Of course we can simply define a function that correctly predicts every example
in the training data, but this will be overfitting to the training data and likely not generalize well to other datasets. Therefore, we restrict our attention to a set of possible mappings (i.e. models), 
which we call the \textit{hypothesis} class. In linear models, the hypothesis class is just the set of all linear functions of the inputs.
\[h_w(x) = x^T w\]
which simplifies the estimation step in the sense that all we need to optimize is the vector of regression coefficients $w$.
GLMs expand this hypothesis class by introducing non-linearities in the form of a link function $g$. 
\[h_w(x) = g^{-1}(x^T w)\]
Neural networks take this a step further. At the core will still be the notion of linearity; that is, the idea that the linear combination $x^T w$ acts as a summary of the input data. The leap is in the 
idea of letting the model find its own features. In the above examples, we feed the model features $x$, then we summarize them via $x^T w$, and in the case of GLMs potentially allow for 
non-linearity $g^{-1}(x^T w)$. What if we repeated this procedure various times? Ignoring non-linearity for the time being, we might consider something like
\[h(x) = W_2 W_1 x = W_2 (W_1 x)\]
We can think of the first layer $z := W_1 x$ as forming linear combinations of the given feature $x$, thus finding \textit{new} features that may be better than the given ones. Then the 
remaining layer $h(z) = W_2 z$ just looks like a typical linear model as a function of the newly discovered feature vector $z$. Intuitively, if we added more and more layers we would expect 
the model to be able to find complicated features, and thus to be more expressive. On the downside, we clearly have many more parameters to optimize now: every entry of the matrices $W_1$
and $W_2$. Throughout these notes we will denote by $\theta$ the vector of all parameters. So in this case $\theta = (W_1, W_2)$, where we can think of the matrices as being flattened into the 
vector $\theta$. To make the above model more flexible, we can introduce non-linearity. To do so we consider functions $f_1, f_2: \R \to \R$ and consider
\[h_\theta(x) = f_2(W_2 f_1(W_1 x))\]
where $f_1(W_1 x)$ means that $f$ is applied element-wise to each component of the vector $W_1 x$, and likewise for $f_2$. This is typically visualized as directed acyclic graph. Suppose 
$x \in \R^d$, $W_1 \in \R^{k \times d}$, $W_2 \in \R^{1 \times k}$. For simplicity, we are assuming that the function outputs a scalar value, which would be suitable for a binary classifier or a regression
problem. To generalize to multi-class problems we could assume that the model outputs a vector. 


\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=1cm
  },
  neuron missing/.style={
    draw=none, 
    scale=4,
    text height=0.333cm,
    execute at begin node=\color{black}$\vdots$
  },
}

\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]

\foreach \m/\l [count=\y] in {1,2,3,missing,4}
  \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};

\foreach \m [count=\y] in {1,missing,2}
  \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2-\y*1.25) {};

\foreach \m [count=\y] in {1}
  \node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,1.5-\y) {};

\foreach \l [count=\i] in {1,2,3,d}
  \draw [<-] (input-\i) -- ++(-1,0)
    node [above, midway] {$x_\l$};

\foreach \l [count=\i] in {1,k}
  \node [above] at (hidden-\i.north) {$f_1(W_1 x)_\l$};

\foreach \l [count=\i] in {1}
  \draw [->] (output-\i) -- ++(1,0)
    node [above, midway] {$h_\theta(x)$};

\foreach \i in {1,...,4}
  \foreach \j in {1,...,2}
    \draw [->] (input-\i) -- (hidden-\j);

\foreach \i in {1,...,2}
  \foreach \j in {1}
    \draw [->] (hidden-\i) -- (output-\j);

\foreach \l [count=\x from 0] in {Input, Hidden, Ouput}
  \node [align=center, above] at (\x*2,2) {\l \\ layer};

\end{tikzpicture}

\bigskip

Thus, we can think of each entry in $W_1$, $W_2$ as weighting a particular edge in the above graph, while the non-linearities are introduced by applying the function $f$ at the nodes
of the graph. In particular, $(W_\ell)_{ij}$ is the weight on the edge coming from the $j^{\text{th}}$ node of the $\ell^{\text{th}}$ layer and connecting to the $i^{\text{th}}$ node of the 
subsequent layer. 


\subsection{Loss Functions}


\section{Connections between Neural Networks and GLMs}
\textbf{TODO}\\
See paper: From Generalized Linear Models to Neural Networks, and Back


\end{document}






