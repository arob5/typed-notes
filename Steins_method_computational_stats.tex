\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Common math commands. 
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand{\R}{\mathbb{R}}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\Exp}[1]{\exp\mathopen{}\left\{#1\right\}\mathclose{}}
\newcommand{\Log}[1]{\log\mathopen{}\left\{#1\right\}\mathclose{}}
\newcommand{\BigO}{\mathcal{O}}
\newcommand{\indicator}[1]{\mathbb{I}[#1]}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% Linear algebra.
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\oneVec}[1][]{\boldsymbol{1}_{#1}}
\newcommand{\idMat}{\mathbf{I}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\ba}{\mathbf{a}}

% Probability. 
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Gaussian}{\mathcal{N}}
\newcommand{\GaussianDens}{\phi}
\newcommand{\GaussianCDF}{\Phi}

% Stein/Kernels. 
\newcommand{\Ker}{k}
\newcommand{\functionSpace}{\mathcal{F}}
\newcommand{\steinOperator}{\mathcal{A}}
\newcommand{\dist}{P}
\newcommand{\distApprox}{Q}
\newcommand{\steinDisc}{\mathcal{S}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Stein's Method in Computational Statistics}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% Introduction to Stein's Lemma
\section{Introduction to Stein's Lemma}
Here we provide a brief introduction to, and proof of, Stein's lemma. Note that various results due to Stein have been dubbed \textit{Stein's lemma}, and the one we present here is that which provides a 
characterization of the Gaussian distribution. This seemingly simple result has had far-reaching consequences in probability theory and theoretical statistics, and more recently in applied statistics. We begin 
with the univariate case before generalizing to the multivariate result, and then finally introduce the operator theoretic viewpoint which will prove to be very useful. 

% A New Characterization of the Standard Normal Distribution
\subsection{A New Characterization of the Standard Normal Distribution}
Stein's lemma provides a unique way to characterize the distribution $Z \sim \Gaussian(0, 1)$ by considering the behavior of $\Gaussian(0, 1)$ when expectations are taken over 
a certain class of functions. This class of functions is defined by a continuity property, which is given below. 

\begin{definition}
Let $[a, b] \subset \R$. Then $f: [a, b] \to \R$ is said to be \textbf{absolutely continuous} provided that the derivative $f^\prime$ of $f$ exists almost everywhere, is Lebesgue integrable, and satisfies 
\[
\int_{a}^{x} f^\prime(t) dt = f(x) - f(a) \text{ for all } x \in [a, b]
\]
We say that a function defined on the entire real line $f: \R \to \R$ is absolutely continuous if the above definition holds for every compact interval $[a, b]$. 
\end{definition}
In words, a function is absolutely continuous if it allows the fundamental theorem of calculus (FTC) to hold. The typical basic statement of the FTC
(see, e.g. \cite{Abbott}, theorem 7.5.1) assumes that $f^\prime$ is integrable and defined everywhere on $[a, b]$. Thus, the above definition considers a potentially larger class of functions 
for which the theorem holds. In particular, differentiability need only hold almost everywhere on $[a, b]$. We note that a more standard $\epsilon$-$\delta$ definition of absolute continuity is 
typically the first provided, but we favor the above equivalent definition, as it identifies the key condition we require of the function $f$ in Stein's lemma; namely, that it satisfy the FTC. 
We now proceed with the main result. 

\begin{thm} \label{Steins_lemma}
(\textbf{Stein's Lemma}) Let $Z \sim \Gaussian(0, 1)$ and $f: \R \to \R$ be an absolutely continuous function satisfying $\E\abs{f^\prime(Z)} < \infty$. Then 
\begin{align}
\E Z f(Z) = \E f^\prime(Z) \label{characterizing_Gaussian_eqn}
\end{align}

Conversely, suppose \ref{characterizing_Gaussian_eqn} holds for a random variable $W$ for all absolutely continuous and bounded functions $f: \R \to \R$. 
Then $W \sim \Gaussian(0, 1)$. 
\end{thm}

Before proceeding with the proof, we discuss the general interpretation of this result and then state an intermediate result that will be useful in proving the converse stated above. 
While both directions of the lemma have proved very useful in applications, the converse is really what makes this quite interesting. We know that there are many ways to uniquely 
characterize distributions: the distribution function, moment generating function, and characteristic function are three such examples. The above result provides yet another way to 
uniquely characterize the standard normal distribution; namely, $Z \sim \Gaussian(0, 1)$ is uniquely characterized by the relation 
\begin{align}
\E\left[f^\prime(Z) - Z f(Z)\right] = 0, \text{ for all } f \in \functionSpace \label{characterizing_Gaussian_eqn_2}
\end{align}
where $\functionSpace$ is the set of absolutely continuous functions on $\R$. Note that we have simply rearranged the equation \ref{characterizing_Gaussian_eqn}, as this 
form will be more useful going forward. 
We will see later that similar characterizing relations can be established for distributions other than 
Gaussian. 

Now we present a lemma that will be used in the proof of the converse portion of \ref{Steins_lemma}. The final goal is to show that if $\E W f(W) = \E f^\prime(W)$ holds for 
all absolutely continuous and bounded $f$, then $W \sim \Gaussian(0, 1)$. To establish this, we will use a sneaky approach. For fixed $x \in \R$ consider, 
\begin{align}
f_x^\prime(w) - wf_x(w) = \indicator{w \leq x} - \GaussianCDF(x). \label{Gaussian_ODE}
\end{align}
We notice that this is an ordinary differential equation (ODE), and that the lefthand side matches the lefthand of \ref{characterizing_Gaussian_eqn_2}, but without the expectation 
and as a function of the variable $w$. Notice that if the expectation (with respect to $W$) of the righthand side is zero, then 
\[
\Prob(W \leq x) = \E_{w \sim W}[\indicator{w \leq x} ] = \GaussianCDF(x),
\]
implying $W \sim \Gaussian(0, 1)$, which is precisely what we want to show. Moreover, the expectation of the righthand side is zero when the same is true of the 
lefthand side; that is, $\E_{w \sim W}[wf_x(w) - f_x^\prime(w)] = 0$, which is the characterizing relation \ref{characterizing_Gaussian_eqn}. Thus, if we can show that the 
ODE \ref{Gaussian_ODE} has a solution $f_x$ for each $x \in \R$ (and that the $f_x$ are absolutely continuous and bounded) then this will imply that $W \sim \Gaussian(0, 1)$. 
The following lemma shows the existence of such solutions. 

\begin{lemma} \label{Gaussian_ODE_lemma}
The ODE \ref{Gaussian_ODE} has unique, bounded solution 
\begin{align}
f_x(w) &= e^{w^2/2} \int_{w}^{\infty} e^{-t^2/2} \left[\GaussianCDF(x) - \indicator{t \leq x} \right] dt.
\end{align}
\end{lemma}

\begin{proof} 
We notice that \ref{Gaussian_ODE} is a linear, inhomogenous equation
\[\steinOperator f_x = b\]
where $\steinOperator f_x(w) := f_x^\prime(w) - w f_x(w)$ and $b(w) = \indicator{w \leq x} - \GaussianCDF(x)$. 
Thus, the general solution is of the form 
\[f_x = f_x^{\text{part}} + \text{Null}(\steinOperator),\]
where $f_x^{\text{part}}$ is a particular solution of \ref{Gaussian_ODE} and $\text{Null}(\steinOperator)$ the null space of the linear operator $\steinOperator$. 
For the particular solution, we employ the method of integrating factors, multiplying both sides of the equation by $e^{-w^2/2}$,
\[
e^{-w^2/2} f_x^\prime(w) - e^{-w^2/2} wf_x(w) = e^{-w^2/2} b(w)
\]
Observing that the lefthand side takes the form of a derivative product rule, then 
\[
D_{t} \left[f_x(t) e^{-t^2/2} \right] = e^{-t^2/2} b(t),
\] 
where we have also changed variables from $w$ to $t$. We then integrate 
\[
\int_{w}^{\infty} D_{t} \left[f_x(t) e^{-t^2/2} \right] dt = \int_{w}^{\infty} e^{-t^2/2} b(t) dt,
\]
and apply the fundamental theorem of calculus to obtain 
\[
0 - f_x(w) e^{-w^2/2} = \int_{w}^{\infty} e^{-t^2/2} b(t) dt.
\]
Solving for $f_x(w)$ and plugging in the expression for $b(t)$ we obtain the particular solution 
\[
f_x^{\text{part}}(w) = e^{w^2/2} \int_{w}^{\infty} e^{-t^2/2} \left[\GaussianCDF(x) - \indicator{t \leq x}\right] dt.
\]
To find a basis for $\text{Null}(\steinOperator)$ we solve the homogenous equation $D_{w} \left[f_x(w) e^{-w^2/2} \right] = 0$. The derivative being zero implies the 
antiderivative of the lefthand side must be a constant 
\[
f_x(w) e^{-w^2/2} = C,
\]
which means $f_x(w) = C e^{w^2/2}$. Therefore, the general solution is of the form 
\[
f_x(w) = e^{w^2/2} \int_{w}^{\infty} e^{-t^2/2} \left[\GaussianCDF(x) - \indicator{t \leq x}\right] dt + C e^{w^2/2}.
\]
It remains to show that this family of solutions contains a unique bounded solution. See \cite{Ross} lemma 2.2 for a proof that the choice $C = 0$ implies 
a bounded solution with $\norm{f_x}_{\infty} \leq \sqrt{\pi/2}$, while all other values of $C$ imply unbounded solutions. 
\end{proof}

With lemma \ref{Gaussian_ODE_lemma} in hand, we are ready to proceed with the proof of Stein's lemma, theorem \ref{Steins_lemma}. 

\begin{proof} \footnote{In this one-dimensional case, there is also a shorter proof that simply involves the application 
				   of integration by parts to $\E f^\prime(Z)$ and the identity $\GaussianDens^\prime(x) = -x \GaussianDens(x)$, but the slightly
				   longer proof presented here is more amenable to generalization.}

Let $\GaussianDens(x) = \frac{e^{-x^2/2}}{\sqrt{2\pi}}$ denote the standard Gaussian density. From the chain rule, we have the simple identity 
\begin{align}
\GaussianDens^\prime(x) &= -x \frac{e^{-x^2/2}}{\sqrt{2\pi}} = -x \GaussianDens(x) \label{Gaussian_dens_identity}
\end{align}

We have, 
\begin{align*}
\E f^\prime(Z) 
&= \int f^\prime(x) \GaussianDens(x) dx \\
&= \int f^\prime(x) \int_{-\infty}^{x} \GaussianDens^\prime(t) dt dx &&\text{by FTC and } \lim_{x \to -\infty} \GaussianDens(x) = 0 \\
&= -\int \int_{-\infty}^{x} f^\prime(x) t \GaussianDens(t) dt dx 	   &&\text{by \ref{Gaussian_dens_identity}} \\
&= \int \int_{x}^{\infty} f^\prime(x) t \GaussianDens(t) dt dx		   &&\text{flip integration bounds} \\
&= \int_{0}^{\infty} \int_{x}^{\infty} f^\prime(x) t \GaussianDens(t) dt dx + \int_{-\infty}^{0} \int_{x}^{\infty} f^\prime(x) t \GaussianDens(t) dt dx &&\text{split integration domain} \\
&= \int_{0}^{\infty} \int_{x}^{\infty} f^\prime(x) t \GaussianDens(t) dt dx - \int_{-\infty}^{0} \int_{-\infty}^{x} f^\prime(x) t \GaussianDens(t) dt dx &&\text{flip integration bounds} \\
&= \int_{0}^{\infty} \int_{0}^{t} f^\prime(x) t \GaussianDens(t) dx dt - \int_{-\infty}^{0} \int_{t}^{0} f^\prime(x) t \GaussianDens(t) dx dt &&\text{Fubini, reverse integration order} \\
&= \int_{0}^{\infty} t \GaussianDens(t) \left[\int_{0}^{t} f^\prime(x) dx \right] dt - \int_{-\infty}^{0}  t \GaussianDens(t) \left[\int_{t}^{0} f^\prime(x)  dx\right] dt \\
&= \int_{0}^{\infty} t \GaussianDens(t) \left[f(t) - f(0) \right] dt - \int_{-\infty}^{0}  t \GaussianDens(t) \left[f(0) - f(t) \right] dt && \text{absolute continuity} \\
&= \E Z f(Z) - f(0) \E Z &&\text{group terms} \\
&= \E Z f(Z) &&\E Z = 0
\end{align*}

Now, for the converse we know from \ref{Gaussian_ODE_lemma} that the ODE 
\[
f_x^\prime(w) - wf_x(w) = \indicator{w \leq x} - \GaussianCDF(x)
\]
has a unique bounded, absolutely continuous solution $f_x$ for each x. Under the assumption that $\E\left[f^\prime(W) - Wf_x(W)\right] = 0$ for all bounded, absolutely 
continuous functions $f$ this implies the expectation of the lefthand side satisfies $\E_{w \sim W} \left[f_x^\prime(w) - wf_x(w)\right] = 0$. Therefore, for all $x$ the expectation 
of the righthand side is also zero, which implies 
\[\Prob(W \leq x) = \E_{w \sim W}\left[\indicator{w \leq x}\right] = \GaussianCDF(x).\]
Since equality of distribution functions implies equality in distribution, we have $W \sim \Gaussian(0,1)$, which completes the proof. 
\end{proof} 

% Generalizing to other Gaussian Distributions
\subsection{Generalizing to other Gaussian Distributions}
In this section we generalize Stein's lemma to non-standard univariate normal distributions and then to multivariate normal distributions. [\textbf{TODO}]

% The Operator Theoretic Viewpoint 
\subsection{The Operator Theoretic Viewpoint}
Having demonstrated Stein's lemma in various Gaussian settings, we now introduce more generic notation and the framework which will be quite useful going forward. Returning to 
the $\Gaussian(0, 1)$ setting recall the characterizing equation \ref{characterizing_Gaussian_eqn} for the Gaussian distribution, 
\[
\E\left[f^\prime(Z) - Z f(Z) \right] = 0, \text{ for all } f \in \functionSpace
\]
where $\functionSpace$ is the set of all absolutely continuous functions. We note that this can equivalently be written as 
\begin{align}
\E\left[(\steinOperator f)(Z) \right] = 0, \text{ for all } f \in \functionSpace, \label{Stein_characterizing_relation}
\end{align}
where 
\begin{align}
\steinOperator f(x) := f^\prime(x) - xf(x).
\end{align}
Notice that $\steinOperator$ is a \textit{linear operator}; it maps functions to functions. In this case the function $f(x)$ is mapped to the new function 
$f^\prime(x) - xf(x)$. The linearity of $\steinOperator$ follows immediately from linearity of differentiation. In the literature, $\steinOperator$ is referred to as 
a \textit{Stein operator}, $\functionSpace$ the Stein class, and \ref{Stein_characterizing_relation} the \textit{Stein characterizing relation}, respectively. We have 
presented specific instantiations of these objects in the case of Gaussian distributions, but different Stein operator-Stein class pairs can be found to establish 
characterizing relations for many different distributions $\dist$. Perhaps a more useful way to state the Stein characterizing relation for distribution $\dist$
 is
 \begin{align}
 \distApprox = \dist \iff \E_{X \sim \distApprox} \left[(\steinOperator f)(X) \right] = 0 \text{ for all } f \in \functionSpace, \label{Stein_characterizing_relation_2}
 \end{align}
 where $\distApprox$ is any other probability distribution. The relation \ref{Stein_characterizing_relation_2} provides us a binary test for equality in distribution; 
 the power of Stein's method will come from the connection of this idea to a notion of distance between probability distributions. This will allow for a more 
 quantitative description of how ``far apart'' distributions $\dist$ and $\distApprox$ are, which will prove very useful in establishing results such as rates of convergence, 
 as well as providing the basis of practical algorithms in computational statistics. 

 
% Stein Discrepencies
\section{Stein Discrepancies}
In this section, we make the crucial step of connecting Stein's lemma to the notion of \textit{probability metrics}, distance functions that quantify discrepancy between 
probability distributions. 

\subsection{Introduction}
The Stein characterizing relation \ref{Stein_characterizing_relation_2} points towards a seemingly reasonable notion of discrepancy between target distribution $\dist$ and 
any other distribution $\distApprox$. 

\begin{definition}
Let $\dist$ be a probability distribution with Stein operator $\steinOperator$ and Stein class $\functionSpace$. Let $\distApprox$ be another probability distribution. 
The \textbf{Stein discrepancy} between $\dist$ and $\distApprox$ is defined as 
\begin{align}
\steinDisc(\dist, \distApprox) := \sup_{f \in \functionSpace} \norm{\E_{X \sim \distApprox} \left[(\steinOperator f)(X) \right]},
\end{align}
\end{definition}
Here, $\norm{\cdot}$ is a valid norm; if we are considering univariate distributions then this is often just absolute value, but the above definition applies to multivariate distributions 
as well. 
The Stein characterizing equation guarantees that $\steinDisc(\dist, \distApprox) = 0$ if and only if $\dist = \distApprox$, but provides no guarantee that $\steinDisc$ defines a valid 
metric. Nonetheless, we hope that $\steinDisc$ captures salient properties describing the difference between $\dist$ and $\distApprox$. In order to put this notion on firm ground, we 
must establish connections between $\steinDisc$ and valid probability metrics. In particular, we eventually want to show that bounding $\steinDisc$ will imply bounds on other metrics of 
interest. There are many flavors of probability metrics, and it turns out that Stein discrepancies share a close connection with a particular class of metrics, known as integral probability 
metrics. 

\subsection{Integral Probability Metrics}


% Bibliography
\begin{thebibliography}{20}
\bibitem{Ross} Nathan Ross "Fundamentals of Stein’s method," Probability Surveys, Probab. Surveys 8(none), 210-293, (2011)
\bibitem{Gorham} Gorham, Jackson and Lester W. Mackey. “Measuring Sample Quality with Stein's Method.” NIPS (2015).
\bibitem{Gorham2} Gorham, Jackson \& Mackey, Lester. (2017). Measuring Sample Quality with Kernels. 
\bibitem{Chen} Chen, Wilson Ye et al. “Stein Points.” International Conference on Machine Learning (2018).
\bibitem{Barp} Barp, Alessandro \& Briol, Francois-Xavier \& Duncan, Andrew \& Girolami, Mark \& Mackey, Lester. (2019). Minimum Stein Discrepancy Estimators. 
\bibitem{Xu} Xu, Wenkai. “Standardisation-function Kernel Stein Discrepancy: A Unifying View on Kernel Stein Discrepancy Tests for Goodness-of-fit.” International Conference on Artificial Intelligence and Statistics (2021).
\bibitem{Chen2} Chen, Wilson Ye et al. “Stein Point Markov Chain Monte Carlo.” International Conference on Machine Learning (2019).
\bibitem{Huggins} Huggins, Jonathan and Lester W. Mackey. “Random Feature Stein Discrepancies.” Neural Information Processing Systems (2018).
\bibitem{Anastasiou} Anastasiou, Andreas \& Barp, Alessandro \& Briol, François-Xavier \& Ebner, Bruno \& Gaunt, Robert \& Ghaderinezhad, Fatemeh \& Gorham, Jackson \& Gretton, Arthur \& Ley, Christophe \& Liu, Qiang \& Mackey, Lester \& Oates, Chris \& Reinert, Gesine \& Swan, Yvik. (2021). Stein's Method Meets Statistics: A Review of Some Recent Developments. 
\bibitem{Shi} Shi, Jiaxin \& Liu, Chang \& Mackey, Lester. (2021). Sampling with Mirrored Stein Operators. 
\bibitem{Detommaso} Detommaso, Gianluca \& Cui, Tiangang \& Spantini, Alessio \& Marzouk, Youssef \& Scheichl, Robert. (2018). A Stein variational Newton method. 
\bibitem{Liu} Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fit tests. In International conference on machine learning, 276–284. PMLR, 2016.
\bibitem{Liu2} Qiang Liu and Dilin Wang. Stein variational gradient descent: a general purpose bayesian inference algorithm. 2019. arXiv:1608.04471.
\bibitem{blog1} Stein variational gradient descent, Random Walks (Blog post).
\bibitem{Abbott} Abbott Stephen and Springer Science Business Media. 2015. Understanding Analysis. 2nd ed. New York etc: Springer.
\bibitem{LiuBlog} A short introduction to kernelized Stein discrepancy, Qiang Liu (notes). 
\end{thebibliography}

\end{document}


