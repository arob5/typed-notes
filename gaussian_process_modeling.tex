\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}} % For lines in matrix to represent columns
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}} % For lines in matrix to represent rows

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Modeling with Gaussian Processes: Theory and Applications
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Section: Background Theory
\section{Background Theory}

\subsection{The Multivariate Normal} \label{mvn_conditioning}
\begin{prop}
Let $\tilde{Y} \in \R^k$ and $Y \in \R^n$ be random vectors with joint distribution $\begin{pmatrix} \tilde{Y} \\ Y \end{pmatrix} \sim N_{k + n}\left(\mu, \Sigma \right)$
where the mean and covariance matrix can be partitioned as 
\begin{align*}
&\mu = \begin{pmatrix} \tilde{\mu} \\ \mu \end{pmatrix} && \Sigma = \begin{pmatrix} \Sigma_{\tilde{Y}, \tilde{Y}} & \Sigma_{\tilde{Y}, Y} \\ \Sigma_{Y, \tilde{Y}} & \Sigma_{\tilde{Y}, \tilde{Y}}  \end{pmatrix} 
\end{align*}
Then for $a \in \R^n$, $\tilde{Y}|Y = a \sim N_k\left(\tilde{\mu}^\prime, \tilde{\Sigma}^\prime \right)$, where 
\begin{align*}
\tilde{\mu}^\prime &= \tilde{\mu} + \Sigma_{\tilde{Y}, Y} \Sigma_{Y, Y}^{-1}(a - \mu) \\
\tilde{\Sigma}^\prime &= \Sigma_{\tilde{Y}, \tilde{Y}} -  \Sigma_{\tilde{Y}, Y} \Sigma_{Y, Y}^{-1} \Sigma_{Y, \tilde{Y}}
\end{align*}
\end{prop}

The above proposition states that conditioning a multivariate normal distribution results in another multivariate normal distribution. Note that 
the result is quite general in the sense that we can condition on any subset of the components of the random vector and still end up with a 
Gaussian. Now for the proof. 
\begin{proof}
\textbf{TODO}
\end{proof}

\subsection{Stochastic Processes}

\subsection{Gaussian Processes}
\subsubsection{Covariance Functions and Kernels}

\subsection{Important Distributions}
\subsubsection{The Inverse Gamma}

\subsubsection{The Normal-Inverse Gamma}


% Section: Gaussian Process Emulation for Computer Experiments
\section{Gaussian Process Emulation for Computer Experiments}

\subsection{Motivation}
Numerical simulation models are ubiquitous in the physical sciences, and are used to study complex real-world phenomena. We consider 
deterministic simulators that will always produce the same output $y$ given a fixed input $x$. Under this assumption we can therefore model 
a simulator as a mathematical function $f$, with the understanding that this function is typically understood only through computer code; that is, 
$f$ may be incredibly complicated and impossible to write down in an analytical form. The simulator is thus modeled as 
\[y = f(x), \qquad x \in \mathcal{P}\]
where $y \in \R^p$ and $\mathcal{P}$ is the ``parameter space''. In general we just need $\mathcal{P}$ to be a metric space, but we will typically 
consider $\mathcal{P} = \R^d$. Now, in general we don't simply run the simulator on a fixed input $x$ and call it a day. We want to quantify the uncertainty 
of the analysis which includes measuring the sensitivity of the simulation to the choice of parameter $x$. This would entail running the simulator at many 
values in the parameter space to see how $y$ varies as a function of $x$; that is, we hope to gain some understanding of the complicated function $f$ by 
sampling points in $\mathcal{P}$ and feeding them through the function to observe the effect on $y$. The problem is that this can be very computationally costly; 
a single simulation run can potentially take a long time, and to obtain any sort of useful uncertainty results many simulation runs may be required. 

One approach to tackle this problem is to build a statistical model that predicts the outputs of $f$ (and is much computationally cheaper to run). That is, although $f$ 
is a deterministic function, we can model the outputs $y$, conditional on the inputs $x$, as if they were random. This is quite natural through a Bayesian lens; the value 
$f(x)$ is unknown to us until we run a potentially time-consuming simulation. We certainly can't feasibly run this simulation for all $x \in \mathcal{P}$ and therefore many 
of the true values $f(x)$ will remain unknown to us. So we are \textit{uncertain} about the true output values, and the Bayesian treatment of this outputs as random will
allow us to encode our degrees of uncertainty. 

The true power of this statistical modeling approach is that it will allow us to interpolate between known about values. That is, we can run the simulator on some set of 
input points and then observe their true values $f(x)$. We will then constrain the statistical model to match values exactly, and then predict unknown values by interpolating 
between these observed values, while also keeping track of the uncertainty around each prediction. Therefore, we're \textit{constraining} the statistical model using observed
data. 

We wrap up this introductory section with some terminology. The statistical model that predicts the outputs of the simulator is called an \textbf{emulator} or a \textbf{surrogate}
(so the emulator approximates the simulator). The points $x_1, \dots, x_N$ that we run the full simulator on (and then interpolate between) are often called \textbf{design points}. 

\subsection{The Model}
So the question is: how should we attempt to model such a potentially complicated mapping? Well, the first assumption typically made is that the outputs are a smooth function
of the inputs, which is a reasonable assumption in many contexts. Given that we want to interpolate between known values, this assumption is quite essential; if $f(x) = y$ is a known 
true mapping, then for $x^\prime$ close to $x$ we will naturally predict a value $y^\prime$ close to $y$. 

Now, as far as model choice the first idea that may come to mind is a neural network. Neural networks as ``universal function approximators'' and therefore seem to be a natural choice
given that we're trying to approximate a complicated function $f$. Indeed, neural networks are used extensively for this purpose; however, given the subject matter of these notes we will 
focus on a second popular option: Gaussian processes. GPs are also quite flexible so also seem like a solid option for this task. Also, recall from the theory section above that there are 
theoretical guarantees that ensure the realization of a GP is a.s. smooth given some conditions on the covariance function. So we can impose these conditions to encode the smoothness 
condition. 

We will adopt the following model. 
\begin{align*}
&\eta(\cdot) \sim \mathcal{GP}(m_\beta (\cdot), v_{\sigma^2}(\cdot, \cdot)) \\
&m_\beta (x) := \phi(x)^T \beta \\
&v_{\sigma^2}(x, x^\prime) := \sigma^2 c(x, x^\prime)
\end{align*}
where $\phi: \mathcal{P} \to \R^q$, $\beta \in \R^q$, $c(\cdot, \cdot)$ is a valid covariance function, and $\sigma^2 > 0$ is a scale parameter. 
This should look quite familiar to those who have studied linear and generalized linear models (GLMs). The generic GLM framework simply assumes a functional form for the mean function
(the systematic component) and a distributional assumption on the error terms (the random component). This is essentially what the above formulation describes. If $\phi$ is the identity function
then $m_\beta (x) := x^T \beta$ is just the typical linear mean function used in linear models. We incorporate the feature map $\phi$ to allow for transformations of $x$ and thus the ability to 
model more complex functional forms. The error component here is encoded by the GP itself, and in particular $v_{\sigma^2}$. Adjusting the scale $\sigma^2$ will result in more or less 
variability in the process, while specific choices for the covariance function will impose different assumptions on the errors (e.g. homoskedasticity, etc.). 

We will also assume that we have observed the true outputs of the simulator $y := (y_1, \dots, y_N)$ when run using design points $x := (x_1, \dots, x_N)$.

\subsection{Parameter Estimation}
The feature map $\phi$ and covariance function $c(\cdot, \cdot)$ are modeling choices that must be made at the beginning of the analysis. On the other hand, $\beta$ and $\sigma^2$ are parameters
that must be estimated, just as in the typical GLM case. Here we will adopt a fully Bayesian approach to parameter estimation and therefore assume prior distributions on these parameters. The specific 
choice of priors will be discussed later. For now, we consider the following hierarchical model with some prior $\pi_0$ on the parameters. 
\begin{align*}
&\eta(\cdot)|\beta, \sigma^2 \sim \mathcal{GP}(m_\beta (\cdot), v_{\sigma^2}(\cdot, \cdot)) \\
&(\beta, \sigma^2) \sim \pi_0
\end{align*}
Note that we're conditioning a stochastic process $\eta(\cdot)$ on the random vector $(\beta, \sigma^2)$. 


Now, suppose that $\tilde{x} := (\tilde{x}_1, \dots, \tilde{x}_m)$ are a set of points in $\mathcal{P}$ and we seek to approximate the values $\tilde{y}_i = f(\tilde{x}_i)$ using the 
emulator output $\tilde{Y}_i = \eta(\tilde{x}_i)$ (note that $\tilde{Y}_i$ is random since $\eta$ is a stochastic process; for a point estimate of $\tilde{y}_i$, we would consider some function of $\tilde{Y}_i$; 
its mean, etc.). Let $\tilde{Y} := (\tilde{Y}_1, \dots, \tilde{Y}_m)$ and similarly $Y := (\eta(x_1), \dots, \eta(x_m))$, the latter being the random vector of emulator outputs run on the design points. We want 
to consider the \textit{predictive distribution} $\tilde{Y}_1, \dots, \tilde{Y}_m$; that is, of the simulator output at the un-observed parameter values. Thus, we are interested 
in the distribution 
\[p(\tilde{Y}, \beta, \sigma^2|Y = y)\]
We emphasize that we're conditioning on $Y = y$ since we want to use the information about the known function values at the design points and constrain the emulator to agree with the true values 
at these points. In other words, we're ``conditioning the model to observations''. We're also conditioning on $(\beta, \sigma^2)$ as per the hierarchical model specified above. 
When faced with multidimensional posteriors of this form, the typical Bayesian approach is to factor the distribution in a 
useful way using conditional distributions (e.g. Gibbs sampling). 

By the definition of $\tilde{Y}$ and the GP assumption we have 
\[\tilde{Y}|\beta, \sigma^2 \overset{d}{=} \eta(\tilde{x})|\beta, \sigma^2 \sim \mathcal{GP}(m_\beta (\tilde{x}), \sigma^2 c(\tilde{x}, \tilde{x}))\]
If we define 
\[
\tilde{\Phi} := 
\left[
  \begin{array}{ccc}
    \horzbar & \phi(\tilde{x}_1)^T & \horzbar \\
    \horzbar & \phi(\tilde{x}_2)^T & \horzbar \\
             & \vdots    &          \\
    \horzbar & \phi(\tilde{x}_m)^T & \horzbar
  \end{array}
\right] \in \R^{m \times q}
\]

and 
\[\tilde{\Sigma} := \begin{bmatrix} c(\tilde{x}_1, \tilde{x}_1) & \hdots & c(\tilde{x}_1, \tilde{x}_m) \\
                                         \vdots & \hdots & \vdots \\
                                          c(\tilde{x}_m, \tilde{x}_1) & \hdots & c(\tilde{x}_m, \tilde{x}_m) 
                                          \end{bmatrix} \in \R^{m \times m}\]
then we can write
\[\tilde{Y}|\beta, \sigma^2 \sim N_m \left(\tilde{\Phi} \beta, \sigma^2 \tilde{\Sigma} \right)\]
This follows directly from the definition of a GP; namely, the joint distribution of any finite collection of random variables of the stochastic process is multivariate normal. 
We also define $\Phi \in \R^{n \times q}$ and $\Sigma \in \R^{n \times n}$ analogously, but evaluated using the design points $x$ in place of the arbitrary points $\tilde{x}$. 
I also use the notation $c(x, \tilde{x}) \in \R^{n \times m}$ to denote the matrix with $(i, j)$ element $c(x_i, \tilde{x}_j)$, and $c(\tilde{x}, x) = c(x, \tilde{x})^T$. Note that this implies
the equality $\Sigma = c(x, x)$ and $\tilde{\Sigma} = c(\tilde{x}, \tilde{x})$. As a final notational note, I will write $\tilde{Y}|\beta, \sigma^2, y$ as a shorthand for 
$\tilde{Y}|(\beta, \sigma^2, Y = y)$. 
The following proposition
gives the predictive distribution for $\tilde{Y}$, conditional on the data $Y$. 
\begin{prop}
Under the running assumption 
\[\eta(\cdot)|\beta, \sigma^2 \sim \mathcal{GP}(m_\beta (\cdot), v_{\sigma^2}(\cdot, \cdot)) \]
it follows that 
\[\tilde{Y}|\beta, \sigma^2, y \sim N_m \left(\tilde{\mu}^*, \sigma^2 \tilde{\Sigma}^* \right)\] 
where 
\begin{align*}
\tilde{\mu}^* &= \tilde{\Phi}\beta + c(\tilde{x}, x)\Sigma^{-1}(y - \Phi \beta) \in \R^m \\ 
\tilde{\Sigma}^* &= \tilde{\Sigma} - c(\tilde{x}, x) \Sigma^{-1} c(x, \tilde{x}) \in \R^{m \times m}
\end{align*}
\end{prop}

\begin{proof}
From the definition of a GP we can derive the joint distribution of $(\tilde{Y}, Y)$, conditional on the model parameters. 
\[\tilde{Y}, Y|\beta, \sigma^2 \overset{d}{=} \eta\left(\begin{bmatrix} \tilde{x} \\ x \end{bmatrix}\right)|\beta, \sigma^2 \sim 
N_{m + n}\left(\begin{bmatrix} \tilde{\Phi}\beta \\ \Phi \beta \end{bmatrix}, \begin{bmatrix} \tilde{\Sigma} & c(x, \tilde{x}) \\ c(\tilde{x}, x) & \Sigma \end{bmatrix} \right)\]
Now, the conditional distribution of interest follows directly from an application of the Gaussian conditioning proposition (\ref{mvn_conditioning}). The given formulas
for $\tilde{\mu}^*$ and $\tilde{\Sigma}^*$ are a direct application of this result. 
\end{proof}

Now recall that a GP is completely characterized by its set of joint distributions over finite index sets. It therefore seems that we should be able to extend the previous 
result (which derives the distribution over an arbitrary finite index set) to conclude that conditioning the GP on data results in another GP. This is indeed the case, as 
demonstrated in the result below. 
\begin{prop}
Under the running assumption 
\[\eta(\cdot)|\beta, \sigma^2 \sim \mathcal{GP}(m_\beta (\cdot), v_{\sigma^2}(\cdot, \cdot)) \]
it follows that 
\[\eta(\cdot)|\beta, \sigma^2, y \sim \mathcal{GP}\left(m^*_\beta(\cdot), \sigma^2 c^*(\cdot, \cdot )\right)\] 
where for any $x^\prime, x^{\prime \prime} \in \mathcal{P}$, 
\begin{align*}
m^*_\beta(x^\prime) &= \phi(x^\prime)^T \beta + c(x^\prime, x) \Sigma^{-1} (y - \Phi \beta) \in \R \\ 
c^*(x^\prime, x^{\prime \prime}) &= c(x^\prime, x^{\prime \prime}) - c(x^\prime, x) \Sigma^{-1} c(x, x^{\prime \prime}) \in \R
\end{align*}
\end{prop}
Note that $x^\prime, x^{\prime \prime}$ are individual vectors in the parameter space, while $x = (x_1, \dots, x_N)^T$ is a \textit{vector} containing the $N$ design 
points in the parameter space. Thus, $c(x^\prime, x) \in \R^{1 \times N}$ and $c(x^\prime, x^{\prime \prime}) \in \R$. 
\begin{proof}
\textbf{TODO}
\end{proof}

\textbf{TODO: } before going into all of this, add a ``roadmap'' at the beginning that lists two goals: deriving posterior on params, and deriving predictive distribution. 

% Section: Kriging: Gaussian Processes in Geostatistics
\section{Kriging: Gaussian Processes in Geostatistics}


\end{document}




