\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}} % For lines in matrix to represent columns
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}} % For lines in matrix to represent rows

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Modeling with Gaussian Processes: Theory and Applications
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Section: Background Theory
\section{Background Theory}

\subsection{The Multivariate Normal} \label{mvn_conditioning}
\begin{prop}
Let $\tilde{Y} \in \R^k$ and $Y \in \R^n$ be random vectors with joint distribution $\begin{pmatrix} \tilde{Y} \\ Y \end{pmatrix} \sim N_{k + n}\left(\mu, \Sigma \right)$
where the mean and covariance matrix can be partitioned as 
\begin{align*}
&\mu = \begin{pmatrix} \tilde{\mu} \\ \mu \end{pmatrix} && \Sigma = \begin{pmatrix} \Sigma_{\tilde{Y}, \tilde{Y}} & \Sigma_{\tilde{Y}, Y} \\ \Sigma_{Y, \tilde{Y}} & \Sigma_{\tilde{Y}, \tilde{Y}}  \end{pmatrix} 
\end{align*}
Then for $a \in \R^n$, $\tilde{Y}|Y = a \sim N_k\left(\tilde{\mu}^\prime, \tilde{\Sigma}^\prime \right)$, where 
\begin{align*}
\tilde{\mu}^\prime &= \tilde{\mu} + \Sigma_{\tilde{Y}, Y} \Sigma_{Y, Y}^{-1}(a - \mu) \\
\tilde{\Sigma}^\prime &= \Sigma_{\tilde{Y}, \tilde{Y}} -  \Sigma_{\tilde{Y}, Y} \Sigma_{Y, Y}^{-1} \Sigma_{Y, \tilde{Y}}
\end{align*}
\end{prop}

The above proposition states that conditioning a multivariate normal distribution results in another multivariate normal distribution. Note that 
the result is quite general in the sense that we can condition on any subset of the components of the random vector and still end up with a 
Gaussian. Now for the proof. 
\begin{proof}
\textbf{TODO}
\end{proof}

\subsection{Stochastic Processes}

\subsection{Gaussian Processes}
\subsubsection{Covariance Functions and Kernels}

\subsection{Important Distributions}
\subsubsection{The Inverse Gamma}

\subsubsection{The Normal-Inverse Gamma}


% Section: Gaussian Process Emulation for Computer Experiments
\section{Gaussian Process Emulation for Computer Experiments}

\subsection{Motivation}
Numerical simulation models are ubiquitous in the physical sciences, and are used to study complex real-world phenomena. We consider 
deterministic simulators that will always produce the same output $y$ given a fixed input $x$. Under this assumption we can therefore model 
a simulator as a mathematical function $f$, with the understanding that this function is typically understood only through computer code; that is, 
$f$ may be incredibly complicated and impossible to write down in an analytical form. The simulator is thus modeled as 
\[y = f(x), \qquad x \in \mathcal{P}\]
where $y \in \R^p$ and $\mathcal{P}$ is the ``parameter space''. In general we just need $\mathcal{P}$ to be a metric space, but we will typically 
consider $\mathcal{P} = \R^d$. Now, in general we don't simply run the simulator on a fixed input $x$ and call it a day. We want to quantify the uncertainty 
of the analysis which includes measuring the sensitivity of the simulation to the choice of parameter $x$. This would entail running the simulator at many 
values in the parameter space to see how $y$ varies as a function of $x$; that is, we hope to gain some understanding of the complicated function $f$ by 
sampling points in $\mathcal{P}$ and feeding them through the function to observe the effect on $y$. The problem is that this can be very computationally costly; 
a single simulation run can potentially take a long time, and to obtain any sort of useful uncertainty results many simulation runs may be required. 

One approach to tackle this problem is to build a statistical model that predicts the outputs of $f$ (and is much computationally cheaper to run). That is, although $f$ 
is a deterministic function, we can model the outputs $y$, conditional on the inputs $x$, as if they were random. This is quite natural through a Bayesian lens; the value 
$f(x)$ is unknown to us until we run a potentially time-consuming simulation. We certainly can't feasibly run this simulation for all $x \in \mathcal{P}$ and therefore many 
of the true values $f(x)$ will remain unknown to us. So we are \textit{uncertain} about the true output values, and the Bayesian treatment of this outputs as random will
allow us to encode our degrees of uncertainty. 

The true power of this statistical modeling approach is that it will allow us to interpolate between known about values. That is, we can run the simulator on some set of 
input points and then observe their true values $f(x)$. We will then constrain the statistical model to match values exactly, and then predict unknown values by interpolating 
between these observed values, while also keeping track of the uncertainty around each prediction. Therefore, we're \textit{constraining} the statistical model using observed
data. 

We wrap up this introductory section with some terminology. The statistical model that predicts the outputs of the simulator is called an \textbf{emulator} or a \textbf{surrogate}
(so the emulator approximates the simulator). The points $x_1, \dots, x_N$ that we run the full simulator on (and then interpolate between) are often called \textbf{design points}. 

\subsection{The Model}
So the question is: how should we attempt to model such a potentially complicated mapping? Well, the first assumption typically made is that the outputs are a smooth function
of the inputs, which is a reasonable assumption in many contexts. Given that we want to interpolate between known values, this assumption is quite essential; if $f(x) = y$ is a known 
true mapping, then for $x^\prime$ close to $x$ we will naturally predict a value $y^\prime$ close to $y$. 

Now, as far as model choice the first idea that may come to mind is a neural network. Neural networks as ``universal function approximators'' and therefore seem to be a natural choice
given that we're trying to approximate a complicated function $f$. Indeed, neural networks are used extensively for this purpose; however, given the subject matter of these notes we will 
focus on a second popular option: Gaussian processes. GPs are also quite flexible so also seem like a solid option for this task. Also, recall from the theory section above that there are 
theoretical guarantees that ensure the realization of a GP is a.s. smooth given some conditions on the covariance function. So we can impose these conditions to encode the smoothness 
condition. 

We will adopt the following model, in which we will make two simplifying assumptions. First assume that $f$ is a scalar function; that is, $f(x) \in \R$. Next, we will assume homoskedastic 
errors. Both of these assumptions can be lifted, but we consider them now to keep things simpler.
\begin{align*}
&\eta(\cdot) \sim \mathcal{GP}(m_\beta (\cdot), v_{\sigma^2}(\cdot, \cdot)) \\
&m_\beta (x) := \phi(x)^T \beta \\
&v_{\sigma^2}(x, x^\prime) := \sigma^2 c(x, x^\prime)
\end{align*}
where $\phi: \mathcal{P} \to \R^q$, $\beta \in \R^q$, $c(\cdot, \cdot)$ is a valid correlation function, and $\sigma^2 > 0$ is a scale parameter. The assumption that $c(\cdot, \cdot)$ is a correlation function
encodes the homoskedasticity assumption, since multiplying through by $\sigma^2$ then yields the covariance function $v_{\sigma^2}(\cdot, \cdot)$. To generalize to less restrictive covariance structures 
we could write $\mathcal{GP}(m_\beta(\cdot), c(\cdot, \cdot))$; in this case $c(\cdot, \cdot)$ is now a \textit{covariance} (not correlation) function. For now, though, we just consider the homoskedastic case. 
This should look quite familiar to those who have studied linear and generalized linear models (GLMs). The generic GLM framework simply assumes a functional form for the mean function
(the systematic component) and a distributional assumption on the error terms (the random component). This is essentially what the above formulation describes. If $\phi$ is the identity function
then $m_\beta (x) := x^T \beta$ is just the typical linear mean function used in linear models. We incorporate the feature map $\phi$ to allow for transformations of $x$ and thus the ability to 
model more complex functional forms. The error component here is encoded by the GP itself, and in particular $v_{\sigma^2}$. To draw comparisons to linear models we might even write this model as
\[f(x) = m_\beta(x) + e(x) = \phi(x)^T \beta + e(x), \qquad e(\cdot) \sim \mathcal{GP}(0, \sigma^2 c(\cdot, \cdot))\]

An important note here is that one of the 
differentiating characteristics of applying GPs in this computer experiment setting is that the true observations $y$ are \textit{noise-free} since the map $f$ is deterministic. This is different from the 
typical regression context where we assume the underlying process is random. This is not to be confused with the fact that we're using a random process to model $f$. 

The feature map $\phi$ and covariance function $c(\cdot, \cdot)$ are modeling choices that must be made at the beginning of the analysis. On the other hand, $\beta$ and $\sigma^2$ are parameters
that must be estimated, just as in the typical GLM case. Here we will adopt a fully Bayesian approach to parameter estimation and therefore assume prior distributions on these parameters. The specific 
choice of priors will be discussed later. For now, we consider the following hierarchical model with some prior $\pi_0$ on the parameters. 
\begin{align*}
&\eta(\cdot)|\beta, \sigma^2 \sim \mathcal{GP}(m_\beta (\cdot), v_{\sigma^2}(\cdot, \cdot)) \\
&(\beta, \sigma^2) \sim \pi_0
\end{align*}
Note that we're conditioning a stochastic process $\eta(\cdot)$ on the random vector $(\beta, \sigma^2)$. 

We will also assume that we have observed the true outputs of the simulator $y := (y_1, \dots, y_N)$ when run using design points $x := (x_1, \dots, x_N)$.

\subsection{Roadmap}
Before diving into the derivations, let's take a step back and think about what the goals are. The first goal will be parameter estimation; that is, we want to estimate the values of 
the parameters characterizing the GP: $\beta$ and $\sigma^2$. Note that the simulator itself might have a set of parameters that we want to tune by comparing the simulated outputs 
to real-world data; this is a separate problem that we are not considering here. As far as GP parameter estimation, we are interested in the posterior 
\[\pi_Y(\beta, \sigma^2) := p(\beta, \sigma^2|Y) \propto p(Y|\beta, \sigma^2)\pi_0(\beta, \sigma^2) = p(\eta(x)|\beta, \sigma^2)\pi_0(\beta, \sigma^2) \overset{d}{=} N_n(\Phi \beta, \sigma^2 \Sigma)\pi_0(\beta, \sigma^2) \]
The GP model yields a nice Gaussian likelihood, and we leave the prior distribution $\pi_0$ arbitrary. For non-conjugate priors we can 
sample from this posterior using MCMC techniques. 

The second goal is interpolation; that is, we're interested in the distribution of the simulator outputs at the locations where the full simulation was not run $\tilde{x}$. We are therefore interested in 
the predictive distribution: 
\begin{align*}
p(\tilde{Y}|y) &= \int p(\tilde{Y}, \beta, \sigma^2|y) d\beta d\sigma^2 \\
		    &= \int p(\tilde{Y}|\beta, \sigma^2, y)p(\beta, \sigma^2|y) d\beta d\sigma^2 \\
		    &= \int p(\tilde{Y}|\beta, \sigma^2, y) \pi_y(d\beta, d\sigma^2)
\end{align*}
The last step clearly shows that we're integrating with respect to the posterior distribution $\pi_y(\beta, \sigma^2)$. In other words, this is just the expectation of the function
$p(\tilde{Y}|\beta, \sigma^2, y)$ (viewed as a function of $\beta$ and $\sigma^2$) with respect to the posterior distribution. In the next section (proposition \ref{conditional_dist})
we show that $p(\tilde{Y}|\beta, \sigma^2, y)$ is just Gaussian and therefore we can approximate this expectation using MCMC, or possible calculate it analytically depending on the 
choice of $\pi_0$. 

There is a lot more theory that could be developed here. We could choose a particular conjugate prior, and then use this to not only derive a closed form distribution $\tilde{Y}|y$ but even 
characterize the conditional stochastic process that results from conditioning on observed data: $\eta(\cdot)|y$. This would require lots of messy matrix algebra, and is not all that enlightening 
so we exclude it here. With computational techniques like MCMC, we are free to choose a broader range of priors and approximate the posterior and predictive distributions as described above. 

\subsection{Predictive Distribution}

Now, suppose that $\tilde{x} := (\tilde{x}_1, \dots, \tilde{x}_m)$ are a set of points in $\mathcal{P}$ and we seek to approximate the values $\tilde{y}_i = f(\tilde{x}_i)$ using the 
emulator output $\tilde{Y}_i = \eta(\tilde{x}_i)$ (note that $\tilde{Y}_i$ is random since $\eta$ is a stochastic process; for a point estimate of $\tilde{y}_i$, we would consider some function of $\tilde{Y}_i$; 
its mean, etc.). Let $\tilde{Y} := (\tilde{Y}_1, \dots, \tilde{Y}_m)$ and similarly $Y := (\eta(x_1), \dots, \eta(x_m))$, the latter being the random vector of emulator outputs run on the design points. We want 
to consider the \textit{predictive distribution} $\tilde{Y}_1, \dots, \tilde{Y}_m$; that is, of the simulator output at the un-observed parameter values. Thus, we are interested 
in the distribution 
\[p(\tilde{Y}, \beta, \sigma^2|Y = y)\]
We emphasize that we're conditioning on $Y = y$ since we want to use the information about the known function values at the design points and constrain the emulator to agree with the true values 
at these points. In other words, we're ``conditioning the model to observations''. We're also conditioning on $(\beta, \sigma^2)$ as per the hierarchical model specified above. 
When faced with multidimensional posteriors of this form, the typical Bayesian approach is to factor the distribution in a 
useful way using conditional distributions (e.g. Gibbs sampling). 

By the definition of $\tilde{Y}$ and the GP assumption we have 
\[\tilde{Y}|\beta, \sigma^2 \overset{d}{=} \eta(\tilde{x})|\beta, \sigma^2 \sim \mathcal{GP}(m_\beta (\tilde{x}), \sigma^2 c(\tilde{x}, \tilde{x}))\]
If we define 
\[
\tilde{\Phi} := 
\left[
  \begin{array}{ccc}
    \horzbar & \phi(\tilde{x}_1)^T & \horzbar \\
    \horzbar & \phi(\tilde{x}_2)^T & \horzbar \\
             & \vdots    &          \\
    \horzbar & \phi(\tilde{x}_m)^T & \horzbar
  \end{array}
\right] \in \R^{m \times q}
\]

and 
\[\tilde{\Sigma} := \begin{bmatrix} c(\tilde{x}_1, \tilde{x}_1) & \hdots & c(\tilde{x}_1, \tilde{x}_m) \\
                                         \vdots & \hdots & \vdots \\
                                          c(\tilde{x}_m, \tilde{x}_1) & \hdots & c(\tilde{x}_m, \tilde{x}_m) 
                                          \end{bmatrix} \in \R^{m \times m}\]
then we can write
\[\tilde{Y}|\beta, \sigma^2 \sim N_m \left(\tilde{\Phi} \beta, \sigma^2 \tilde{\Sigma} \right)\]
This follows directly from the definition of a GP; namely, the joint distribution of any finite collection of random variables of the stochastic process is multivariate normal. 
We also define $\Phi \in \R^{n \times q}$ and $\Sigma \in \R^{n \times n}$ analogously, but evaluated using the design points $x$ in place of the arbitrary points $\tilde{x}$. 
I also use the notation $c(x, \tilde{x}) \in \R^{n \times m}$ to denote the matrix with $(i, j)$ element $c(x_i, \tilde{x}_j)$, and $c(\tilde{x}, x) = c(x, \tilde{x})^T$. Note that this implies
the equality $\Sigma = c(x, x)$ and $\tilde{\Sigma} = c(\tilde{x}, \tilde{x})$. As a final notational note, I will write $\tilde{Y}|\beta, \sigma^2, y$ as a shorthand for 
$\tilde{Y}|(\beta, \sigma^2, Y = y)$. 
The following proposition
gives the predictive distribution for $\tilde{Y}$, conditional on the data $Y$. 
\begin{prop} \label{conditional_dist}
Under the running assumption 
\[\eta(\cdot)|\beta, \sigma^2 \sim \mathcal{GP}(m_\beta (\cdot), v_{\sigma^2}(\cdot, \cdot)) \]
it follows that 
\[\tilde{Y}|\beta, \sigma^2, y \sim N_m \left(\tilde{\mu}^*, \sigma^2 \tilde{\Sigma}^* \right)\] 
where 
\begin{align*}
\tilde{\mu}^* &= \tilde{\Phi}\beta + c(\tilde{x}, x)\Sigma^{-1}(y - \Phi \beta) \in \R^m \\ 
\tilde{\Sigma}^* &= \tilde{\Sigma} - c(\tilde{x}, x) \Sigma^{-1} c(x, \tilde{x}) \in \R^{m \times m}
\end{align*}
\end{prop}

\begin{proof}
From the definition of a GP we can derive the joint distribution of $(\tilde{Y}, Y)$, conditional on the model parameters. 
\[\tilde{Y}, Y|\beta, \sigma^2 \overset{d}{=} \eta\left(\begin{bmatrix} \tilde{x} \\ x \end{bmatrix}\right)|\beta, \sigma^2 \sim 
N_{m + n}\left(\begin{bmatrix} \tilde{\Phi}\beta \\ \Phi \beta \end{bmatrix}, \begin{bmatrix} \tilde{\Sigma} & c(x, \tilde{x}) \\ c(\tilde{x}, x) & \Sigma \end{bmatrix} \right)\]
Now, the conditional distribution of interest follows directly from an application of the Gaussian conditioning proposition (\ref{mvn_conditioning}). The given formulas
for $\tilde{\mu}^*$ and $\tilde{\Sigma}^*$ are a direct application of this result. 
\end{proof}

Now recall that a GP is completely characterized by its set of joint distributions over finite index sets. It therefore seems that we should be able to extend the previous 
result (which derives the distribution over an arbitrary finite index set) to conclude that conditioning the GP on data results in another GP. This is indeed the case, as 
demonstrated in the result below. 
\begin{prop}
Under the running assumption 
\[\eta(\cdot)|\beta, \sigma^2 \sim \mathcal{GP}(m_\beta (\cdot), v_{\sigma^2}(\cdot, \cdot)) \]
it follows that 
\[\eta(\cdot)|\beta, \sigma^2, y \sim \mathcal{GP}\left(m^*_\beta(\cdot), \sigma^2 c^*(\cdot, \cdot )\right)\] 
where for any $x^\prime, x^{\prime \prime} \in \mathcal{P}$, 
\begin{align*}
m^*_\beta(x^\prime) &= \phi(x^\prime)^T \beta + c(x^\prime, x) \Sigma^{-1} (y - \Phi \beta) \in \R \\ 
c^*(x^\prime, x^{\prime \prime}) &= c(x^\prime, x^{\prime \prime}) - c(x^\prime, x) \Sigma^{-1} c(x, x^{\prime \prime}) \in \R
\end{align*}
\end{prop}
Note that $x^\prime, x^{\prime \prime}$ are individual vectors in the parameter space, while $x = (x_1, \dots, x_N)^T$ is a \textit{vector} containing the $N$ design 
points in the parameter space. Thus, $c(x^\prime, x) \in \R^{1 \times N}$ and $c(x^\prime, x^{\prime \prime}) \in \R$. 
\begin{proof}
\textbf{TODO}
\end{proof}

\subsection{Fer et al emulator workflow}
\begin{enumerate} 
\item Propose $N_\text{knots}$ parameter vectors. \\[.2cm]
Let $f: \mathcal{P} \to \R$ denote the deterministic computer simulator, where $\mathcal{P}$ is the parameter space and I assume for simplicity that $f$ is scalar-valued. 
We propose $x_1, \dots, x_N \in \mathcal{P}$ as specific parameters on which we will run the full simulation. 

\item Run the full model with each parameter vector (parallelizable) \\[.2cm]
Obtain model outputs $y_1, \dots, y_N$ where $y_i := f(x_i)$ is obtained by running the full simulation. 

\item Comparing to real world data/summarizing model error through sufficient statistic. \\[.2cm]
Suppose we obtain real-world data $\{(x_i, z_i)\}_{i = 1}^N$, where the $z_i$ are the outputs of the actual physical process that $f$ simulates. We assume an error model 
for the $z_i$, $z_i|x_i \overset{ind}{\sim} N(f(x_i), \tau)$. Then $T_z(x) := \sum_{i = 1}^{N} (z_i - f(x_i))^2$ is a sufficient statistic for $\tau$.

\item Emulate sufficient statistic \\[.2cm]
Instead of directly emulating $f$ we instead emulate the map $x \mapsto T_z(x)$. We view the sufficient statistic as a function of the parameter $x$, but note that we are conditioning 
on the observed real-world data $z$. The model looks something like 
\begin{align*}
&\eta(\cdot) \sim \mathcal{GP}(m_\beta(\cdot), \sigma^2 c(\cdot, \cdot)) \\
&m_{\beta}(x) = \phi(x)^T \beta \\
&c(x, x^\prime) = \text{corr}\left\{T_z(z), T_z(x^\prime)\right\}
\end{align*}

\item Hierarchical model \\[.2cm]
\begin{align*}
&z_i|x_i \overset{ind}{\sim} N(f(x_i), \tau) \\
&\eta(\cdot)|\beta, \sigma^2 \sim \mathcal{GP}(m_\beta(\cdot), \sigma^2 c(\cdot, \cdot)) \\
&m_{\beta}(x) = \phi(x)^T \beta \\
&c(x, x^\prime) = \text{corr}\left\{T_z(z), T_z(x^\prime)\right\} \\
&\tau \sim \gamma_0 \\
&(\beta, \sigma^2) \sim \pi_0
\end{align*}

\item MCMC \\[.2cm]


\end{enumerate}



% Section: Kriging: Gaussian Processes in Geostatistics
\section{Kriging: Gaussian Processes in Geostatistics}

\section{TODOs}
\begin{itemize}
\item Add section comparing Bayesian linear regression with GP regression (with feature map equal to identity)
\item Compare introducing non-linearity via a feature map vs. a link function 
\item For GP emulation section I am assuming scalar outputs $y$; make this clear, and provide generalization
\item Include Dietze method of emulating likelihood and sufficient statistics, rather than the model outputs themselves
\item Should also mention the goal of making simulator outputs close to observed data
\item Improve the introduction of GP regression by introducing it in a step-wise fashion: first mention Bayesian interpretation of treating $f$ as a random function, then use of GPs to model random functions, then general mean and covariance functions, then hierarchical models for mean and covariance functions (which then introduces the correlation function and scale parameter)
\end{itemize}


\end{document}




