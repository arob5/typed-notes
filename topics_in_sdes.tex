\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand{\B}[1]{\boldsymbol{#1}}
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\be}{\mathbf{e}}

% SDEs
\newcommand{\state}[1][t]{X_{#1}}
\newcommand{\stateValue}[1][t]{x_{#1}}
\newcommand{\BM}[1][t]{B_{#1}} % Brownian Motion
\newcommand{\op}[1][t]{\mathcal{P}_{#1}} % Transition Operator, i.e. one-parameter semi-group
\newcommand{\opId}{\mathcal{I}} % Identity operator. 
\newcommand{\gen}{\mathcal{L}} % Infinitesimal generator
\newcommand{\ito}{\text{It}\hat{\text{o}}}
\newcommand{\dimBM}{d} % Number of Brownian motions
\newcommand{\dimState}{n} 
\newcommand{\diffMat}{\mathbf{A}} % Diffusion matrix 

\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}


\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Topics in Stochastic Differential Equations}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% Generator and Adjoint
\section{Time-Homogenous Diffusions}
We are considering the SDE 
\begin{align}
 d\state = \mu(\state) dt + \sigma(\state) d\BM, \label{time_hom_SDE}
\end{align}
where $\mu(\state)$ and $\sigma(\state)$ are assumed time-independent for now. Under this assumption, the solution $\state$ to the SDE has a transition 
kernel which only depends on the time increment; i.e. it is \textbf{time-homogenous}. By \textbf{diffusion}, we mean a solution to an SDE 
(i.e. \ref{time_hom_SDE}), or more generally a continuous-time Markov process with continuous sample paths. 

In particular, we often want to understand the evolution of $f(\state)$, some function of the SDE solution. The question is: how can we study the properties 
of the random process $f(\state)$? A common approach is to instead consider a \textit{non-random} quantity associated with $f(\state)$, which reduces the 
problem to a (presumably easier) deterministic one. Some examples of non-random quantities might be the expectation, probability density, or transition 
density of $f(\state)$. As a starting point, let us consider analyzing the behavior of expectations such as $\E[f(\state) | \state[0] = \stateValue[0]]$. 
Given that we want to understand the evolution of this quantity, a natural thing to do is to consider its time derivative. Appealing to the definition of the 
derivative, we have 
\begin{align*}
\frac{d}{dt} \E[f(\state) | \state[0] = \stateValue[0]] 
&= \lim_{h \to 0} \frac{\E[f(\state[t+h]) | \state[0] = \stateValue[0]] - \E[f(\state[t]) | \state[0] = \stateValue[0]]}{h} \\
&=  \lim_{h \to 0} \frac{\E[f(\state[h]) | \state[0] = \stateValue[0]] - \E[f(\state[0]) | \state[0] = \stateValue[0]]}{h} \\
&=  \lim_{h \to 0} \frac{\E[f(\state[h]) | \state[0] = \stateValue[0]] - \E[f(\state[0]) | \state[0] = \stateValue[0]]}{h} \\
&=  \lim_{h \to 0} \frac{\E[f(\state[h]) | \state[0] = \stateValue[0]] - f(\stateValue[0])}{h}
\end{align*}
In the second step we have used the time-homogeneity property, which allowed us to just consider $t = 0$. The expression on the right is very important, 
and therefore we introduce a few definitions. First, define the operator $\op$ defined by 
\[
(\op f)(\stateValue[]) := \E[f(\state) | \state[0] = \stateValue[]].
\]

Note that this is a generalization of the finite-state space setting, where $\op$ is a matrix and $f$ a column vector constructed by evaluating $f$ at each state. 

As a specific example, if we take $f(u) = \mathbbm{1}_B(u)$ for some Borel set $B$, then this becomes 
\[
(\op f)(\stateValue[]) = \E[\mathbbm{1}_B(\state) | \state[0] = \stateValue[]] = \Prob(\state \in B | \state[0] = \stateValue[]).
\]

Similarly, choosing $f(u) = \mathbbm{1}(u \leq y)$ yields the transition distribution function for a time-homogenous process:
\[
(\op f)(\stateValue[]) = \E[\mathbbm{1}(\state \leq y) | \state[0] = \stateValue[]] = \Prob(\state \leq y | \state[0] = \stateValue[]).
\]

Let's now plug this new definition for $\op$ back into the derivative expression we had before. Doing so, and letting $\opId$ denote the identity operator, yields 
\begin{align*}
\frac{d}{dt} (\op f)(\stateValue[0]) 
&= \lim_{h \to 0} \frac{(\op[t+h] f)(\stateValue[0]) - (\op[t] f)(\stateValue[0])}{h} \\
&= \lim_{h \to 0} \frac{(\op[t]\op[h] f)(\stateValue[0]) - (\op[t] f)(\stateValue[0])}{h} \\
&= \lim_{h \to 0} \frac{\op[t] \left[(\op[h] f)(\stateValue[0]) - f(\stateValue[0])\right]}{h} \\
&= \op[t] \left(\lim_{h \to 0} \frac{\op[h] - \opId}{h}\right)[f](x_0) \\
\end{align*}

We can thus view the expression in parentheses as another operator that acts on $f$ in order to give the instantaneous time-evolution of the quantity 
$\E[f(\state) | \state[0] = \stateValue[0]]$. This new operator, called the \textbf{infinitesimal generator} and denoted $\gen$, is very important. We have thus defined 
two fundamental operators, $\op$ and $\gen$, which are explored in detail in the following two sections. 

% The Markov Semigroup
\subsection{The Markov Semigroup}
\textbf{TODO} 

% The Generator 
\subsection{The Generator}
The \textbf{infinitesimal generator}, or just \textbf{generator}, of a time-homogenous Markov process is defined as the operator
\begin{align}
\gen = \lim_{h \to 0} \frac{\op[h] - \opId}{h},
\end{align}
which is just the expression we stumbled across in the previous section. In fact, what we showed in that section is 
\begin{align}
\frac{d}{dt} (\op f)(\stateValue[0]) = \frac{d}{dt} \E[f(\state) | \state[0] = \stateValue[0]] = \op \gen f(\stateValue[0]). \label{generator_time_deriv}
\end{align}
Currently, this quantity is quite-abstract; however, we will shortly derive the specific form it takes for the time-homogenous SDE \ref{time_hom_SDE}. Before that, 
we give a bit more motivation about why the generator will be such a key object to study. 

Let's again consider the case where we choose $f(u) = \mathbbm{1}(u \leq y)$ so that $(\op f)(\stateValue[]) = \Prob(\state \leq y | \state[0] = \stateValue[])$ is the 
transition distribution function. Under certain conditions, the transition density $p(t, \stateValue[], y)$ -- the derivative of this distribution function -- will also exist. 
Notice that this form of the transition density relies on the time-homogeneity assumption; it gives the density of transitioning from state $x$ to state $y$ in a time 
interval of length $t$, independent of the actual starting time. In this case, the expression \ref{generator_time_deriv} becomes 
\begin{align*}
\frac{d}{dt} \Prob(\state \leq y | \state[0] = \stateValue[0]) = p(t, \stateValue[0], y) = \op \gen f(\stateValue[0]), 
\end{align*}
so we see that the generator is intimately tied to the transition distribution of the diffusion process. 
This will lead to some powerful results. Peaking ahead, we might 
reasonably call the process \textbf{stationary} if its transition distribution is not changing; i.e. 
\begin{align*}
\frac{d}{dt} \Prob(\state \leq y | \state[0] = \stateValue[]) = (\gen f)(\stateValue[]) = 0,
\end{align*} 
which means that $\Prob(\state \leq y | \state[0] = \stateValue[]) = $ is not a function of $t$. We notice that this reasonable definition can be 
re-stated in terms of the null space of the generator. We have been considering functions of the form $f_y(u) = \mathbbm{1}(u \leq y)$. The above 
equality states that in order for the process to be stationary, such functions must be in the null space of $\gen$. We will return to these ideas when we consider
\textbf{ergodic processes}. 

We now derive the generator of the time-independent SDE. 
\begin{thm} 
The generator $\gen$ for the time-homogenous $\ito$ process 
\[ d\state = \mu(\state) dt + \sigma(\state) d\BM \] 
is given by 
\begin{align}
(\gen f)(x) = \mu(x) f^\prime(x) + \frac{1}{2} \sigma^2(x) f^{\prime\prime}(x) 
\end{align}

\end{thm}

\begin{proof} 
There are multiple ways to arrive at this formula. Here we consider a straightforward application of the $\ito$ formula. Let $f \in \mathcal{C}^2$ have compact support. 
We apply the $\ito$ formula to $f(\state)$ to obtain 
\begin{align*}
df(\state)
&= f^\prime(\state) d\state + \frac{1}{2} f^{\prime\prime}(\state) d[\state, \state](t) \\
&= f^\prime(\state) d\state + \frac{1}{2} f^{\prime\prime}(\state) \sigma^2(\state) dt \\
&= f^\prime(\state) \left[\mu(\state) dt + \sigma(\state) d\BM \right] + \frac{1}{2} f^{\prime\prime}(\state) \sigma^2(\state) dt \\
&= \left[f^\prime(\state)\mu(\state) +  \frac{1}{2} f^{\prime\prime}(\state) \sigma^2(\state) \right] dt + f^\prime(\state) \sigma(\state) d\BM
\end{align*}

Now, we notice that the expression in brackets is precisely what we are claiming to be the generator $(\gen f)(\state)$. Plugging this notation in, we have shown
\begin{align}
df(\state) = (\gen f)(\state) dt +  f^\prime(\state) \sigma(\state) d\BM. \label{Ito_formula_generator}
\end{align}

Converting this differential to its integral representation (i.e. integrating from $0$ to $t$), we obtain 
\[
f(\state) - f(\stateValue[]) = \int_{0}^{t} (\gen f)(\state[s]) ds + \int_{0}^{t} f^\prime(\state[s]) \sigma(\state[s]) d\BM[s]
\]
We now consider taking the expectation, conditional on $\state[0] = \stateValue[]$, of both sides. This yields 
\[
\E[f(\state) | \state[0] = \stateValue[]] - f(\stateValue[]) = \int_{0}^{t} \E[(\gen f)(\state[s]) | \state[0] = \stateValue[]] ds + \E\left[\int_{0}^{t} f^\prime(\state[s]) \sigma(\state[s]) d\BM[s] \bigg| \state[0] = \stateValue[] \right] 
\]
We have applied Fubini's theorem for the first term on the righthand side. We make a few observations. First, note that the lefthand side is precisely $(\op f)(\stateValue[]) - f(\stateValue[])$. The second term on the righthand side 
is an $\ito$ integral. Using the assumption that $f$, and hence $f^\prime$, has compact support, then we conclude the expectation of this $\ito$ integral is zero. Applying these two observations, we now have
\[
(\op f)(\stateValue[]) - f(\stateValue[])  = \int_{0}^{t} \E[(\gen f)(\state[s]) | \state[0] = \stateValue[]] ds.
\] 
Now, we can recover the definition of the generator on the lefthand side by dividing through by $t$ and letting $t \to 0$. Doing so gives, 
\begin{align*}
\lim_{t \to 0} \frac{(\op f)(\stateValue[]) - f(\stateValue[]) }{t} 
&= \lim_{t \to 0} \frac{1}{t} \int_{0}^{t} \E[(\gen f)(\state[s]) | \state[0] = \stateValue[]] ds \\
&= \frac{d}{dt}\bigg|_{t=0} \int_{0}^{t} \E[(\gen f)(\state[s]) | \state[0] = \stateValue[]] ds \\
&= \E[(\gen f)(\state[0]) | \state[0] = \stateValue[]] \\
&= (\gen f)(\stateValue[]), 
\end{align*}
where the penultimate line uses the fundamental theorem of calculus. The result is proven. 
\end{proof}

Note that \ref{Ito_formula_generator} also gives a nice, compact way to write the $\ito$ formula in terms of the generator; in integral form, 
\begin{align}
f(\state) =  f(\state[0]) + \int_{0}^{t} (\gen f)(\state[s]) ds + \int_{0}^{t} f^\prime(\state[s]) \sigma(\state[s]) d\BM[s].
\end{align}

The way to think about this ties back to our motivation for the definition of the generator. What the above result gives us is a PDE describing 
the evolution of statistics of the SDE. 
\begin{align}
\frac{d}{dt} \E[f(\state) | \state[0] = \stateValue[0]] = \mu(x) f^\prime(x) + \frac{1}{2} \sigma^2(x) f^{\prime\prime}(x),
\end{align}
noting that the righthand side is $(\gen f)(\stateValue[0])$, as just derived above. The time-derivative of the SDE statistic on the left is 
related to the state derivatives on the right. As a specific example, choosing $f$ to be the identity would yields a PDE describing the 
evolution of the mean of $\state$. 

% The Fokker-Planck Equation
\subsection{The Fokker-Planck Equation}
The \textbf{Fokker-Planck equation} equation, also known as the \textbf{Kolmogorov forward equation}, is a partial differential equation (PDE) describing the evolution of the 
probability density of the solution of the SDE. Recall that we are considering time-homogenous SDEs, so the density under consideration here as the form 
$p(t, \stateValue[0], \stateValue[])$: the density of starting at state $\stateValue[0]$ and ending up at state $\stateValue[]$ after time $t$. The equation we will derive is known 
as the \textit{forward} equation as the PDE will describe the evolution of this density in the \textit{forward variables} $t$ and $\stateValue[]$, as opposed to considering derivatives
with respect to the starting location $\stateValue[0]$. 

There are many ways to derive this equation. The derivation below simply applies the $\ito$ formula and then uses integration by parts, sweeping some of the theoretical 
justifications under the rug. Later in this section we re-cast this derivation using more generic tools from operator theory. 

\begin{thm}
(The Fokker-Planck/Forward Kolmogorov Equation) The transition density $p(t, \stateValue[0], \stateValue)$ associated with the time-homogenous SDE
\ref{time_hom_SDE} satisfies the PDE
\begin{align}
\frac{\partial p}{\partial t} = -\frac{\partial}{\partial x} [p(t, \stateValue[0], \stateValue[])\mu(\stateValue[])] + \frac{1}{2} \frac{\partial^2}{\partial x^2} [p(t, \stateValue[0], \stateValue[]) \sigma^2(\stateValue[])].
\end{align}
\end{thm}

\begin{proof}
Let $f(\stateValue[], t)$ be a twice continuously-differentiable function which vanishes outside of the time interval $(0, T)$. As usual, we start by applying the 
$\ito$ formula,
\begin{align*}
df(\stateValue[], t) 
&= \frac{\partial f}{\partial x} d\state + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} \sigma^2(\state) dt + \frac{\partial f}{\partial t} dt \\
&= \left[\frac{\partial f}{\partial t} + \frac{\partial f}{\partial x}\mu(\state) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\sigma^2(\state) \right]dt + \frac{\partial f}{\partial x}\sigma(\state) d\BM,
\end{align*}
where in the second step we plugged in the differential $d\state$ and grouped terms. Integrating from $0$ to $T$, 
\begin{align*}
f(\state[T], T) - f(\state[0], 0) 
&= \int_{0}^{T} \left[\frac{\partial f}{\partial t} + \frac{\partial f}{\partial x}\mu(\state) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\sigma^2(\state) \right]dt 
	+ \int_{0}^{T} \frac{\partial f}{\partial x}\sigma(\state) d\BM
\end{align*}
and now taking expectation conditional on $\state[0] = \stateValue[0]$,
\begin{align*}
\E[f(\state[T], T) | \state[0] = \stateValue[0]] - f(\stateValue[0], 0)
&= \int_{0}^{T} \E\left[\frac{\partial f}{\partial t} + \frac{\partial f}{\partial x}\mu(\state) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\sigma^2(\state) \bigg| \state[0] = \stateValue[0] \right]dt.
\end{align*}
On the righthand side, we have taken two steps. We reversed the order of integration in the first term, which can be justified using Fubini's theorem.
We also applied the fact that the expectation of the second term is zero, using the facts that it is an $\ito$ integral and that $f$ was assumed to have compact support. 
We now re-write the expectation in the remaining term using the transition density, 
\begin{align*}
\E[f(\state[T], T) | \state[0] = \stateValue[0]] - f(\stateValue[0], 0)
&= \int_{0}^{T} \int_{-\infty}^{\infty} \left[\frac{\partial f}{\partial t} + \frac{\partial f}{\partial x}\mu(\state) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\sigma^2(\state)\right] p(t,\stateValue[0],\stateValue[]) dx dt
\end{align*}
At this point we will apply linearity to write this as the sum of three integrals, and consider each in turn. To lighten notation, we drop the function arguments, writing, for example, 
$p = p(t, \stateValue[0], \stateValue)$. We thus have, 
\begin{align*}
\E[f(\state[T], T) | \state[0] = \stateValue[0]] - f(\stateValue[0], 0)
&= \int_{0}^{T} \int_{-\infty}^{\infty} p\frac{\partial f}{\partial t} dxdt + \int_{0}^{T} \int_{-\infty}^{\infty} p\mu \frac{\partial f}{\partial x} dxdt + \frac{1}{2} \int_{0}^{T} \int_{-\infty}^{\infty} p\sigma^2 \frac{\partial^2 f}{\partial x^2} dxdt
\end{align*}
We now deal with each of this integrals in turn. Our goal is to eliminate derivatives the of $f$, instead picking up derivatives of $p$. 

\bigskip
\noindent
\textbf{First Integral:} The strategy for this term is to flip the order of integration (by Fubini) and then integrate by parts with respect to $t$. 
Integrating by parts with $u(t) = p(t, \stateValue[0], \stateValue)$ and $v^\prime(t) = \frac{\partial f}{\partial t}$ yields 
\begin{align*}
t
\end{align*}

\end{proof}

We have phrased the Fokker-Planck equation as describing the evolution of the transition density, but note that this is essentially the same thing as 
describing the evolution of the density of $\state$ itself. The density $p_t(\stateValue[])$ of $\state$ will always depend on the initial condition. If the initial condition is 
deterministic, say $\stateValue[0]$, then we simply have $p_t(\stateValue[]) = p(t, \stateValue[0], \stateValue[])$. Essentially, what we showed above can be re-phrased
as the initial value problem
\begin{align}
&\frac{\partial p}{\partial t} = -\frac{\partial}{\partial x} [p(t, \stateValue[0], \stateValue[])\mu(\stateValue[])] + \frac{1}{2} \frac{\partial^2}{\partial x^2} [p(t, \stateValue[0], \stateValue[]) \sigma^2(\stateValue[])] \\
&p(0, \stateValue[0], \stateValue[]) = \delta_{\stateValue[0]}(\stateValue[]) \nonumber
\end{align}

% SDEs in Multiple Dimensions
\section{SDEs in Multiple Dimensions}

\subsection{$\ito$ processes}
There are two ways we can consider adding more dimensions here: 1.) a scalar SDE being driven by multiple Brownian motions, and 
2.) a system of SDEs (with each equation potentially being driven by multiple Brownian motions). Most of the results discussed above have 
natural analogs to these settings; the notation just becomes more burdensome. Beginning with the first case, suppose we have 
$\dimBM$ independent Brownian motions driving the system. We will write these in the vector 
\[
\B\BM := \begin{pmatrix} \BM[1](t) & \dots & \BM[\dimBM](t) \end{pmatrix}^\top,
\]
noting that we will be switching between the functional and subscript notation for dependence on time. This will prevent double-subscripts. 
It is not difficult to  show that the quadratic variation between independent Brownian motions is zero, 
\[
d\BM[i](t) d\BM[j](t) = t \delta_{ij}. 
\]
Given a vector-valued function $G: \R \to \R^{\dimBM}$, we can then define the SDE
\begin{align*}
d\state &= \mu(\state) dt + \sum_{j=1}^{\dimBM} G_j(\state) d\BM[j](t), 
\end{align*}
which is to be interpreted as 
\begin{align*}
\state &= \state[0] + \int_{0}^{t} \mu(\state[s]) ds + \sum_{j=1}^{\dimBM} \int_{0}^{t} G_j(\state[s]) d\BM[j](s).
\end{align*}
To lighten notation, we write 
\begin{align*}
d\state &= \mu(\state) dt + \langle G(\state), d\B\BM \rangle.
\end{align*}
Dot product notation in place of the inner product notation is also common. We next generalize to the situation where there are $\dimState$ 
SDEs, which we write in the vector
\[
\B\state := \begin{pmatrix} \state[1](t) & \dots & \state[\dimState](t) \end{pmatrix}^\top.
\]
Each of these state variables may be driven by all of the Brownian motions, yielding a coupled system of $\dimState$ equations
driven by $\dimBM$ Brownian motions. For functions $\mu(\cdot, t): \R^{\dimState} \to \R^{\dimState}$ and $\sigma(\cdot, t): \R^{\dimState} \to \R^{\dimState \times \dimBM}$ this
 system can be written component-wise as,
\begin{align*}
d\state[i](t) &= \mu_i(\state,t) dt + \sum_{j=1}^{\dimBM} \sigma_{ij}(\state,t) d\BM[j](t), 
\end{align*}
and the entire system can be represented succinctly as
\begin{align}
d\B\state &= \B\mu(\state,t) dt + \B\sigma(\state,t) d\B\BM, \label{SDE_multidim}
\end{align}
noting that $\B\sigma$ is a matrix-valued function. One can derive the covariation 
\begin{align*}
&d\state[i](t) d\state[j](t) = \diffMat_{ij} dt, &&\diffMat = \B\sigma \B\sigma^\top, 
\end{align*}
where $\diffMat$ is known as the \textbf{diffusion matrix}. This fact can be written in matrix form as 
\begin{align*}
d\B\state d\B\state^\top = \B\sigma\B\sigma^\top dt = \diffMat dt.
\end{align*}

\subsection{$\ito$'s Formula}
We now provide the generalization of the $\ito$ formula for scalar-valued functions $f: \R^{\dimState} \to \R$. The formula is given in differential form as
\begin{align*}
df(\B\state) &= \sum_{i=1}^{\dimState} \frac{\partial f(\B\state)}{\partial x_i} d\state[i](t) + \frac{1}{2} \sum_{i=1}^{\dimState}\sum_{j=1}^{\dimState} \frac{\partial^2 f(\B\state)}{\partial x_i \partial x_j} d\state[i](t) d\state[j](t).
\end{align*}
Notice that in the second term the mixed $i,j$ partial derivative is matched with the $i,j$ covariation. A sum of this form brings to mind a trace of a matrix product, or some sort of generalized inner product indexed by the 
two variables $i,j$. Notationally, we opt for the latter and choose to denote the above formula using the shorthand 
\begin{align*}
df(\B\state) &= \langle \nabla f(\B\state), d\B\state \rangle + \frac{1}{2} \langle \nabla^2 f(\B\state), d\B\state d\B\state^\top  \rangle,
\end{align*}
where $\nabla^2$ is used to denote the Hessian here. We note that there are a variety of other notations for the sum in the second term, including 
\begin{align*}
\text{tr}\left\{(\nabla^2 f(\B\state)) d\B\state d\B\state^\top \right\} \text{ and } (d\B\state d\B\state^\top) : \nabla\nabla f.
\end{align*}

Note that we can choose one of the entries of the vector $\B\state$ to simply equal $t$. Doing so gives us the formula for functions of the form $f(\B\state, t)$, which are 
often of interest. To obtain this, note that the covariation between $t$ and all of the Brownian motions is zero. Moreover, the quadratic variation of $t$ is also zero. Therefore, 
all terms corresponding to the $t$ drop out of the second term. We thus obtain, 
\begin{align*}
df(\B\state, t) &= \frac{\partial f(\B\state,t)}{\partial t}dt  + \langle \nabla f(\B\state), d\B\state \rangle + \frac{1}{2} \langle \nabla^2 f(\B\state), d\B\state d\B\state^\top  \rangle.
\end{align*}

To note one more special case, consider the $\ito$ diffusion \ref{SDE_multidim}. Recalling the form of the covariation term in this case, we obtain 
\begin{align*}
df(\B\state, t) &= \frac{\partial f(\B\state,t)}{\partial t}dt + \langle \nabla f(\B\state,t), d\B\state \rangle + \frac{1}{2} \langle \nabla^2 f(\B\state,t), \diffMat dt  \rangle.
\end{align*}




\end{document}


