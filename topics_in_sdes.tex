\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand{\B}[1]{\boldsymbol{#1}}
\newcommand{\Gaussian}{\mathcal{N}}
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\be}{\mathbf{e}}

% SDEs
\newcommand{\state}[1][t]{X_{#1}}
\newcommand{\stateValue}[1][t]{x_{#1}}
\newcommand{\BM}[1][t]{B_{#1}} % Brownian Motion
\newcommand{\op}[1][t]{\mathcal{P}_{#1}} % Transition Operator, i.e. one-parameter semi-group
\newcommand{\opId}{\mathcal{I}} % Identity operator. 
\newcommand{\gen}{\mathcal{L}} % Infinitesimal generator
\newcommand{\ito}{\text{It}\hat{\text{o}}}
\newcommand{\dimBM}{d} % Number of Brownian motions
\newcommand{\dimState}{n} 
\newcommand{\diffMat}{\mathbf{A}} % Diffusion matrix 
\newcommand{\invariantDens}{h} % Density of invariant distribution
\newcommand{\potential}{\Phi} % Potential function

\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}


\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Topics in Stochastic Differential Equations}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% Generator and Adjoint
\section{Time-Homogenous Diffusions}
We are considering the SDE 
\begin{align}
 d\state = \mu(\state) dt + \sigma(\state) d\BM, \label{time_hom_SDE}
\end{align}
where $\mu(\state)$ and $\sigma(\state)$ are assumed time-independent for now. Under this assumption, the solution $\state$ to the SDE has a transition 
kernel which only depends on the time increment; i.e. it is \textbf{time-homogenous}. By \textbf{diffusion}, we mean a solution to an SDE 
(i.e. \ref{time_hom_SDE}), or more generally a continuous-time Markov process with continuous sample paths. 

In particular, we often want to understand the evolution of $f(\state)$, some function of the SDE solution. The question is: how can we study the properties 
of the random process $f(\state)$? A common approach is to instead consider a \textit{non-random} quantity associated with $f(\state)$, which reduces the 
problem to a (presumably easier) deterministic one. Some examples of non-random quantities might be the expectation, probability density, or transition 
density of $f(\state)$. As a starting point, let us consider analyzing the behavior of expectations such as $\E[f(\state) | \state[0] = \stateValue[0]]$. 
Given that we want to understand the evolution of this quantity, a natural thing to do is to consider its time derivative. Appealing to the definition of the 
derivative, we have 
\begin{align*}
\frac{d}{dt} \E[f(\state) | \state[0] = \stateValue[0]] 
&= \lim_{h \to 0} \frac{\E[f(\state[t+h]) | \state[0] = \stateValue[0]] - \E[f(\state[t]) | \state[0] = \stateValue[0]]}{h} \\
&=  \lim_{h \to 0} \frac{\E[f(\state[h]) | \state[0] = \stateValue[0]] - \E[f(\state[0]) | \state[0] = \stateValue[0]]}{h} \\
&=  \lim_{h \to 0} \frac{\E[f(\state[h]) | \state[0] = \stateValue[0]] - \E[f(\state[0]) | \state[0] = \stateValue[0]]}{h} \\
&=  \lim_{h \to 0} \frac{\E[f(\state[h]) | \state[0] = \stateValue[0]] - f(\stateValue[0])}{h}
\end{align*}
In the second step we have used the time-homogeneity property, which allowed us to just consider $t = 0$. The expression on the right is very important, 
and therefore we introduce a few definitions. First, define the operator $\op$ defined by 
\[
(\op f)(\stateValue[]) := \E[f(\state) | \state[0] = \stateValue[]].
\]

Note that this is a generalization of the finite-state space setting, where $\op$ is a matrix and $f$ a column vector constructed by evaluating $f$ at each state. 

As a specific example, if we take $f(u) = \mathbbm{1}_B(u)$ for some Borel set $B$, then this becomes 
\[
(\op f)(\stateValue[]) = \E[\mathbbm{1}_B(\state) | \state[0] = \stateValue[]] = \Prob(\state \in B | \state[0] = \stateValue[]).
\]

Similarly, choosing $f(u) = \mathbbm{1}(u \leq y)$ yields the transition distribution function for a time-homogenous process:
\[
(\op f)(\stateValue[]) = \E[\mathbbm{1}(\state \leq y) | \state[0] = \stateValue[]] = \Prob(\state \leq y | \state[0] = \stateValue[]).
\]

Let's now plug this new definition for $\op$ back into the derivative expression we had before. Doing so, and letting $\opId$ denote the identity operator, yields 
\begin{align*}
\frac{d}{dt} (\op f)(\stateValue[0]) 
&= \lim_{h \to 0} \frac{(\op[t+h] f)(\stateValue[0]) - (\op[t] f)(\stateValue[0])}{h} \\
&= \lim_{h \to 0} \frac{(\op[t]\op[h] f)(\stateValue[0]) - (\op[t] f)(\stateValue[0])}{h} \\
&= \lim_{h \to 0} \frac{\op[t] \left[(\op[h] f)(\stateValue[0]) - f(\stateValue[0])\right]}{h} \\
&= \op[t] \left(\lim_{h \to 0} \frac{\op[h] - \opId}{h}\right)[f](x_0) \\
\end{align*}

We can thus view the expression in parentheses as another operator that acts on $f$ in order to give the instantaneous time-evolution of the quantity 
$\E[f(\state) | \state[0] = \stateValue[0]]$. This new operator, called the \textbf{infinitesimal generator} and denoted $\gen$, is very important. We have thus defined 
two fundamental operators, $\op$ and $\gen$, which are explored in detail in the following two sections. 

% The Markov Semigroup
\subsection{The Markov Semigroup}
\textbf{TODO} 

% The Generator 
\subsection{The Generator}
The \textbf{infinitesimal generator}, or just \textbf{generator}, of a time-homogenous Markov process is defined as the operator
\begin{align}
\gen = \lim_{h \to 0} \frac{\op[h] - \opId}{h},
\end{align}
which is just the expression we stumbled across in the previous section. In fact, what we showed in that section is 
\begin{align}
\frac{d}{dt} (\op f)(\stateValue[0]) = \frac{d}{dt} \E[f(\state) | \state[0] = \stateValue[0]] = \op \gen f(\stateValue[0]). \label{generator_time_deriv}
\end{align}
Currently, this quantity is quite-abstract; however, we will shortly derive the specific form it takes for the time-homogenous SDE \ref{time_hom_SDE}. Before that, 
we give a bit more motivation about why the generator will be such a key object to study. 

Let's again consider the case where we choose $f(u) = \mathbbm{1}(u \leq y)$ so that $(\op f)(\stateValue[]) = \Prob(\state \leq y | \state[0] = \stateValue[])$ is the 
transition distribution function. Under certain conditions, the transition density $p(t, \stateValue[], y)$ -- the derivative of this distribution function -- will also exist. 
Notice that this form of the transition density relies on the time-homogeneity assumption; it gives the density of transitioning from state $x$ to state $y$ in a time 
interval of length $t$, independent of the actual starting time. In this case, the expression \ref{generator_time_deriv} becomes 
\begin{align*}
\frac{d}{dt} \Prob(\state \leq y | \state[0] = \stateValue[0]) = p(t, \stateValue[0], y) = \op \gen f(\stateValue[0]), 
\end{align*}
so we see that the generator is intimately tied to the transition distribution of the diffusion process. 
This will lead to some powerful results. Peaking ahead, we might 
reasonably call the process \textbf{stationary} if its transition distribution is not changing; i.e. 
\begin{align*}
\frac{d}{dt} \Prob(\state \leq y | \state[0] = \stateValue[]) = (\gen f)(\stateValue[]) = 0,
\end{align*} 
which means that $\Prob(\state \leq y | \state[0] = \stateValue[]) = $ is not a function of $t$. We notice that this reasonable definition can be 
re-stated in terms of the null space of the generator. We have been considering functions of the form $f_y(u) = \mathbbm{1}(u \leq y)$. The above 
equality states that in order for the process to be stationary, such functions must be in the null space of $\gen$. We will return to these ideas when we consider
\textbf{ergodic processes}. 

We now derive the generator of the time-independent SDE. 
\begin{thm} 
The generator $\gen$ for the time-homogenous $\ito$ process 
\[ d\state = \mu(\state) dt + \sigma(\state) d\BM \] 
is given by 
\begin{align}
(\gen f)(x) = \mu(x) f^\prime(x) + \frac{1}{2} \sigma^2(x) f^{\prime\prime}(x) 
\end{align}

\end{thm}

\begin{proof} 
There are multiple ways to arrive at this formula. Here we consider a straightforward application of the $\ito$ formula. Let $f \in \mathcal{C}^2$ have compact support. 
We apply the $\ito$ formula to $f(\state)$ to obtain 
\begin{align*}
df(\state)
&= f^\prime(\state) d\state + \frac{1}{2} f^{\prime\prime}(\state) d[\state, \state](t) \\
&= f^\prime(\state) d\state + \frac{1}{2} f^{\prime\prime}(\state) \sigma^2(\state) dt \\
&= f^\prime(\state) \left[\mu(\state) dt + \sigma(\state) d\BM \right] + \frac{1}{2} f^{\prime\prime}(\state) \sigma^2(\state) dt \\
&= \left[f^\prime(\state)\mu(\state) +  \frac{1}{2} f^{\prime\prime}(\state) \sigma^2(\state) \right] dt + f^\prime(\state) \sigma(\state) d\BM
\end{align*}

Now, we notice that the expression in brackets is precisely what we are claiming to be the generator $(\gen f)(\state)$. Plugging this notation in, we have shown
\begin{align}
df(\state) = (\gen f)(\state) dt +  f^\prime(\state) \sigma(\state) d\BM. \label{Ito_formula_generator}
\end{align}

Converting this differential to its integral representation (i.e. integrating from $0$ to $t$), we obtain 
\[
f(\state) - f(\stateValue[]) = \int_{0}^{t} (\gen f)(\state[s]) ds + \int_{0}^{t} f^\prime(\state[s]) \sigma(\state[s]) d\BM[s]
\]
We now consider taking the expectation, conditional on $\state[0] = \stateValue[]$, of both sides. This yields 
\[
\E[f(\state) | \state[0] = \stateValue[]] - f(\stateValue[]) = \int_{0}^{t} \E[(\gen f)(\state[s]) | \state[0] = \stateValue[]] ds + \E\left[\int_{0}^{t} f^\prime(\state[s]) \sigma(\state[s]) d\BM[s] \bigg| \state[0] = \stateValue[] \right] 
\]
We have applied Fubini's theorem for the first term on the righthand side. We make a few observations. First, note that the lefthand side is precisely $(\op f)(\stateValue[]) - f(\stateValue[])$. The second term on the righthand side 
is an $\ito$ integral. Using the assumption that $f$, and hence $f^\prime$, has compact support, then we conclude the expectation of this $\ito$ integral is zero. Applying these two observations, we now have
\[
(\op f)(\stateValue[]) - f(\stateValue[])  = \int_{0}^{t} \E[(\gen f)(\state[s]) | \state[0] = \stateValue[]] ds.
\] 
Now, we can recover the definition of the generator on the lefthand side by dividing through by $t$ and letting $t \to 0$. Doing so gives, 
\begin{align*}
\lim_{t \to 0} \frac{(\op f)(\stateValue[]) - f(\stateValue[]) }{t} 
&= \lim_{t \to 0} \frac{1}{t} \int_{0}^{t} \E[(\gen f)(\state[s]) | \state[0] = \stateValue[]] ds \\
&= \frac{d}{dt}\bigg|_{t=0} \int_{0}^{t} \E[(\gen f)(\state[s]) | \state[0] = \stateValue[]] ds \\
&= \E[(\gen f)(\state[0]) | \state[0] = \stateValue[]] \\
&= (\gen f)(\stateValue[]), 
\end{align*}
where the penultimate line uses the fundamental theorem of calculus. The result is proven. 
\end{proof}

Note that \ref{Ito_formula_generator} also gives a nice, compact way to write the $\ito$ formula in terms of the generator; in integral form, 
\begin{align}
f(\state) =  f(\state[0]) + \int_{0}^{t} (\gen f)(\state[s]) ds + \int_{0}^{t} f^\prime(\state[s]) \sigma(\state[s]) d\BM[s].
\end{align}

The way to think about this ties back to our motivation for the definition of the generator. What the above result gives us is a PDE describing 
the evolution of statistics of the SDE. 
\begin{align}
\frac{d}{dt} \E[f(\state) | \state[0] = \stateValue[0]] = \mu(x) f^\prime(x) + \frac{1}{2} \sigma^2(x) f^{\prime\prime}(x),
\end{align}
noting that the righthand side is $(\gen f)(\stateValue[0])$, as just derived above. The time-derivative of the SDE statistic on the left is 
related to the state derivatives on the right. As a specific example, choosing $f$ to be the identity would yields a PDE describing the 
evolution of the mean of $\state$. 

% The Fokker-Planck Equation
\subsection{The Fokker-Planck Equation}
The \textbf{Fokker-Planck equation} equation, also known as the \textbf{Kolmogorov forward equation}, is a partial differential equation (PDE) describing the evolution of the 
probability density of the solution of the SDE. Recall that we are considering time-homogenous SDEs, so the density under consideration here as the form 
$p(t, \stateValue[0], \stateValue[])$: the density of starting at state $\stateValue[0]$ and ending up at state $\stateValue[]$ after time $t$. The equation we will derive is known 
as the \textit{forward} equation as the PDE will describe the evolution of this density in the \textit{forward variables} $t$ and $\stateValue[]$, as opposed to considering derivatives
with respect to the starting location $\stateValue[0]$. 

There are many ways to derive this equation. The derivation below simply applies the $\ito$ formula and then uses integration by parts, sweeping some of the theoretical 
justifications under the rug. Later in this section we re-cast this derivation using more generic tools from operator theory. 

\begin{thm}
(The Fokker-Planck/Forward Kolmogorov Equation) The transition density $p(t, \stateValue[0], \stateValue)$ associated with the time-homogenous SDE
\ref{time_hom_SDE} satisfies the PDE
\begin{align}
\frac{\partial p}{\partial t} = -\frac{\partial}{\partial x} [p(t, \stateValue[0], \stateValue[])\mu(\stateValue[])] + \frac{1}{2} \frac{\partial^2}{\partial x^2} [p(t, \stateValue[0], \stateValue[]) \sigma^2(\stateValue[])].
\end{align}
\end{thm}

\begin{proof}
Let $f(\stateValue[], t)$ be a twice continuously-differentiable function which vanishes outside of the time interval $(0, T)$. As usual, we start by applying the 
$\ito$ formula,
\begin{align*}
df(\stateValue[], t) 
&= \frac{\partial f}{\partial x} d\state + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} \sigma^2(\state) dt + \frac{\partial f}{\partial t} dt \\
&= \left[\frac{\partial f}{\partial t} + \frac{\partial f}{\partial x}\mu(\state) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\sigma^2(\state) \right]dt + \frac{\partial f}{\partial x}\sigma(\state) d\BM,
\end{align*}
where in the second step we plugged in the differential $d\state$ and grouped terms. Integrating from $0$ to $T$, 
\begin{align*}
f(\state[T], T) - f(\state[0], 0) 
&= \int_{0}^{T} \left[\frac{\partial f}{\partial t} + \frac{\partial f}{\partial x}\mu(\state) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\sigma^2(\state) \right]dt 
	+ \int_{0}^{T} \frac{\partial f}{\partial x}\sigma(\state) d\BM
\end{align*}
and now taking expectation conditional on $\state[0] = \stateValue[0]$,
\begin{align*}
\E[f(\state[T], T) | \state[0] = \stateValue[0]] - f(\stateValue[0], 0)
&= \int_{0}^{T} \E\left[\frac{\partial f}{\partial t} + \frac{\partial f}{\partial x}\mu(\state) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\sigma^2(\state) \bigg| \state[0] = \stateValue[0] \right]dt.
\end{align*}
On the righthand side, we have taken two steps. We reversed the order of integration in the first term, which can be justified using Fubini's theorem.
We also applied the fact that the expectation of the second term is zero, using the facts that it is an $\ito$ integral and that $f$ was assumed to have compact support. 
We now re-write the expectation in the remaining term using the transition density, 
\begin{align*}
\E[f(\state[T], T) | \state[0] = \stateValue[0]] - f(\stateValue[0], 0)
&= \int_{0}^{T} \int_{-\infty}^{\infty} \left[\frac{\partial f}{\partial t} + \frac{\partial f}{\partial x}\mu(\state) + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\sigma^2(\state)\right] p(t,\stateValue[0],\stateValue[]) dx dt
\end{align*}
At this point we will apply linearity to write this as the sum of three integrals, and consider each in turn. To lighten notation, we drop the function arguments, writing, for example, 
$p = p(t, \stateValue[0], \stateValue)$. We thus have, 
\begin{align*}
\E[f(\state[T], T) | \state[0] = \stateValue[0]] - f(\stateValue[0], 0)
&= \int_{0}^{T} \int_{-\infty}^{\infty} p\frac{\partial f}{\partial t} dxdt + \int_{0}^{T} \int_{-\infty}^{\infty} p\mu \frac{\partial f}{\partial x} dxdt + \frac{1}{2} \int_{0}^{T} \int_{-\infty}^{\infty} p\sigma^2 \frac{\partial^2 f}{\partial x^2} dxdt
\end{align*}
We now deal with each of this integrals in turn. Our goal is to eliminate derivatives the of $f$, instead picking up derivatives of $p$. 

\bigskip
\noindent
\textbf{First Integral:} The strategy for this term is to flip the order of integration (by Fubini) and then integrate by parts with respect to $t$. 
Integrating by parts with $u(t) = p(t, \stateValue[0], \stateValue)$ and $v^\prime(t) = \frac{\partial f}{\partial t}$ yields 
\begin{align*}
t
\end{align*}

\end{proof}

We have phrased the Fokker-Planck equation as describing the evolution of the transition density, but note that this is essentially the same thing as 
describing the evolution of the density of $\state$ itself. The density $p_t(\stateValue[])$ of $\state$ will always depend on the initial condition. If the initial condition is 
deterministic, say $\stateValue[0]$, then we simply have $p_t(\stateValue[]) = p(t, \stateValue[0], \stateValue[])$. Essentially, what we showed above can be re-phrased
as the initial value problem
\begin{align}
&\frac{\partial p}{\partial t} = -\frac{\partial}{\partial x} [p(t, \stateValue[0], \stateValue[])\mu(\stateValue[])] + \frac{1}{2} \frac{\partial^2}{\partial x^2} [p(t, \stateValue[0], \stateValue[]) \sigma^2(\stateValue[])] \\
&p(0, \stateValue[0], \stateValue[]) = \delta_{\stateValue[0]}(\stateValue[]) \nonumber
\end{align}

% SDEs in Multiple Dimensions
\section{SDEs in Multiple Dimensions}

\subsection{$\ito$ processes}
There are two ways we can consider adding more dimensions here: 1.) a scalar SDE being driven by multiple Brownian motions, and 
2.) a system of SDEs (with each equation potentially being driven by multiple Brownian motions). Most of the results discussed above have 
natural analogs to these settings; the notation just becomes more burdensome. Beginning with the first case, suppose we have 
$\dimBM$ independent Brownian motions driving the system. We will write these in the vector 
\[
\B\BM := \begin{pmatrix} \BM[1](t) & \dots & \BM[\dimBM](t) \end{pmatrix}^\top,
\]
noting that we will be switching between the functional and subscript notation for dependence on time. This will prevent double-subscripts. 
It is not difficult to  show that the quadratic variation between independent Brownian motions is zero, 
\[
d\BM[i](t) d\BM[j](t) = \delta_{ij} dt,
\]
that is, 
\[
d\B\BM d\B\BM^\top = \B{I} dt.
\]
Given a vector-valued function $G: \R \to \R^{\dimBM}$, we can then define the SDE
\begin{align*}
d\state &= \mu(\state) dt + \sum_{j=1}^{\dimBM} G_j(\state) d\BM[j](t), 
\end{align*}
which is to be interpreted as 
\begin{align*}
\state &= \state[0] + \int_{0}^{t} \mu(\state[s]) ds + \sum_{j=1}^{\dimBM} \int_{0}^{t} G_j(\state[s]) d\BM[j](s).
\end{align*}
To lighten notation, we write 
\begin{align*}
d\state &= \mu(\state) dt + \langle G(\state), d\B\BM \rangle.
\end{align*}
Dot product notation in place of the inner product notation is also common. We next generalize to the situation where there are $\dimState$ 
SDEs, which we write in the vector
\[
\B\state := \begin{pmatrix} \state[1](t) & \dots & \state[\dimState](t) \end{pmatrix}^\top.
\]
Each of these state variables may be driven by all of the Brownian motions, yielding a coupled system of $\dimState$ equations
driven by $\dimBM$ Brownian motions. For functions $\mu(\cdot, t): \R^{\dimState} \to \R^{\dimState}$ and $\sigma(\cdot, t): \R^{\dimState} \to \R^{\dimState \times \dimBM}$ this
 system can be written component-wise as,
\begin{align*}
d\state[i](t) &= \mu_i(\state,t) dt + \sum_{j=1}^{\dimBM} \sigma_{ij}(\state,t) d\BM[j](t), 
\end{align*}
and the entire system can be represented succinctly as
\begin{align}
d\B\state &= \B\mu(\B\state,t) dt + \B\sigma(\B\state,t) d\B\BM, \label{SDE_multidim}
\end{align}
noting that $\B\sigma$ is a matrix-valued function. One can derive the covariation 
\begin{align*}
&d\state[i](t) d\state[j](t) = \diffMat_{ij} dt, &&\diffMat = \B\sigma \B\sigma^\top, 
\end{align*}
where $\diffMat$ is known as the \textbf{diffusion matrix}. This fact can be written in matrix form as 
\begin{align*}
d\B\state d\B\state^\top = \B\sigma\B\sigma^\top dt = \diffMat dt.
\end{align*}
Finally, we note that this setup can be generalized to introduce correlation among the different driving Brownian motions. Such correlation would manifest as 
\[
d\B\BM d\B\BM^\top = \B{Q} dt, 
\]
for some covariance matrix $\B{Q}$. \textbf{TODO: expand on this, see Saarka book.}

\subsection{$\ito$'s Formula}
We now provide the generalization of the $\ito$ formula for scalar-valued functions $f: \R^{\dimState} \to \R$. The formula is given in differential form as
\begin{align*}
df(\B\state) &= \sum_{i=1}^{\dimState} \frac{\partial f(\B\state)}{\partial x_i} d\state[i](t) + \frac{1}{2} \sum_{i=1}^{\dimState}\sum_{j=1}^{\dimState} \frac{\partial^2 f(\B\state)}{\partial x_i \partial x_j} d\state[i](t) d\state[j](t).
\end{align*}
Notice that in the second term the mixed $i,j$ partial derivative is matched with the $i,j$ covariation. A sum of this form brings to mind a trace of a matrix product, or some sort of generalized inner product indexed by the 
two variables $i,j$. Notationally, we opt for the latter and choose to denote the above formula using the shorthand 
\begin{align*}
df(\B\state) &= \langle \nabla f(\B\state), d\B\state \rangle + \frac{1}{2} \langle \nabla^2 f(\B\state), d\B\state d\B\state^\top  \rangle,
\end{align*}
where $\nabla^2$ is used to denote the Hessian here. We note that there are a variety of other notations for the sum in the second term, including 
\begin{align*}
\text{tr}\left\{(\nabla^2 f(\B\state)) d\B\state d\B\state^\top \right\} \text{ and } (d\B\state d\B\state^\top) : \nabla\nabla f.
\end{align*}

Note that we can choose one of the entries of the vector $\B\state$ to simply equal $t$. Doing so gives us the formula for functions of the form $f(\B\state, t)$, which are 
often of interest. To obtain this, note that the covariation between $t$ and all of the Brownian motions is zero. Moreover, the quadratic variation of $t$ is also zero. Therefore, 
all terms corresponding to the $t$ drop out of the second term. We thus obtain, 
\begin{align*}
df(\B\state, t) &= \frac{\partial f(\B\state,t)}{\partial t}dt  + \langle \nabla f(\B\state), d\B\state \rangle + \frac{1}{2} \langle \nabla^2 f(\B\state), d\B\state d\B\state^\top  \rangle.
\end{align*}

To note one more special case, consider the $\ito$ diffusion \ref{SDE_multidim}. Recalling the covariation term takes the form $d\B\state d\B\state^\top = \diffMat dt$ in this case, we obtain 
\begin{align}
df(\B\state, t) 
&= \frac{\partial f(\B\state,t)}{\partial t}dt + \langle \nabla f(\B\state,t), \B\mu(\B\state,t) dt + \B\sigma(\B\state,t) d\B\BM \rangle + \frac{1}{2} \langle \nabla^2 f(\B\state,t), \diffMat dt  \rangle \label{Ito_formula_diffusion} \\
&= \left[\frac{\partial f(\B\state,t)}{\partial t} + \langle \nabla f(\B\state,t), \B\mu(\B\state,t) \rangle + \frac{1}{2} \langle \nabla^2 f(\B\state,t), \diffMat \rangle \right] dt + 
\left\langle \nabla f(\B\state,t), \sigma(\B\state, t) d\B\BM  \right\rangle \nonumber
\end{align}
Finally, we note that the $\ito$ formula can be generalized to vector-valued functions $f$ by simply applying the above formula in a componentwise fashion. The subsequent 
section shows how the $\ito$ formula applied to an $\ito$ process can be written more succinctly using the generator. 

\subsection{Generator and Adjoint}
The multivariate generator and adjoint are also similar to their univariate analogs. Indeed, for the $\ito$ process 
\[
d\B\state = \B\mu(\B\state,t) dt + \B\sigma(\B\state,t) d\B\BM
\]
we have,
\begin{align*}
\gen f &= \langle \nabla f, \B\mu \rangle + \frac{1}{2} \langle \nabla^2 f, \diffMat \rangle \\
\gen^* f &= -\langle \nabla, \B\mu f \rangle + \frac{1}{2} \langle \nabla^2, \diffMat f\rangle
\end{align*}

Recall the definition of the diffusion matrix $\diffMat = \B\sigma \B\sigma^\top$. Note that these equations are defined for potentially time-inhomogenous diffusions 
($\B\mu$ and $\B\sigma$ are potentially functions of $t$), but the function $f$ is only a function of $x$. A generalization to functions of the form 
$f(x,t)$ is provided in Saarka 5.1 under the name \textit{generalized generator}. To be clear on the notation used in these expressions, note that they 
can be fully written out as 
\begin{align*}
\gen f(\bx) &= \sum_{i=1}^{\dimState} \frac{\partial}{\partial x_i} \left[f(\bx)\right] \B\mu_i(\bx,t) + 
\frac{1}{2}\sum_{i=1}^{\dimState}\sum_{j=1}^{\dimState} \frac{\partial^2}{\partial x_i \partial x_j}\left[f(\bx)\right] \diffMat_{ij} \\
\gen^* f(\bx) 
&= -\sum_{i=1}^{\dimState} \frac{\partial}{\partial x_i}\left[\B\mu_i(\B{x},t)f(\B{x}) \right] + \frac{1}{2} \sum_{i=1}^{\dimState}\sum_{j=1}^{\dimState} \frac{\partial^2}{\partial x_i \partial x_j}\left[\diffMat_{ij}(\bx,t)f(\bx) \right].
\end{align*}

The $\ito$ formula applied to a $\ito$ diffusion can be written succinctly using the generator. Indeed, notice that the term in brackets in \ref{Ito_formula_diffusion} is precisely the 
generator $\gen f(\B\state, t)$ (plus the time derivative). Thus, the $\ito$ formula for the $\ito$ diffusion \ref{SDE_multidim} can be written 
\begin{align}
df(\B\state, t) &= \left[\frac{\partial f(\B\state,t)}{\partial t} + \gen f(\B\state, t)\right] dt + \left\langle \nabla f(\B\state,t), \sigma(\B\state, t) d\B\BM  \right\rangle.
\end{align}

% Numerical Methods
\section{Numerical Methods}



% Stationary Distributions 
\section{Stationary Distributions}
In this section we are considering time-homogenous diffusions 
\[
d\B\state = \B\mu(\B\state) dt + \B\sigma(\B\state) d\B\BM
\]
If we consider a diffusion process $\{\B\state\}$ with initial condition $\B{\state[0]} \sim \nu$, then $\nu$ is said to be a \textbf{stationary} or \textbf{invariant}
distribution of the process if $\B\state \sim \nu$ for all $t \geq 0$; equivalently, 
\[
\nu(A) = \nu P(A), \text{ for all measurable } A,
\]
where $P(x,A)$ denotes the one-step transition operator of the time-homogenous diffusion. In general, finding stationary distributions using this integral 
equation is difficult, but Fokker-Planck equation gives a much more convenient criterion when the stationary distribution has a density $\invariantDens$. By Fokker-Planck, 
\[
\frac{\partial \invariantDens}{\partial t} = \gen^* \invariantDens.
\]
However, the invariant density must satisfy 
\[
\frac{\partial \invariantDens}{\partial t} = 0.
\]
Combining these, we obtain the condition 
\begin{align}
\gen^* \invariantDens = 0, 
\end{align}
so the invariant density can be viewed as an element of the null space of the adjoint operator. \textbf{TODO: include example of stationary dist for OU process}

\subsection{Designing Diffusions for Algorithms}
Here we discuss how to define a diffusion that has a desired target stationary distribution. In general this is difficult. However, a specific class of SDEs is amenable to 
this task, which are known as \textbf{Langevin diffusions}. We start with such a diffusion of the form 
\begin{align}
d\B\state = -\frac{1}{2} \nabla \potential(\B\state) dt + \sigma d\B\BM \label{Langevin_diffusion_1},
\end{align}
where $\sigma > 0$ is a constant, and $\potential$ is a \textbf{potential function}. This equation can be thought of as describing the trajectory of a particle then tends 
to be in the \textit{potential well} specified by $\potential$, but in a noisy fashion due to the noise $d\BM$ with variance $\sigma^2$. 
By potential well, we simply mean that the particle tends to move in the negative direction of the gradient - the direction of steepest descent. This equation has 
stationary distribution 
\[
\invariantDens(\bx) = \frac{1}{Z} \exp\left\{-\frac{\potential(\bx)}{\sigma^2} \right\},
\]
with normalization constant $Z$. This can be checked by confirming $\gen^* \invariantDens = 0$. We will be using this fact to reverse engineer this process; that is, we 
will have a target density $\invariantDens$ and want to choose the potential correctly so that the resulting SDE has invariant distribution with this density. To this end, let's 
think about how the density relates to the potential in the above equations. We have 
\[
\log \invariantDens(\bx) = -\frac{\potential(\bx)}{\sigma^2}  - \log(Z), 
\]
so 
\[
\potential(\bx) = -\sigma^2\left(\log \invariantDens(\bx) + \log Z \right).
\]
However, note that the potential only appears in the diffusion \ref{Langevin_diffusion_1} through its gradient. Therefore, the normalizing constant $\log Z$ won't affect the 
resulting SDE. This is great news for developing sampling algorithms, where we typically only know the target density up to a normalizing constant. Therefore, in this case 
we see that setting the potential to the scaled negative log density 
\[
\potential(\bx) = -\sigma^2 \log \invariantDens(\bx)
\]
would do the trick, with the understanding that the density $\invariantDens(\bx)$ need not be normalized. Intuitively, this makes sense; we know the density looks something like 
a downward facing bowl, so the negative log density will indeed look like some sort of ``well''. Continuing on the discussion of reverse-engineering this process, suppose we are 
given a (potentially unnormalized) density $\invariantDens(\bx)$ and set the potential function as above. Then its gradient is 
\[
\nabla \potential(\bx) = -\sigma^2 \nabla \log \invariantDens(\bx). 
\]
Plugging this choice into the diffusion \ref{Langevin_diffusion_1} then yields 
\begin{align}
d\B\state = \frac{\sigma^2}{2} \nabla \log \invariantDens(\state) dt + \sigma d\B\BM \label{Langevin_diffusion_2}. 
\end{align}
Note that we could have set $\sigma = 1$ in \ref{Langevin_diffusion_1} and then adjusted the potential accordingly to maintain the desired stationary distribution. The choice of 
how to set the diffusion coefficient in \ref{Langevin_diffusion_1} is thus a design choice. We can actually generalize this and replace $\sigma^2$ in \ref{Langevin_diffusion_2} 
with a positive definite matrix $\Sigma$. The correct scaling on the diffusion term is then the square root of this matrix, $\Sigma^{1/2}$. When we write $\Sigma^{1/2}$ we assume 
this is a symmetric square root of $\Sigma$. This yields a more general Langevin diffusion 
\begin{align}
d\B\state = \frac{1}{2} \Sigma \nabla \log \invariantDens(\state) dt + \Sigma^{1/2} d\B\BM \label{Langevin_diffusion_3}.
\end{align}
Given that \ref{Langevin_diffusion_3} will be the typical starting point for our considerations of algorithm design, we take the time to actually verify the claim that the 
\ref{Langevin_diffusion_3} has invariant distribution with density $\invariantDens(\bx)$. 

\begin{prop}
The Langevin diffusion \ref{Langevin_diffusion_3} has a stationary distribution with probability density $\invariantDens(\bx)$. 
\end{prop}

\begin{proof}
We check that $\gen^* \invariantDens = 0$. The equation \ref{Langevin_diffusion_3} defines an $\ito$ process and therefore has adjoint
\[
\gen^* \invariantDens(\bx) = -\left\langle \nabla, \frac{1}{2} \Sigma \invariantDens(\bx) \nabla \log \invariantDens(\bx) \right\rangle + \frac{1}{2} \left\langle \nabla^2, \Sigma \invariantDens(\bx)\right\rangle
\]
We can simplify the expression by plugging in 
\[
\nabla \log \invariantDens(\bx) = \frac{\nabla \invariantDens(\bx)}{\invariantDens(\bx)}
\]
and noting that the $\invariantDens(\bx)$ terms cancel in the first term. Thus, we have 
\begin{align*}
\gen^* \invariantDens(\bx) 
&= -\frac{1}{2}  \left\langle \nabla, \Sigma \nabla \invariantDens(\bx) \right\rangle + \frac{1}{2} \left\langle \nabla^2, \Sigma \invariantDens(\bx)\right\rangle \\
&= -\frac{1}{2} \sum_{i=1}^{\dimState} \frac{\partial}{\partial x_i} \left[\Sigma \nabla \invariantDens(\bx) \right]_i + \frac{1}{2} \sum_{i,j=1}^{\dimState} \frac{\partial^2}{\partial x_i \partial x_j} \left[\Sigma_{ij} \invariantDens(\bx) \right] \\
&= -\frac{1}{2} \sum_{i=1}^{\dimState} D\left[\Sigma \nabla \invariantDens(\bx) \right]_{ii} + \frac{1}{2} \sum_{i,j=1}^{\dimState} \Sigma_{ij}  \frac{\partial^2}{\partial x_i \partial x_j} \left[\invariantDens(\bx) \right] \\
&= -\frac{1}{2} \text{tr}\left\{D\left[\Sigma \nabla \invariantDens(\bx) \right] \right\} + \frac{1}{2} \text{tr}\left\{\Sigma \nabla^2 \invariantDens(\bx) \right\} \\
&= -\frac{1}{2} \text{tr}\left\{\Sigma D \nabla \invariantDens(\bx) \right\} + \frac{1}{2} \text{tr}\left\{\Sigma \nabla^2 \invariantDens(\bx) \right\} \\
&= -\frac{1}{2} \text{tr}\left\{\Sigma \nabla^2 \invariantDens(\bx) \right\} + \frac{1}{2} \text{tr}\left\{\Sigma \nabla^2 \invariantDens(\bx) \right\} \\
&= 0
\end{align*}
\end{proof}

As an example, let's construct a Langevin diffusion with stationary distribution $\Gaussian(\B\mu, \Sigma)$. In this case, the unnormalized log density is 
\[
\log \invariantDens(\bx) = -\frac{1}{2} (\bx - \B\mu)^\top \Sigma^{-1}(\bx - \B\mu) = -\frac{1}{2} \norm{\Sigma^{-1}(\bx - \B\mu)}^2_2
\]
so 
\[
\nabla \log \invariantDens(\bx) = -\Sigma^{-1} (\bx - \B\mu). 
\]
Plugging this into \ref{Langevin_diffusion_3} we obtain 
\[
d\state = -\frac{1}{2} \Sigma^{-1} (\bx - \B\mu) dt + d\B\BM.
\]
We notice this is the Ornstein-Uhlenbeck (OU) process when $\B\mu = 0$, and a shifted version of this process otherwise. 

% Langevin Diffusions for Sampling Algorithms
\section{Langevin Diffusions for Sampling Algorithms}




\end{document}


