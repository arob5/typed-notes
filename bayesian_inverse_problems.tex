\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Bayesian Inverse Problems}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% General Framework
\section{The General Framework}

\subsection{Introduction to Finite Dimensional Inverse Problems}
Inverse problems generically take the form
\begin{align}
y &= f(\theta),
\end{align}
where $f: \R^d \to \R$ is a forward model (i.e. parameter-to-observation operator), $\theta \in \mathcal{D}$ is an unknown input to the forward model which we seek to learn, and
$y \in \R^n$ are observations of the output of the forward model. The task is to infer the input parameter $\theta$ that gave rise to the observations $y$. Typically, we consider finite
dimensional parameter spaces $\mathcal{D} \subset \R^d$ but will also touch on scenarios where $\mathcal{D}$ is infinite-dimensional (e.g. inferring a spatial field). 
To reflect the reality that 
observations are almost always corrupted by noise, we adopt a statistical framework
\begin{align}
y &= f(\theta) + \epsilon \label{inverse_problem_statistical} \\
\epsilon &\sim \rho_\epsilon \nonumber
\end{align}
which now defines a likelihood $p(y|\theta) := \rho(y - f(\theta))$ that, loosely speaking, expresses the probability of observing the data $y$ given that the input parameter was $\theta$. It is quite common
to assume Gaussian noise 
\begin{align}
y &= f(\theta) + \epsilon \label{inverse_problem_statistical_Gaussian} \\
\epsilon &\sim N_n(0, \Sigma_\epsilon) \nonumber,
\end{align}
which then yields the Gaussian likelihood $p(y|\theta) = N_n(y|f(\theta), \Sigma_\epsilon)$. The function $\rho: \R^d \to \R$ in this case is given by 
\[\rho(s) \propto \exp\left\{-\frac{1}{2} s^{\top} \Sigma_{\epsilon}^{-1} s \right\}\]
 I will consider this particular choice of likelihood throughout much of the exposition, but consider non-Gaussian 
likelihoods later in the paper. Assuming $\Sigma_\epsilon$ is known, a staightforward approach to solving [\ref{inverse_problem_statistical_Gaussian}] (i.e. estimating the value of $\theta$ 
that gave rise to $y$) is to use MLE
\begin{align}
\hat{\theta}_{MLE} &:= \argmax_{\theta} N_n(y|f(\theta), \Sigma_\epsilon) \\
			     &= \argmin_{\theta \in \mathcal{D}} \frac{1}{2} (y - f(\theta))^{\top} \Sigma_\epsilon^{-1} (y - f(\theta)) \nonumber
\end{align}
However, in applications this approach commonly suffers from various problems, including the lack of a unique solution to the optimization problem and high sensitivity of the minimizer to perturbations of 
the data $y$. To ameliorate these issues, one approach is to \textit{regularize} the problem by penalizing values of $\theta$ that drift too far away from some fixed point in the input space $\mathcal{D}$. 
\begin{align}
\hat{\theta}_{\theta_0, \Sigma_0} &= \argmin_{\theta \in \mathcal{D}} \frac{1}{2} (y - f(\theta))^{\top} \Sigma_\epsilon^{-1} (y - f(\theta)) + \frac{1}{2} (\theta - \theta_0)^{\top} \Sigma_0^{-1}(\theta - \theta_0) \label{regularization}
\end{align}
where $\Sigma_0$ characterizes the weighted norm that quantifies the penalty imposed as a function of the distance between $\theta$ and $\theta_0$. The values $\theta_0$ and $\Sigma_0$ must be chosen 
a priori, and this seems problematic given that they appear to be somewhat arbitrary in the formulation [\ref{regularization}]. Adopting a Bayesian framework helps put these choices in a more transparent 
modeling setup. Following typical Bayesian procedure, we place a prior $\pi_0$ on $\theta$ which encodes our current beliefs about the true value of the parameter. 
\begin{align}
y &= f(\theta) + \epsilon \label{inverse_problem_Bayesian} \\
\theta &\sim \pi_0 \\
\epsilon &\sim N_n(0, \Sigma_\epsilon) \nonumber.
\end{align}
Solving the Bayesian inverse problem $[\ref{inverse_problem_Bayesian}]$ now means obtaining (or summarizing) the posterior distribution of the parameter given the data, which is calculated via Bayes' rule. 
\begin{align}
p(\theta|y) &= \frac{N_n(y|f(\theta), \Sigma_\epsilon)\pi_0(\theta)}{\int N(y|f(\theta), \Sigma_\epsilon)\pi_0(\theta) d\theta} \propto N_n(y|f(\theta), \Sigma_\epsilon)\pi_0(\theta) \label{posterior}
\end{align}
The integral in the denominator of [\ref{posterior}] is typically intractable, but $p(\theta|y)$ can be sampled from using Markov Chain Monte Carlo (MCMC) techniques. In the case of a Gaussian prior 
$\theta \sim N_d(\theta_0, \Sigma_0)$ the posterior becomes
\begin{align}
p(\theta|y) &\propto N_n(y|f(\theta), \Sigma_\epsilon)N_d(\theta|\theta_0, \Sigma_0) \\
 		&\propto \exp\left\{-\frac{1}{2} (y - f(\theta))^{\top} \Sigma_\epsilon^{-1} (y - f(\theta)) - \frac{1}{2} (\theta - \theta_0)^{\top} \Sigma_0^{-1}(\theta - \theta_0) \right\} \nonumber
\end{align}
which in general is not Gaussian if $f$ is non-linear. The maximum a posterior (MAP) estimate is thus
\begin{align}
\hat{\theta}_{\text{MAP}} := \argmin_{\theta \in \mathcal{D}} \frac{1}{2} (y - f(\theta))^{\top} \Sigma_\epsilon^{-1} (y - f(\theta)) + \frac{1}{2} (\theta - \theta_0)^{\top} \Sigma_0^{-1}(\theta - \theta_0) \label{MAP}
\end{align}
which we notice is identical to the regularized MLE estimate $\hat{\theta}_{\theta_0, \Sigma_0}$. Therefore, the seemingly arbitrary choices $m_0$ and $\Sigma_0$ in the regularized setting are naturally 
interpreted as the parameters defining the prior on $\theta$ in the Bayesian setting. The Bayesian framework also provides more than just this new interpretation; it provides a solution to the inverse problem 
in terms of a distribution $p(\theta|y)$ rather than just a point estimate $\hat{\theta}_{\theta_0, \Sigma_0}$. While we can still calculate the point estimate $\hat{\theta}_{\text{MAP}}$, there are many other ways 
to interrogate the posterior to conduct more robust uncertainty quantification. 

\subsection{More General Theoretical Framework}
We now consider a general setting for Bayesian inverse problems, where the input parameter space $\mathcal{D}$ may be infinite dimensional. We again consider a forward operator (or parameter-to-observation 
map) $f: \mathcal{D} \to \mathcal{Y}$. As before, the task is to solve the equation
\begin{align*}
y = f(\theta)
\end{align*}
for $\theta \in \mathcal{D}$, given fixed $y \in \mathcal{Y}$. For full generality, we can consider $(\mathcal{D}, \norm{\cdot}_{\mathcal{D}})$ and $(\mathcal{Y},  \norm{\cdot}_{\mathcal{Y}})$ as Banach spaces. As discussed previously, this problem is often plagued by ill-posedness--non-existence 
of solutions or extreme sensitivity of the solution to $y$. A classical approach to address these issues is to instead consider a least-squares, or regularized least-squares, solution to the problem. The notion of distance in the least-squares problem is given by the norm $\norm{\cdot}_{\mathcal{Y}}$, which yields the least-squares problem  
\begin{align*}
\hat{\theta} := \argmin_{\theta \in \mathcal{D}} \frac{1}{2} \norm{y - f(\theta)}_{\mathcal{Y}}^2
\end{align*}
where the $\frac{1}{2}$ is included for consistency, recalling the relation between the squared error and Gaussian distribution discussed in the previous section. For the regularized problem, we penalize deviations 
of $\theta$ from some pre-specified $m_0$. We could of course define this penalty as $\norm{u - m_0}_{\mathcal{D}}$, but in the interest of generality we consider the option of restricting $\theta$ to some smaller 
Banach space $\mathcal{E}$, equipped with arbitrary norm $\norm{\cdot}_\mathcal{E}$:
\begin{align*}
\hat{\theta} := \argmin_{\theta \in \mathcal{E}} \left(\frac{1}{2} \norm{y - f(\theta)}_{\mathcal{Y}}^2 + \frac{1}{2} \norm{\theta - m_0}^2_E\right)
\end{align*}
Moving away from pure optimization and instead adopting a Bayesian statistical framework offers many benefits, as described in the previous section. To this end, we consider a prior probability measure $\mu_0$ on 
$\theta$ which encodes prior information as to the true value of $\theta$, before observing any data. Moreover, we now suppose that the data $y$ has been measured with noise. This yields the model
\begin{align*}
y = f(\theta) + \epsilon
\end{align*}
where $\epsilon$ is the \textit{observational noise}, which we in general consider to be a mean zero random variable. The distributions on $\epsilon$ and $\theta$ induce a distribution on $y$, as well as a joint 
distribution over $(y, \theta)$. The marginal of this joint distribution corresponding to $\theta$ is simply the prior measure $\mu_0$, while the marginal corresponding to $y$ is known as the \textit{evidence}. The primary object of interest is the conditional random variable $\theta|y$, with associated probability measure $\mu^y$. The distribution $\mu^y$ encodes knowledge about the parameter after observing $y$, thus combining 
information from prior beliefs and observed data. 

To make this concrete, we now turn back to the finite dimensional case where $\mathcal{Y} \subset \in \R^n$ and $\mathcal{D} \subset \R^d$ and probability densities exist. In this simpler setting, we consider characterizing the posterior measure $\mu^y$. 

\begin{thm}
Assume $\mathcal{Y} \subset \R^n$, $\mathcal{D} \subset \R^d$, and that the measure $\mu_0$ has associated density (i.e. Radon-Nikodym derivative with respect to Lebesgue measure) $\pi_0$. 
Moreover, assume the likelihood $p(y|\theta)$ is proportional to a density $\rho(y - f(\theta))$. Then, 
\begin{enumerate}
\item The posterior measure $\mu^y$ is absolutely continuous with respect to the prior measure $\mu_0$ (written $\mu^y << \mu_0$). 
\item The Radon-Nikodym derivative of $\mu^y$ with respect to $\mu_0$ is proportional to $\rho(y - f(\theta))$, written $\frac{d\mu^y}{d\mu_0} \propto \rho(y - f(\theta))$. 
\end{enumerate}
\end{thm}

\begin{proof} 
Note that since the prior and likelihood are both given by densities, then the posterior density is given by Bayes' rule
\[\pi^y(\theta) \propto \pi_0(\theta) \rho(y - f(\theta))\]
Let $A$ be any measurable set. To establish the first claim we must show that $\mu_0(A) = 0 \implies \mu^y(A) = 0$. Assuming $\mu_0(A) = 0$ means
\[\int_A \pi_0(\theta) d\theta = 0\]
and since $\pi_0$ is non-negative this implies that $\pi_0(\theta) = 0$ almost surely on $A$. Then, 
\[\mu^y(A) = \int_A \pi^y(\theta) d\theta = \int_A \pi_0(\theta) \rho(y - f(\theta)) d\theta = 0\]
which follows from the fact that $\pi_0(\theta) = 0$ almost surely on $A$. Thus, $\mu^y << \mu_0$. For the second claim, recall that the definition of the Radon-Nikodym derivative requires 
\[\mu^y(A) = \int_A \frac{d\mu^y}{d\mu_0} d\mu_0\]
for any measurable $A$. But we have 
\[\mu^y(A) = \int_A \pi^y(\theta) d\theta \propto \int_A \pi_0(\theta) \rho(y - f(\theta)) d\theta = \int_A \rho(y - f(\theta)) d\mu_0 \]
Comparing the two expressions, we observe $\frac{d\mu^y}{d\mu_0} \propto \rho(y - f(\theta))$. 
\end{proof} 
In the finite-dimensional setting, we now have two representations for the posterior measure:
\begin{enumerate}
\item $\pi^y(\theta) := \frac{d\mu^y}{d\lambda}(\theta) \propto \pi_0(\theta)\rho(y - f(\theta))$, the Radon-Nikodym derivative with respect to the Lebesgue measure $\lambda$ (what we typically think of as the posterior density).
\item $\frac{d\mu^y}{d\mu_0}(\theta) \propto \rho(y - f(\theta))$, the Radon-Nikodym derivative with respect to the prior measure $\mu_0$. 
\end{enumerate}
The density $\frac{d\mu^y}{d\mu_0}$ can be interpreted as the function that re-weights the prior measure by taking into account the data $y$, and thus updating the current beliefs about $\theta$. 

We now consider re-writing $\rho$ in terms of a \textit{potential function} $\Phi$. It is not immediately clear how this is helpful, but the potential function formulation is the one which will naturally extend to settings
where $\mathcal{D}$ or $\mathcal{Y}$ are infinite-dimensional. Moreover, in the common case of a Gaussian likelihood, the potential function has a very natural interpretation as an $L_2$ error metric summarizing 
the discrepancy between $y$ and $f(\theta)$. To this end, note that $\rho$ is a density and hence non-negative so we can define $\Phi(\theta; y)$ such that
\[\rho(y - f(\theta)) \propto \exp\left\{-\Phi(\theta; y)\right\}\]
holds. This gives us a new representation of the Radon-Nikodym derivative of the posterior with respect to the prior:
\[\frac{d\mu^y}{d\mu_0}(\theta) \propto \rho(y - f(\theta)) \propto \exp\left\{-\Phi(\theta; y)\right\}\]
To see that this is quite natural in the familiar Gaussian likelihood setting, consider that $y|\theta \sim \mathcal{N}_n(f(\theta), \Sigma_\epsilon)$ so that 
\[\rho(y - f(\theta)) \propto  \exp\left\{-\frac{1}{2}(y - f(\theta))^{\top} \Sigma_\epsilon^{-1}(y - f(\theta))\right\}.\]
Therefore, we see that 
\[\Phi(\theta; y) = \frac{1}{2}(y - f(\theta))^{\top} \Sigma_\epsilon^{-1}(y - f(\theta))\]
which is just the weighted $L_2$ error between the observed data $y$ and the model output $f(\theta)$. It will be notationally and conceptually useful to explicitly write this as a weighted norm 
\[\norm{y - f(\theta)}^2_{\Sigma_{\epsilon}} := (y - f(\theta))^{\top} \Sigma_\epsilon^{-1}(y - f(\theta))\]
To better understand this definition, note that $\Sigma_\epsilon$ is positive definite and thus has a square root $\Sigma_\epsilon^{1/2}$ satisfying $\Sigma_\epsilon = \Sigma_\epsilon^{1/2} \left(\Sigma_\epsilon^{1/2}\right)^{\top}$. Then, 
\begin{align*}
(y - f(\theta))^{\top} \Sigma_\epsilon^{-1}(y - f(\theta)) &= (y - f(\theta))^{\top} \Sigma_\epsilon^{-1/2} \left(\Sigma_\epsilon^{-1/2}\right)^{\top}(y - f(\theta)) \\
									   &= \left[\left(\Sigma_\epsilon^{-1/2}\right)^{\top} (y - f(\theta)) \right]^{\top} \left[\left(\Sigma_\epsilon^{-1/2}\right)^{\top} (y - f(\theta)) \right] \\
									   &= \norm{\left(\Sigma_\epsilon^{-1/2}\right)^{\top} (y - f(\theta))}_2^2
\end{align*}
so we see the weighted metric can be thought equivalently as applying the standard $L_2$ metric to the difference $y - f(\theta)$ that is first pre-multiplied by $\left(\Sigma_\epsilon^{-1/2}\right)^{\top}$. 

% Linear Gaussian Model
\section{The Linear Gaussian Model}
To include: 
\begin{itemize}
\item Notes from Youssef's class
\item Example 2.1 from Inverse Problems: A Bayesian Perspective and the following example. These examples consider the effect of noise-free models on the posterior, which depends on whether or not the system is over or under determined. 
\end{itemize}

The linear Gaussian model is a special case of a Bayesian inverse problem where the forward model is linear (i.e., $G(\theta) = G\theta$), the prior distribution on $\theta$ is Gaussian, and the likelihood is Gaussian. 
\begin{align}
y &= G\theta + \epsilon \label{linear_Gaussian} \\
\theta &\sim \mathcal{N}_D(\mu_0, \Sigma_0) \nonumber \\
\epsilon &\sim \mathcal{N}_N(0, \Sigma_\epsilon) \nonumber
\end{align}
Under these assumptions, the posterior $p(\theta|y)$ is available in closed-form, and is shown below to also be Gaussian. Before proceeding with the details, note that the posterior has the general form 
\begin{align*}
p(\theta|y) &\propto \mathcal{N}_N(y|G\theta, \Sigma_\epsilon)\mathcal{N}_D(\theta|\mu_0, \Sigma_0) \\
		&\propto \exp\left\{-\frac{1}{2} \norm{y - G\theta}^2_{\Sigma_\epsilon} - \frac{1}{2} \norm{\theta - \mu_0}^2_{\Sigma_0} \right\} \\
		&= \exp\left\{-J(\theta) \right\}
\end{align*}
where 
\begin{align} 
J(\theta) = \frac{1}{2} \norm{y - G\theta}^2_{\Sigma_\epsilon} + \frac{1}{2} \norm{\theta - \mu_0}^2_{\Sigma_0} \label{regularized_LS}
\end{align}
Since $p(\theta|y)$ is proportional to the exponential of a quadratic function of $\theta$ then we know it must be Gaussian. Writing the posterior in this form also provides a nice connection to the optimization approach to 
inverse problems, since \ref{regularized_LS} is a regularized least squares objective. Instead of minimizing this objective, the Bayesian approach converts it into a probability distribution via $\exp\left\{-J(\theta) \right\}$, thus 
allowing a more complete accounting of uncertainty. We now derive the mean and covariance matrix to complete the characterization of the posterior. 
\begin{thm}
The linear Gaussian model \ref{linear_Gaussian} has posterior 
\[p(\theta|y) = \mathcal{N}_D(\theta|\mu, \Sigma)\]
where 
\begin{align} 
\mu &= \Sigma \left[G^{\top} \Sigma_{\epsilon}^{-1}y + \Sigma_0^{-1} \mu_0 \right] \\
\Sigma &= \left[G^{\top} \Sigma_{\epsilon}^{-1} G + \Sigma_0^{-1} \right]^{-1}
\end{align}
\end{thm}
\begin{proof} 
As noted above, $p(\theta|y)$ is Gaussian, which means $J(\theta)$ must have the form 
\begin{align}
J(\theta) = \frac{1}{2} \norm{\theta - \mu}^2_{\Sigma} + C \label{posterior_structure}
\end{align}
where $C$ is a constant with respect to $\theta$. Perhaps the easiest way to derive $\mu$ and $\Sigma$ is to compare this expression to \ref{regularized_LS} and match up the terms. 
Expanding the latter expression yields 
\begin{align*}
J(\theta) &\propto \frac{1}{2} \theta^{\top} G^{\top} \Sigma_{\epsilon}^{-1} G\theta - y^{\top} \Sigma_{\epsilon}^{-1} G\theta + \frac{1}{2} \theta^{\top} \Sigma_0^{-1} \theta - \mu_0^{\top} \Sigma_0^{-1} \theta \\ 
&= \frac{1}{2} \theta^{\top} \left[G^{\top} \Sigma_\epsilon^{-1} G + \Sigma_0^{-1} \right] \theta - \left[y^{\top} \Sigma_{\epsilon}^{-1} G + \mu_0^{\top} \Sigma_0^{-1} \right] \theta
\end{align*}
Anything not dependent on $\theta$ is dropped in the proportionality symbol, as these terms are simply absorbed in the constant $C$.  
Now we similarly multiply out \ref{posterior_structure} and compare the two. 
\begin{align*}
J(\theta) &\propto \frac{1}{2} \theta^{\top} \Sigma^{-1} \theta - \mu^{\top} \Sigma^{-1} \theta
\end{align*}
Setting the two expressions equal and matching up the terms yields
\begin{align*}
\Sigma^{-1} &= G^{\top} \Sigma_{\epsilon}^{-1} G + \Sigma_0^{-1} \\
\mu^{\top} \Sigma^{-1} &= y^{\top} \Sigma_{\epsilon}^{-1} G + \mu_0^{\top} \Sigma_0^{-1}
\end{align*}
Inverting the first line yields the form of the posterior covariance matrix. For the second line, we take the transpose of each side, then multiple 
each side by $\Sigma$, yielding an expression for the posterior mean in terms of the posterior covariance matrix. 
\begin{align*}
\Sigma &= \left[G^{\top} \Sigma_{\epsilon}^{-1} G + \Sigma_0^{-1} \right]^{-1} \\
\mu &= \Sigma \left[G^{\top} \Sigma_{\epsilon}^{-1}y + \Sigma_0^{-1} \mu_0 \right]
\end{align*}

\end{proof} 

\subsection{Small Noise Limit}
We now parameterize the prior on the noise $\epsilon$ as $\epsilon \sim \mathcal{N}(0, \gamma^2 \Sigma_\epsilon)$ in order to study the effect on the posterior as the noise variance 
decreases ($\gamma \to 0$). This is the so-called \textit{small noise limit}. Under this new parameterization, the posterior moments become 
\begin{align*}
\Sigma &=  \left[\frac{1}{\gamma^2} G^{\top} \Sigma_{\epsilon}^{-1} G + \Sigma_0^{-1} \right]^{-1}  = \gamma^2 \left[G^{\top} \Sigma_{\epsilon}^{-1} G + \gamma^2 \Sigma_0^{-1} \right]^{-1} \\
\mu &= \left[G^{\top} \Sigma_{\epsilon}^{-1} G + \gamma^2 \Sigma_0^{-1} \right]^{-1}  \left[G^{\top} \Sigma_{\epsilon}^{-1}y + \gamma^2 \Sigma_0^{-1} \mu_0 \right]
\end{align*}

We follow Stuart et al and consider separately the overdetermined, underdetermined, and determined cases. Before proceeding, note the important fact that 
weak convergence of Gaussian distributions is equivalent to the convergence of their means and covariances. In particular, if the covariance matrix converges 
to the zero matrix, then the limiting distribution is a Dirac delta centered at the limiting mean. 

\subsubsection{Overdetermined Case}
We first consider the case $N > D$. This is the common regression scenario when there are more equations than unknowns ($G$ is skinny). This means that unless $y$ happens 
to lie in the column space of $G$ then the system $y = G\theta$ has no solution. Note that if $G$ has full column rank then the null space of $G$ is $\{0\}$. In this case,
we know there is a unique least-squares solution, and thus we would expect the posterior $p(\theta|y)$ to converge to this distribution as $\gamma \to 0^+$. This is indeed the case. 

\begin{thm}
Suppose that $N > D$ and $\text{null}(G) = \{0\}$. Then the posterior distribution $p(\theta|y)$ of the linear Gaussian model converges weakly to $\delta_{\theta_{\text{LS}}}$, where 
$\theta_{\text{LS}}$ is the solution to the regularized least squares problem
\[\theta_{\text{LS}} = \text{argmin}_{\theta \in \R^D} \norm{y - G\theta}_{\Sigma_0} \]
\end{thm}

\begin{proof} 
We show that $\Sigma \to 0$ and $\mu \to \theta_{\text{LS}}$. In light of the above comments on convergence of Gaussians, this implies $p(\theta|y) \to \delta_{\theta_{\text{LS}}}$. 
To show $\Sigma \to 0$, we prove that the eigenvalues of $\Sigma$ all converge to $0$. To this end, we first show that $\frac{1}{\gamma^2} G^{\top} \Sigma_{\epsilon}^{-1} G + \Sigma_0^{-1}$
is positive definite. Since $\Sigma_0^{-1}$ is positive definite and $\gamma > 0$, it suffices to show that $G^{\top} \Sigma_{\epsilon}^{-1} G$ is positive definite. To this end let $\theta \neq 0$
and consider 
\[\langle G^{\top} \Sigma_{\epsilon}^{-1}G \theta, \theta \rangle = \langle \Sigma_{\epsilon}^{-1/2}G \theta, \Sigma_{\epsilon}^{-1/2}G \theta \rangle = \norm{\Sigma_{\epsilon}^{-1/2}G \theta}_2^2 > 0 \]
The inequality follows from the assumption $\text{null}(G) = \{0\}$, which implies $G\theta \neq 0$, and the fact that $\Sigma_{\epsilon}^{-1/2}$ is positive definite, which implies 
$\Sigma_{\epsilon}^{-1/2}\left(G\theta \right) \neq 0$. This verifies that $\frac{1}{\gamma^2} G^{\top} \Sigma_{\epsilon}^{-1} G + \Sigma_0^{-1}$ is positive definite and hence has positive 
eigenvalues. Note that the eigenvalues converge to $+\infty$ as $\gamma \to 0^+$. Thus, the eigenvalues of $\Sigma = \left[\frac{1}{\gamma^2} G^{\top} \Sigma_{\epsilon}^{-1} G + \Sigma_0^{-1}\right]^{-1}$
converge to $0$ as $\gamma \to 0^+$. This verifies $\Sigma \to 0$. 

For the convergence of the mean, consider 
\begin{align*}
\mu &= \Sigma \left[\frac{1}{\gamma^2} G^{\top} \Sigma_{\epsilon}^{-1} y + \Sigma_0^{-1} \mu_0 \right] \\
       &= \frac{1}{\gamma^2} \Sigma G^{\top} \Sigma_{\epsilon}^{-1}y + \Sigma \Sigma_0^{-1} \mu_0 \\
       &= \left[G^{\top} \Sigma_{\epsilon}^{-1} G + \gamma^2 \Sigma_0^{-1}\right]^{-1} G^{\top} \Sigma_{\epsilon}^{-1} y + \Sigma \Sigma_0^{-1} \mu_0 
\end{align*}
As $\gamma \to 0^+$ we see that the second term will vanish since $\Sigma \to 0$. Moreover, $\gamma^2 \Sigma_0^{-1} \to 0$. Therefore, 
\[\lim_{\gamma \to 0^+} \mu = \left(G^{\top} \Sigma_{\epsilon}^{-1} G \right)^{-1} G^{\top} \Sigma_{\epsilon}^{-1} y\]
We recognize the limit as the solution to the regularized least-squares problem $\theta_{\text{LS}}$. 
\end{proof}
The takeaway from this result is that in the overdetermined case, if the quantity of noise is very small then the Bayesian solution of the inverse problem is very similar to the 
frequentist MLE approach. One can use this result to show that the posterior is consistent in the small-noise limit; in particular, the posterior concentrates around the true
$\theta$ as $\gamma \to 0^+$ (see Stuart et al, theorem 2.7). 

\subsubsection{Determined Case}
In the determined case $D = N$, $G$ is a square matrix. If we continue with the assumption $\text{null}(G) = \{0\}$ then $G$ is invertible and the system $y = G\theta$ has 
a unique solution given by $\theta^* = G^{-1}y$. In the small noise limit, we would hope that the posterior collapses on this solution; that is, 
$p(\theta|y) \overset{w}{\to} \delta_{G^{-1}y}$. This is indeed the case. 

\begin{thm}
Suppose that $D = N$ and $\text{null}(G) = \{0\}$. Then the posterior distribution of the linear Gaussian model satisfies
\[p(\theta|y) \overset{w}{\to} \delta_{G^{-1}y} \text{ as } \gamma \to 0^+\]
\end{thm}

\begin{proof}
Under the same exact reasoning as in the overdetermined case, $\Sigma \to 0$ as $\gamma \to 0^+$ (since this just relied on $G$ having a trivial null space). In fact, the 
same reasoning also holds for the mean convergence, so 
\[\lim_{\gamma \to 0^+} \mu = \left(G^{\top} \Sigma_{\epsilon}^{-1} G \right)^{-1} G^{\top} \Sigma_{\epsilon}^{-1} y\]
But in this case $G$ is invertible so 
\[\left(G^{\top} \Sigma_{\epsilon}^{-1} G \right)^{-1} G^{\top} \Sigma_{\epsilon}^{-1} y = G^{-1} \Sigma_{\epsilon} \left(G^{T}\right)^{-1} G^{\top} \Sigma_{\epsilon}^{-1} y = G^{-1}y\]
which proves the result. 
\end{proof}
Therefore, the prior again plays no role in the small-noise limit as in the overdetermined case. Similarly, the posterior can be shown to contract on the true value in this case. 

\subsubsection{Underdetermined Case}
In the underdetermined case $D > N$, $G$ is a ``fat'' matrix. In this setting, $G$ has a non-trivial null space, so if there exists a solution to the system $G\theta = y$ then there 
is entire subspace of solutions. In this case, once a particular solution is found the subspace can be obtained by adding vectors in the null space to the particular solution. 
We suppose that $G$ has full row rank, which implies that the columns span the co-domain so that there is indeed a subspace of solutions. It can be shown that 
\[\theta_{\text{LN}} := G^T (GG^T)^{-1}y\]
is the solution with minimal norm (i.e. the least-norm solution). The matrix $G^T(G G^T)^{-1}$ is a special \textit{right inverse} of $G$ known as the \textit{Moore-Penrose pseudoinverse}. 
Thus, every solution to the system $G\theta = y$ can be written in the form 
\[\theta^* = \theta_{\text{LN}} + \theta_{\text{null}}, \text{ where } \theta_{\text{null}} \in \text{null}(G)\]

We now return to the statistical setting. It is perhaps not as clear what will happen in the small-noise limit in this case. 
It turns out that in this case the prior plays a role in the limit. This makes a certain amount of sense; 
there is a subspace of solutions in the absence of noise, and the prior will favor some solutions in this subspace more than others. 

The QR decomposition will prove very useful in studying this case. The QR decomposition 
can be applied to matrices with at least as many rows as columns. Therefore, we will apply the QR decomposition to $G^{\top} \in \R^{D \times N}$, noting that 
the range of $G^{\top}$ has rank $N$. 
We recall that the (full) QR decomposition of $G^{\top}$ produces: 1.) an orthogonal matrix $Q_1 \in \R^{D \times N}$ with columns 
that form an orthonormal basis for the range of $G^{\top}$ (this can be formed by applying the Gram-Schmidt procedure to the columns of $G^{\top}$; and 2.) a matrix 
$Q_2 \in \R^{D \times (D - N)}$ with orthonormal columns which completes the basis for $\R^D$ (the columns of $Q_2$ are a basis for the null space of $G^{\top}$). 
Thus, the matrix $Q := \begin{pmatrix} Q_1 & Q_2 \end{pmatrix} \in \R^{D \times D}$ has columns that form an orthonormal basis for $\R^D$. We can relate $G^{\top}$ 
to $Q$ as 
\[
G^{\top} = QR = \begin{pmatrix} Q_1 & Q_2 \end{pmatrix} \begin{pmatrix} R_1 \\ 0 \end{pmatrix} = Q_1 R_1
\]
where $R_1 \in \R^{N \times N}$ is an upper triangular triangular matrix, which can be thought of as encoding the steps of the Gram-Schmidt procedure that produced 
$Q_1$. Since it is a bit annoying to be decomposing $G^{\top}$ instead of $G$, we can simply take transposes to obtain 
\[
G = R^{\top} Q^{\top} = \begin{pmatrix} R_1^{\top} & 0 \end{pmatrix}  \begin{pmatrix} Q_1^{\top} \\ Q_2^{\top} \end{pmatrix} = R^{\top}_1 Q^{\top}_1 
\]
Now we can define $L := R^{\top}$, $L_1 := R_1^{\top}$ so that 
\[
G = L Q^{\top} = \begin{pmatrix} L_1 & 0 \end{pmatrix}  \begin{pmatrix} Q_1^{\top} \\ Q_2^{\top} \end{pmatrix} = L_1 Q^{\top}_1 
\]
The QR decomposition yields a nice simple representation of the Moore-Penrose pseudoinverse. Indeed, 
\begin{align*}
\theta_{\text{LN}} = G^T \left(G G^T \right)^{-1}y &= Q_1 L_1^T \left(L_1 Q_1^{\top} Q_1 L_1^{\top} \right)^{-1} y \\
									 &= Q_1 L_1^T \left(L_1 I_N L_1^{\top} \right)^{-1} y \\
									 &= Q_1 L_1^{\top} \left(L_1^{\top} \right)^{-1} L_1^{-1} y \\
									 &= Q_1 L_1^{-1} y
\end{align*}
With this background established, we proceed with the main result. 

\begin{thm}
Suppose $D > N$ and $\text{rank}(G) = N$. Then in the small-noise limit $\gamma \to 0^+$, the posterior distribution of the linear Gaussian model satisfies
\[p(\theta|y) \overset{w}{\to} \mathcal{N}_D(\mu^+, \Sigma^+)\]
where 
\begin{align*}
\mu^+ &= \Sigma_0 Q_1\left(Q_1^\top \Sigma_0 Q_1 \right)^{-1} L_1^{-1} y + Q_2\left(Q_2^\top \Sigma_0^{-1} Q_2 \right)^{-1} Q_2^\top \Sigma_0^{-1} \mu_0 \\
\Sigma^+ &= Q_2 \left(Q_2^\top \Sigma_0^{-1} Q_2 \right)^{-1} Q_2^\top
\end{align*}
\end{thm}
Before proceeding to the proof, let's try to understand this result. Note that the first term in the expression for $\mu^+$ looks a lot like the least-norm solution 
from the non-statistical setting: $\theta_{\text{LN}} = Q_1 L_1^{-1} y$. The prior covariance $\Sigma_0$ complicates the expression but it certainly bears a close 
resemblance to the least-norm solution. The second term is the analog of $\theta_{\text{null}}$ discussed above. An important observation is that the limiting posterior 
covariance $\Sigma^+$ is not the zero matrix as in the previous cases; it depends on the prior covariance $\Sigma_0$ and the forward model $G$. In addition, this matrix 
is not full rank. Indeed, 
\[\text{rank}\left(\Sigma^+\right) = \text{rank}\left(Q_2 \left(Q_2^\top \Sigma_0^{-1} Q_2 \right)^{-1} Q_2^\top \right) = \text{rank}\left(Q_2 \right) = D - N < D\]
This implies that the posterior collapses on a subspace of dimension $N$, but uncertainty in the posterior remains in a subspace of dimension $D - N$; the 
precise form of this uncertainty depends on the prior. For the proof of the above theorem, see Stuart et al theorem 2.11. 


\section{Surrogate Models}
TODO: consider example in Teckentrup paper where forward model $f$ is emulated and results in approximate inverse problem 

\section{Quantifying Posterior Approximation Error}
In classical optimization-based inverse problems, we want to be able to measure the sensitivity of the solution $\hat{\theta}$ of the inverse problem to the observed data. When this sensitivity is high we might call the 
problem \textit{ill-posed}. It is of interest to be able to quantify the well-posedness of a problem, which can help guide practical applications. In the Bayesian setting, we no longer have a single point solution 
$\hat{\theta}$ to the inverse problem, but rather a distribution $\pi(\theta)$ summarizing the uncertainty in the true value of the parameter. However, such sensitivity analyses are still of considerable interest here. We now 
ask how the posterior distribution $\mu^y$ responds to changes in the observed data $y$. While the Bayesian approach is in general more robust to ill-posedness, sensitivity to observed data is still a concern. Moreover, if we further complicate things by replacing the true posterior with some approximation $\hat{\mu^y}$ (e.g. through the use of GP surrogates) then we also naturally want some measure of the approximation error between the true and approximate distributions. All of these questions point to the need for a notion of distance between probability measures. There is no one perfect metric that will apply to all problems, but we detail some common choices below. We introduce these distances using generic measures $\mu_1$ and $\mu_2$, and reference measure $\nu$, but one should keep in mind that we can think of $\mu_1$ as the true posterior 
$\mu^y$ and $\mu_2$ as some approximation of $\mu^y$, for example.  

\subsection{Total Variation Distance}
Total variation distance (TV) is a natural candidate to start with, and we will use it to motivate some of the other distances. The TV distance simply asks the question: what is the largest difference in probabilities that can occur when measuring the probability according to $\mu_1$ versus $\mu_2$? This is quite natural. One nice way to think about this is to sample from the 
two distributions. If $\pi_1$ and $\pi_2$ are very similar then we expect the samples generated from each distribution to be almost indistinguishable. On the other hand, if they are completely 
different then we might only need a few samples to distinguish between them with high confidence. The TV distance considers the ``worst-case'' event $A$, in the sense that $A$ is the event 
on which $\pi_1$ and $\pi_2$ disagree the most. Mathematically, the definition is given by 
\begin{align*}
\text{TV}(\pi_1, \pi_2) := \sup_{A} \abs{\mu_1(A) - \mu_2(A)}
\end{align*}
where the supremum is taken over all measurable sets $A$. Since $\mu_1(A)$ and $\mu_2(A)$ take values in $[0, 1]$ then 
\[0 \leq \text{TV}(\pi_1, \pi_2) \leq 1\]
If the distributions have densities $\pi_1$ and $\pi_2$ with respect to the Lebesgue measure $\lambda$ then this can be 
written 
\begin{align*}
\text{TV}(\pi_1, \pi_2) = \sup_{A} \abs{\int_A \left[\pi_1(\theta) - \pi_2(\theta)\right] d\theta }
\end{align*}
We can immediately establish a connection with $L_1$ distance. 
\begin{align*}
\text{TV}(\pi_1, \pi_2) &= \sup_{A} \abs{\int_A \left[\pi_1(\theta) - \pi_2(\theta)\right] d\theta } \\
				 &\leq \sup_{A} \int_A \abs{\pi_1(\theta) - \pi_2(\theta)} d\theta \\
				 &\leq \sup_{A} \int \abs{\pi_1(\theta) - \pi_2(\theta)} d\theta \\
				 &= \int \abs{\pi_1(\theta) - \pi_2(\theta)} d\theta \\
				 &= \norm{\pi_1 - \pi_2}_{L_1^\lambda}
\end{align*}
so the TV distance between the measures $\mu_1$ and $\mu_2$ is no larger than the $L_1$ distance between their densities $\pi_1$ and $\pi_2$. Trying to establish the opposite 
inequality is tricky since we need to bring the absolute value outside of the integral. The trick is to split the integral into two parts to avoid absolute values. To this end, let 
$B := \{\theta \in \mathcal{D}: \pi_1(\theta) \geq \pi_2(\theta)\}$. Then 
\begin{align*}
\norm{\pi_1 - \pi_2}_{L_1^\lambda} &= \int \abs{\pi_1(\theta) - \pi_2(\theta)} d\theta \\
						      &= \int_B \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta +  \int_B \left(\pi_2(\theta)  - \pi_1(\theta)\right) d\theta \\
						      &\leq \sup_A \int_A \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta + \sup_A \int_A \left(\pi_2(\theta)  - \pi_1(\theta)\right) d\theta \\
						      &\leq \sup_A \abs{\int_A \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta} + \sup_A \abs{\int_A \left(\pi_2(\theta)  - \pi_1(\theta)\right) d\theta} \\
						      &= 2 \sup_A \abs{\int_A \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta} \\
						      &= 2 \cdot \text{TV}(\pi_1, \pi_2)
\end{align*}
So we have shown 
\begin{align*}
\text{TV}(\pi_1, \pi_2) &\leq \norm{\pi_1 - \pi_2}_{L_1^\lambda} \\
\frac{1}{2} \norm{\pi_1 - \pi_2}_{L_1^\lambda} &\leq \text{TV}(\pi_1, \pi_2)
\end{align*}
This might make us wonder if the constant in the first inequality is $\frac{1}{2}$ so that we have the exact equivalence 
\begin{align*}
\text{TV}(\pi_1, \pi_2) &= \frac{1}{2} \norm{\pi_1 - \pi_2}_{L_1^\lambda}
\end{align*}
Indeed this is the case. To show this we will need to be a bit more careful with the first bound. To start, note that 
\begin{align*}
\int_\mathcal{D} \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta &= \int_B \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta + \int_{\mathcal{D} \setminus B} \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta \\
											     &=  \int_B \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta - \int_{\mathcal{D} \setminus B} \left(\pi_2(\theta) - \pi_1(\theta)\right) d\theta \\
\end{align*}
But the lefthand side is $0$ since $\pi_1$ and $\pi_2$ are densities, so 
\begin{align*}
\int_B \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta &= \int_{\mathcal{D} \setminus B} \left(\pi_2(\theta) - \pi_1(\theta)\right) d\theta
\end{align*}
a fact which we will exploit below. Now let $A$ be any measurable set. It follows that 
\begin{align*}
\abs{\int_A \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta} &= \max\left\{\int_A \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta, -\int_A \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta \right\} \\
										      &=  \max\left\{\int_A \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta, \int_A \left(\pi_2(\theta) - \pi_1(\theta)\right) d\theta \right\} \\
										      &\leq \max\left\{\int_{A \cap B} \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta, \int_{A \cap (\mathcal{D} \setminus B)} \left(\pi_2(\theta) - \pi_1(\theta)\right) d\theta \right\} \\
										      &\leq \max\left\{\int_{B} \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta, \int_{\mathcal{D} \setminus B} \left(\pi_2(\theta) - \pi_1(\theta)\right) d\theta \right\} \\
										      &= \int_{B} \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta \\
										      &= \frac{1}{2} \left[\int_{B} \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta + \int_{\mathcal{D} \setminus B} \left(\pi_2(\theta) - \pi_1(\theta) \right) d\theta \right] \\
										      &= \frac{1}{2} \left[\int_{B} \abs{\pi_1(\theta) - \pi_2(\theta)} d\theta + \int_{\mathcal{D} \setminus B} \abs{\pi_1(\theta) - \pi_2(\theta)} d\theta \right] \\
										      &= \frac{1}{2} \int_{\mathcal{D}} \abs{\pi_1(\theta) - \pi_2(\theta)} d\theta \\
										      &= \frac{1}{2} \norm{\pi_1 - \pi_2}_{L_1^{\lambda}(\mathcal{D})}
\end{align*}
The first inequality follows from the fact that intersecting the domain of integration of the first integral with $B$ removes values of $\theta$ which results in a negative integrand, and hence the 
integral gets larger. The same reasoning applies to the second integral. The fifth and seventh lines both use the fact that was proved above. This inequality holds for every measurable set $A$ and thus 
\begin{align*}
\text{TV}(\pi_1, \pi_2) &= \sup_A \abs{\int_A \left(\pi_1(\theta) - \pi_2(\theta)\right) d\theta} \leq \frac{1}{2} \norm{\pi_1 - \pi_2}_{L_1^{\lambda}(\mathcal{D})}
\end{align*}
which establishes the reverse inequality and thus verifies 
\begin{align*}
\text{TV}(\pi_1, \pi_2) &= \frac{1}{2} \norm{\pi_1 - \pi_2}_{L_1^\lambda}
\end{align*}
Recalling that $\text{TV}(\pi_1, \pi_2)$ takes values in $[0, 1]$, then we see that 
\[0 \leq \norm{\pi_1 - \pi_2}_{L_1^\lambda} \leq 2\]
so the two distances $\text{TV}(\pi_1, \pi_2)$ and $\norm{\pi_1 - \pi_2}_{L_1^\lambda}$ are measuring the same exact thing, just on a different scale. 

To review, we started with the very natural definition of TV distance as the largest possible difference in probabilities that the two measures could assign to any given set $A$. We then showed that this notion could be equivalently defined (up to a multiplicative constant) as the $L_1$ distance between the densities of the measures. One way to see that this makes sense is to consider that the density functions $\pi_1$ and $\pi_2$ are unit vectors with respect to $\norm{\cdot}_{L_1^\lambda}$ (since they have to integrate to $1$ by definition). Therefore, the distance 
$\norm{\pi_1 - \pi_2}_{L_1^\lambda}$ is not affected by ``size'' of $\pi_1$ or $\pi_2$. We would not, for example, want to calculate the $L_1$ distance between unnormalized densities, since arbitrarily scaling the densities would change the notion of distance, even though the underlying probability distribution would remain the same. 


\subsection{Hellinger Distance}
The Hellinger distance is perhaps the most popularly used measure of distance between probability distributions in the Bayesian inverse problem literature, for reasons that will be discussed below. I will motivate this motion of distance using the $\text{TV}$/$L_1$ notions discussed above. We established that $L_1$ notions of distance between probability densities yields a reasonable interpretation of the distance between two probability measures. It is natural to wonder: would replacing $L_1$ with $L_2$ also give us something useful? In short, not really. As mentioned in the previous section, the $L_1$ notion was reasonable since $\pi_1$ and $\pi_2$ are unit vectors with respect to $L_1$. However, $\pi_1$ and $\pi_2$ are \textit{not} unit vectors with respect to $L_2$. We see that 
\[\norm{\pi_1 - \pi_2}_{L_2^\lambda} = \int (\pi_1(\theta) - \pi_2(\theta))^2 d\theta = \int \pi_1^2(\theta) d\theta + \int \pi_2^2(\theta) d\theta - 2\int \pi_1(\theta)\pi_2(\theta) d\theta\]
so changes in the first two terms will affect the distance, which is not a desirable property. But what if we just dropped those terms and considered a our notion of distance to be the inner product 
\[- 2\int \pi_1(\theta)\pi_2(\theta) d\theta\]
But this still doesn't make sense at all. Consider the two cases
\begin{enumerate}
\item $\pi_1(0) = \frac{1}{10} \text{ and } \pi_2(0) = \frac{2}{10}$
\item $\pi_1(0) = \frac{9}{10} \text{ and } \pi_2(0) = 1$
\end{enumerate}
If we sampled from these distributions, then in the first case we would expect to observe the sample $0$ about twice as much from the second distribution compared to the first, while $0$ would appear with pretty similar frequency in the second example. The relevant terms in the inner product would be $\frac{1}{10} \cdot \frac{2}{10} = \frac{2}{100}$ and $\frac{9}{10} \cdot 1 = \frac{90}{100}$, so we would actually be \textit{penalizing} the first distribution for being closer, the opposite of what we want. To flip this around, it seems we would actually want to consider the quotient $\frac{\pi_1(\theta)}{\pi_2(\theta)}$ rather than the product of the densities, which is exactly what many notions of distance between probability measures considers (e.g. KL divergence). Obviously considering the quotient instead abandons the idea of $L_2$ distance. However, it turns out that with a slight tweak we can still define an $L_2$ measure that actually makes sense. 
\textbf{Also discuss difference vs quotient wrt normalizing constants:} e.g. $\abs{\frac{\pi_1(\theta)}{Z_1} - \frac{\pi_2(\theta)}{Z_2}}$ (norm constants affect results). vs. 
$\abs{\frac{\frac{\pi_1(\theta)}{Z_1}}{\frac{\pi_2(\theta)}{Z_2}}} = \frac{Z_2}{Z_1} \frac{\pi_1(\theta)}{\pi_2(\theta)}$ so norm constants will just scale the integral which is fine. 


% Ensemble Kalman Inversion
\section{Ensemble Kalman Inversion}

% Appendix
\section{Appendix}

\subsection{Linear Least Squares Optimization}
In this section, we characterize the optimum of a regularized linear least squares optimization problem. This calculation is provided here as it is used in many different settings; the same calculation is also required to characterize 
the posterior distribution in a linear Gaussian inverse problem and to derive the analysis step of the Kalman filter. With this latter application in mind, we also write the below result in terms of the \textit{Kalman gain}, which has 
relevance to inverse problems via their solution by ensemble Kalman inversion techniques. 

\begin{thm}
Consider the regularized linear least squares objective 
\begin{align}
\frac{1}{2} \norm{y - G\theta}^2_{\Sigma_\epsilon} + \frac{1}{2} \norm{\theta - \mu_0}^2_{\Sigma_0} \label{LLS_obj}
\end{align}
Define the Kalman gain $K = \Sigma_0 G^\top (G \Sigma_0 G^\top + \Sigma_{\epsilon})^{-1}$. Then, 
\begin{enumerate}
\item \ref{LLS_obj} can be written as 
\begin{align}
\norm{\theta - \mu}^2_\Sigma + c \label{LLN_post}
\end{align}
where $c$ is a constant with respect to $\theta$ and 
\begin{align*}
\Sigma &= \left[G^\top \Sigma^{-1}_{\epsilon} G + \Sigma^{-1}_0\right]^{-1} \\
\mu &= \Sigma \left[G^\top \Sigma^{-1}_{\epsilon}y + \Sigma_0^{-1} \mu_0  \right]
\end{align*}

\item The above formulas can alternatively be written in terms of the Kalman gain as 
\begin{align*}
\Sigma &=  (I - KG)\Sigma_0 \\
\mu &= Ky + (I - KG) \mu_0
\end{align*}

\item The Kalman gain can alternatively be written as 
\[K = \Sigma G^\top \Sigma_{\epsilon}^{-1} \]
\end{enumerate}
\end{thm}

\begin{proof} 
\begin{enumerate}
\item For the first result, we simply expand the expressions \ref{LLS_obj} and \ref{LLN_post} and equate their terms. To this end, we have
\begin{align*}
\ref{LLS_obj} &\propto \frac{1}{2} \theta^\top (G^\top \Sigma^{-1}_\epsilon G) \theta - \theta^\top G^\top \Sigma_\epsilon^{-1} y + \frac{1}{2} \theta^\top \Sigma_0^{-1} \theta - \theta^\top \Sigma_0^{-1} \mu_0 \\
	             &= \frac{1}{2} \theta^\top [G^\top \Sigma_\epsilon^{-1} G + \Sigma_0^{-1}]\theta - \theta^\top [G^\top \Sigma_{\epsilon}^{-1}y + \Sigma_0^{-1}\mu_0] \\
\ref{LLN_post} &\propto \frac{1}{2} \theta^\top \Sigma^{-1} \theta - \theta^\top \Sigma^{-1} \mu + \frac{1}{2} \mu^\top \Sigma^{-1} \mu 	           
\end{align*}
Setting the coefficients of the respective terms equal yields
\begin{align*}
\Sigma^{-1} &= G^\top \Sigma_{\epsilon}^{-1}G + \Sigma_0^{-1} \\
\Sigma^{-1} \mu &= G^\top \Sigma_{\epsilon}^{-1}y + \Sigma_0^{-1} \mu_0
\end{align*}
which implies the desired formulas. 

\item The alternative expression for $\Sigma$ is derived by applying the Woodbury matrix inversion identity to the above expression for $\Sigma$. Indeed, 
\begin{align*}
\Sigma = \left[G^\top \Sigma_{\epsilon}^{-1}G + \Sigma_0^{-1} \right]^{-1} &= \Sigma_0 - \Sigma_0 G^\top (\Sigma_\epsilon + G \Sigma_0 G^\top)^{-1} G\Sigma_0 \\
													       &= \Sigma_0 - KG\Sigma_0 \\
													       &= (I - KG) \Sigma_0 
\end{align*}

For the $\mu$ expression I use $K = \Sigma G^\top \Sigma_{\epsilon}^{-1}$, which will be verified below. We have 
\begin{align*}
\mu &= \Sigma\left[G^\top \Sigma_{\epsilon}^{-1}y + \Sigma_0^{-1} \mu_0\right] \\
       &= \Sigma G^\top \Sigma_{\epsilon}^{-1}y + \Sigma \Sigma_0^{-1} \mu_0 \\
       &= Ky + \Sigma \Sigma_0^{-1} \mu_0 \\
       &= Ky + (I - KG)\Sigma_0  \Sigma_0^{-1} \mu_0 \\
       &= Ky + (I - KG)\mu_0
\end{align*}
where the penultimate line applies the result $\Sigma = (I - KG)\Sigma_0$ just proved above. 

\item Finally, we verify $K = \Sigma G^\top \Sigma_{\epsilon}^{-1}$. To show this, we use the identity 
\[G^\top \Sigma^{-1}_{\epsilon} \left[G \Sigma_0 G^\top + \Sigma_\epsilon \right] = \left[\Sigma_0^{-1} + G^\top \Sigma^{-1}_{\epsilon}G \right] \Sigma_0 G^\top \]
To see the equality simply distribute the expressions on both sides. Under the assumption that $G$ has full column rank then the expressions in brackets on each side 
of the equation are both invertible. Multiplying through by their inverses yields, 
\begin{align}
\left[\Sigma_0^{-1} + G^\top \Sigma_{\epsilon}^{-1} G \right]^{-1} G^\top \Sigma_{\epsilon}^{-1} = \Sigma_0 G^\top \left[G \Sigma_0 G^\top + \Sigma_{\epsilon} \right]^{-1} \label{gain_identity}
\end{align}
With this identity in hand, we then have 
\begin{align*}
K &= \Sigma_0 G^\top \left(G\Sigma_0 G^\top + \Sigma_\epsilon \right)^{-1} && \text{Definition} \\
   &= \left[\Sigma_0^{-1} + G^\top \Sigma_{\epsilon}^{-1} G \right]^{-1} G^\top \Sigma_{\epsilon}^{-1} &&\ref{gain_identity} \\
   &= \Sigma G^\top \Sigma_{\epsilon}^{-1}
\end{align*}

\end{enumerate}
\end{proof}




\end{document}
