\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Kernel Smoothing Methods}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Kernel Density Estimation}
Consider the following problem: we have $N$ iid samples from a univariate distribution $X_i \sim \mu$ and would like to estimate the density function (with respect to Lebesgue measure) $\pi$ associated 
with the distribution $\mu$. This is a non-parametric problem, in contrast to the approach of assuming that the density $\pi$ belongs to some parameterized family (e.g. Gaussian) and then 
estimating the parameters of that family. How might we go about this? Well, we know that the density is the derivative of the distribution function
\[\pi(x) = F^\prime(x)\]
and also that the distribution function can be represented as an expectation 
\[F(x) = \Prob(X \leq x) = \E \left[\mathbbm{1}\{X \leq x\}\right].\]
This latter fact implies that we can compute a Monte Carlo estimate of $F(x)$
\[\hat{F}_N(x) := \frac{1}{N} \sum_{i = 1}^{N} \mathbbm{1}\{X_i \leq x\}\]
which is unbiased and has nice limited results (law of large numbers, central limit theorem). Note that $\hat{F}_N(x)$ is simply the count of the number of samples $X_i$ that lie below the threshold 
$x$. Since we can approximate the distribution function, we might try to estimate the density by approximating the derivative of the distribution function (adding a second layer of approximation). Consider the approximation 
\[F^\prime(x) \approx \frac{F(x + h) - F(x)}{h}\] 
for some $h > 0$. Clearly, in general this approximation will be better as $h$ gets closer to zero, but we will see that practically the choice of $h$ is limited by how much data we have. Let us now combine these two approximations 
to yield an estimate of the density at some value $x$.
\begin{align*}
\pi(x) = F^\prime(x) &\approx  \frac{F(x + h) - F(x)}{h} \\
			     &\approx \frac{\hat{F}_N(x + h) - \hat{F}_N(x)}{h} \\
			    &= \frac{1}{Nh} \sum_{i = 1}^{N} \left[\mathbbm{1}\{X_i \leq x + h\} - \mathbbm{1}\{X_i \leq x\} \right]
\end{align*}
Note that the summation simply counts the number of samples $X_i$ contained in the interval $[x, x + h]$. There are a couple of glaring issues with this approach. First of all, it doesn't seem to make a lot of sense to only 
consider samples in $[x, x + h]$. Really we should be considering samples contained in some interval centered at $x$. Thus, let's re-define $h$ to be the width of this interval (i.e. the interval $\left[x - \frac{h}{2}, x + \frac{h}{2}\right]$ 
and denote by $N_h(x)$ the number of points in this interval. Then the estimate becomes 
\[\hat{\pi}_N(x) := \frac{N_h(x)}{Nh}\]
Intuitively, $\frac{N_h(x)}{N}$ is a proportion that represents an estimate of the change in probability mass from $x - \frac{h}{2}$ to $x + \frac{h}{2}$; that is, an estimate of the change in the distribution function $F$ around $x$. And $h$ 
represents in small change in $x$. Thus, $\hat{\pi}_{N}(x)$ is simply a crude estimate $\frac{\Delta F}{\Delta x}$ of the derivative $F^\prime(x)$. If we consider the extreme where $F^\prime(x) = 0$ then this means there is no probability 
mass locally at $x$, and hence we don't expect any samples locally around $x$. In this case $\Delta F = \hat{\pi}_N(x) = 0$ so the estimate of the density is $0$, as desired. Aside from the need to select the width of the interval $h$, the major 
weakness of this estimator is that it does not provide a continuous estimator of $\pi(x)$. Consider increasing $x$ and thinking about how the estimate $\hat{\pi}_N(x)$ will vary. The estimate will be constant as long as the $h$-length interval 
contains the same samples. As soon as this interval collects a new point or passes over a sample it previously contained, the estimate $\hat{\pi}_N(x)$ will typically make a discrete jump. This is not desirable in the common case where the 
true density is continuous, and moreover can result in a higher variance estimator.

The idea of KDE is thus to adjust this estimator by considering more of a smoothed average in the changes in the number of samples as $x$ increases. To this end, let $K_h(\cdot; x)$ be some continuous function in the first argument. 
Previously, we centered an interval on $x$ that defined a sharp cut-off that either included or excluded each sample $x_i$. On the other hand, $K_h(x_i; x)$ provides some sort of smooth notion of the distance of the samples $x_i$ from $x$. 
Replacing $N_h(x)$ in the previous estimator (which was a sum over indicator functions) with a sum over the continuous functions $K_h(x_i; x)$ yields the new generic estimator 
\[\hat{\pi}_N(x) = \frac{1}{Nh} \sum_{i = 1}^{N} K_h(x_i; x)\]
Since this represents a sum of continuous functions, then the resulting estimate of the density will be continuous. 
While we have not yet considered a specific form for $K_h(\cdot; x)$, we can think of $h$ as analogous to the width of the interval in the discontinuous estimator. 

\end{document}


