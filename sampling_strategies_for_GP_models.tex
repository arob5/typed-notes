\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}


\begin{document} 

\begin{center}
\Large
Sampling Strategies for Gaussian Process Models
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

Elliptical slice sampling is a variant of slice sampling that is specialized to sampling from distributions with Gaussian priors; that is, distributions of the form 
\[\pi(f) \propto \mathcal{N}_M(f|0, \Sigma)\mathcal{L}(f)\]
where $\mathcal{L}(f) = p(\text{Data}|f)$ is a likelihood function. We have changed notation to let $f \in \R^M$ denote the vector of parameters we wish to sample from, given that elliptical slice sampling 
is motivated by sampling function values in a Gaussian Process (GP) model. We elaborate on this motivation below. 

\section{Motivation: Gaussian Process Regression}
Consider a standard GP regression model
\begin{align*}
y_i &= f(x_i) + \epsilon_i, \ i = 1, \dots, N \\
f(\cdot) &\sim \mathcal{GP}(0, k(\cdot, \cdot)) \\
\epsilon_i &\overset{iid}{\sim} N(0, \sigma^2)
\end{align*}
where $y \in \R^N$ is a vector of observed data at inputs $X \in \R^{N \times d}$. I will denote the vector of function evaluations at the training points by $f := f(X) \in \R^N$. 
We are ultimately interested in posterior inference over  $\tilde{f} := f(\mathcal{X}) \in \R^M$, the latent function values at unobserved inputs $\mathcal{X} \in \R^{M \times d}$.
There are two primary tasks that are of central importance in such GP models 
\begin{enumerate}
\item Determining the kernel parameters used to define $k(\cdot, \cdot)$, which is often achieved my maximizing the marginal likelihood $p(y|X)$. 
\item Performing posterior inference for $p(\tilde{f}|X, y)$.
\end{enumerate}
For the former task, the marginal log-likelihood is given by the integral
\[p(y|X) = \int p(y|f)p(f) df\] 
which is ``marginal'' in the sense that its computation requires marginalizing over $f$. For the second task, the posterior is given by the integral
\[p(\tilde{f}|X, y) = \int p(\tilde{f}|f)p(f|y) df\]
which again requires integrating out $f$, this time with respect to the posterior over the function values at the training points $p(f|y)$. In the regression model presented above, with 
a GP prior and Gaussian likelihood, all of these computations admit closed-form solutions. Indeed, for the marginal likelihood we have 
\[p(y|X) = \int \mathcal{N}_N(y|f, \sigma^2I_N)N_{N}(f|0, k(X)) df\] 
which results in a Gaussian integrand and an analytical solution. Similarly, for the posterior over $\tilde{f}$, 
\[p(\tilde{f}|X, y) = \int p(\tilde{f}|f)p(f|y) df \qquad (\star)\]
$p(\tilde{f}|f)$ is a conditional of a multivariate Gaussian and hence Gaussian, and $p(f|y) \propto p(y|f)p(f) = \mathcal{N}_N(y|f, \sigma^2 I_N)\mathcal{N}_N(f|0, k(X))$ and is 
hence also Gaussian. Therefore, there seems to be no immediate need for sampling procedures, since everything is essentially Gaussian and available in closed form. However, if we
move away from the Gaussian likelihood in the regression model, the analytical tractability is broken. We thus motivate elliptical slice sampling by the need to approximate the integral 
$(\star)$ in the non-Gaussian likelihood setup. In particular, we note that this integral can be approximated using a Monte Carlo approach by drawing samples from the joint
\[p(\tilde{f}, f|X, y) \propto p(f, \tilde{f}) p(y|f) = \mathcal{N}_{N + M}\left(\begin{pmatrix} f \\ \tilde{f}\end{pmatrix} \bigg|0, k\begin{pmatrix} X \\ \mathcal{X}\end{pmatrix}\right)p(y|f)\]
 and then extracting the marginal samples from $\tilde{f}$. 
 We see that this is a particular example of the distribution we said slice sampling would consider (to make this explicit, note that the $f$ vector in the original problem definition 
 is equivalent to the vector $\begin{pmatrix} f \\ \tilde{f} \end{pmatrix}$ in this example). 

\subsection{More motivation: Gaussian Process Classification}
Next we consider a more complicated GP model that further motivates the need for fast sampling schemes. We now consider a binary classification task with observed targets 
$y_i \in \{0, 1\}$. We now define the function $f: \R^d \to \R$ to give the log-odds ratio
\[f(x_i) = \log \frac{\Prob(y_i = 1)}{1 - \Prob(y_i = 1)} = \text{logit}(\Prob(y_i = 1))\]
so unlike in the above regression example the function values $f_i$ are not observed, even in the training set. Inverting the logit function gives the probability of the output equaling $1$
\[p_i := \Prob(y_i = 1) = \frac{e^{f(x_i)}}{1 + e^{f(x_i)}}\]
Considering a Bernoulli likelihood, the model is thus
\begin{align*}
y_i|f_i &\overset{ind}{\sim} \text{Bern}\left(\frac{e^{f(x_i)}}{1 + e^{f(x_i)}}\right), \ i = 1, \dots, N \\
f(\cdot) &\sim \mathcal{GP}(0, k(\cdot, \cdot))
\end{align*}
What we are really interested in the class probabilities 
\[\tilde{p}_i := p(\tilde{y}_i = 1|\tilde{x}_i) = \frac{e^{f(\tilde{x}_i)}}{1 + e^{f(\tilde{x}_i)}}\]
at test inputs $\tilde{x}_i, i = 1, \dots, M$ collected in the matrix $\mathcal{X} \in \R^{M \times d}$. Thus, the vector of log-odds ratios $\tilde{f} \in \R^M$ at the unobserved 
points $\mathcal{X}$ is really a nuisance parameter here (unlike in the regression case, where $\tilde{f}$ was the main object of inference). The main object of inference is 
now $\tilde{p} \in \R^M$, the class probabilities. Note that the GP prior on $f(\cdot)$ induces 
a prior on the class probability function $p(\cdot)$, though the latter will not be Gaussian due to the fact that $\tilde{p}_i$ is a non-linear function of $\tilde{f}_i$. The posterior of interest $p(\tilde{p}|X, y)$ now requires a two-step inference procedure:
\begin{enumerate}
\item Obtain the posterior over $\tilde{f}$:
\[p(\tilde{f}|X, y) = \int p(\tilde{f}|f)p(f|y) df\]
where $p(\tilde{f}|f)$ is Gaussian but $p(f|y) \propto p(y|f)p(f)$ is not ($p(y|f)$ is Bernoulli). 
\item Utilize $p(\tilde{f}|X, y)$ from the above step to obtain $p(\tilde{p}|X, y)$. \\[.2cm]
If sampling is used, this simply means transforming a sample of $\tilde{f}$ into a sample of $\tilde{p}$ via
\[\tilde{p} = \frac{e^{\tilde{f}}}{1 + e^{\tilde{f}}}\]
\end{enumerate}
Neither of these steps admit closed-form solutions, so either analytical or stochastic (sampling) approximations are necessary. Focusing on sampling-based approaches, the 
distribution $p(\tilde{f}|X, y)$ in step one can be approximated by drawing samples from the joint posterior $p(\tilde{f}, f|X, y)$ and then extracting the $\tilde{f}$ samples to 
marginalize over $f$. In particular, we can sample from this joint via the following two-step procedure: 
\begin{itemize}
\item Sample $f^\prime \sim p(f|X, y)$
\item Sample $\tilde{f}^\prime \sim p(\tilde{f}|f^\prime)$
\end{itemize}
which can be repeated and then the samples of $\tilde{f}$ are returned (the sampling procedure has approximated the marginalization over $f$). 
For step two, the samples from $p(\tilde{f}|X, y)$ can simply be transformed by applying the inverse logit to obtain samples from $p(\tilde{p}|X, y)$. Finally, note 
that this procedure is marginalizing over the latent function values $f$ associated with the \textit{observed} responses $y$. Performing inference over
latent values $\tilde{f}$ at input unobserved input locations requires averaging over the uncertainty in latent values for the observed responses. 

\section{Sampling Strategies for Gaussian Processes}
In both of the above examples, inference for GPs boils down to essentially the same problem: sampling from $p(f|y)$, the posterior distribution over function values
(technically we should write $p(f|X, y)$ but I suppress the $X$ to lighten notation). In the standard GP regression problem with independent Gaussian noise, $p(f|y)$
was available in closed-form. However, analytical calculations were broken when non-Gaussian noise was considered. Moreover, in the GP classification example, this 
distribution is likewise unavailable in closed form. Note that in the classification problem, we detailed a two-step sampling procedure to sample from 
$p(\tilde{f}, f|X, y)$. Sampling from $p(f|y)$ (the first step) is the challenging step of this procedure, since the second step just requires sampling from the Gaussian 
$p(\tilde{f}|f^\prime)$. In general, we have access to the unnormalized density
\[p(f|y) \propto p(y|f)p(f) = p(y|f)\mathcal{N}_N(f|0, K)\]
where $K$ is the $N \times N$ matrix given by $(K)_{ij} := k(x_i, x_j)$. In general, if the likelihood $p(y|f)$ is non-Gaussian, then the posterior $p(f|y)$ will not take the 
form of some well-known distribution. For example, in the classification example, $p(f|y)$ is a product over Bernoullis:
\begin{align}
p(f|y) &\propto \mathcal{N}_N(f|0, K) \prod_{i = 1}^{N} p_i^{y_i}(1 - p_i)^{1 - y_i} \label{logistic_post} \\
	 &= \mathcal{N}_N(f|0, K) \prod_{i = 1}^{N} \left(\frac{e^{f_i}}{1 + e^{f_i}}\right)^{y_i}\left(\frac{1}{1 + e^{f_i}}\right)^{1 - y_i} \nonumber \\
	 &= \mathcal{N}_N(f|0, K) \prod_{i = 1}^{N} \frac{e^{f_i y_i}}{1 + e^{f_i}} \nonumber
\end{align}
There are a few issues that make sampling from $p(f|y)$ difficult. First of all, $f \in \R^N$ and the number of observations $N$ can be quite large. Second, the distribution 
$p(f|y)$ will often be quite correlated, which poses problems for many standard sampling schemes. This correlation is often stemming from correlations in the prior
$\mathcal{N}_N(f|0, K)$, reflecting correlations that encode the smoothness of the latent function. We will begin by discussing why two standard MCMC schemes--Gibbs sampling
and Metropolis-Hastings--might run into trouble here, and then proceed to explore more sophisticated alternatives.  

\subsection{Gibbs Sampling.} A Gibbs sampling approach would involve cycling through the conditional posteriors $p(f_i|y, f_{-i})$, sampling from them one at a time. 
Here, $f_{-i}$ is shorthand for $f_1, \dots, f_{i - 1}, f_{i + 1}, \dots, f_N$ (i.e. all of the latent function values are fixed at their current values, except for $f_i$). So the question is:
can we sample from these conditional posteriors? First, a couple observations. Note that the conditional posterior is proportional to the posterior itself, since
\[p(f_i|y, f_{-i}) \propto p(y|f_i, f_{-i})p(f_i, f_{-i}) = p(y|f)p(f)\]
Thus, a strategy for determining $p(f_i|y, f_{-i})$ (up to a constant) is to take the density $p(f|y)$ and drop all terms without dependence on $f_i$. To this end, let's once again 
take a look at $p(f|y)$. Assuming the likelihood factorizes, then this density takes the form 
\begin{align*}
p(f|y) &\propto \mathcal{N}_N(f|0, K) \prod_{j = 1}^{N} p(y_j|f_j) \\
	 &\propto \exp\left\{-\frac{1}{2} f^T K^{-1} f \right\} \prod_{j = 1}^{N} p(y_j|f_j) \\
	 &= \exp\left\{-\frac{1}{2} \sum_{j,l = 1}^{N} f_j f_l \left(K^{-1}\right)_{jl} \right\} \prod_{j = 1}^{N} p(y_j|f_j) \\
	 &= \prod_{j = 1}^{N} \prod_{l = 1}^{N} \exp\left\{-\frac{1}{2} f_j f_l \left(K^{-1}\right)_{jl} \right\} \cdot \prod_{j = 1}^{N} p(y_j|f_j) \\
\end{align*}
The density (\ref{logistic_post}) for the classification model is a specific example of this form, where the likelihood factorized into a product of Bernoulli densities. We expanded 
the Gaussian density in a product in the interest of dropping terms that do not depend on $f_i$, as mentioned before. By dropping these terms, we obtain 
\begin{align*}
p(f_i|y, f_{-i}) &\propto \exp\left\{-\frac{1}{2}f_i^2 \left(K^{-1}\right)_{ii} \right\} \prod_{j \neq i} \exp\left\{-f_i f_j \left(K^{-1}\right)_{ij} \right\} \cdot p(y_i|f_i) \\
		    &= \exp\left\{-\frac{1}{2} f_i^2 \left(K^{-1}\right)_{ii} - \sum_{j \neq i} f_i f_j \left(K^{-1}\right)_{ij} \right\} \cdot p(y_i|f_i)
\end{align*}
which uses the fact that the precision matrix $K^{-1}$ is symmetric. 
\textbf{TODOs:}
\begin{itemize}
\item Show above conditional posterior is log-concave. 
\item Discuss potential for adaptive rejection algorithm of Gilks and Wild (1992). 
\end{itemize} 

\subsection{Random Walk Metropolis and the pCN Algorithm}
We now consider Metropolis-Hastings algorithms for sampling from GPs. Designing proposal distributions for this task is a tricky business. This challenge stems from the 
fact that the underlying quantity of interest (the latent function) is an infinite-dimensional object. We sample at a finite set of points that represent a discretization of the infinite-dimensional smooth function. One would therefore hope that as we increase the number of sampling locations--thus creating a higher-fidelity discretization--that inference over the latent function would improve, approaching inference over the true infinite-dimensional object in some sense. In reality, increasing the number of input locations often degrades the MCMC performance. Since the underlying function is smooth, then its function values are typically highly correlated, and finer discretization only amplify this correlation. Since highly-correlated distributions can prove a challenge for MCMC, the performance of the sampler will tend to decline when using standard MCMC algorithms. It is therefore necessary to consider modifications to such standard algorithms that are better adapted to this setting. 

Let's begin by intuitively thinking about what a good proposal would look like. The GP prior imposes quite a lot of structure on the posterior distribution; in particular, the posterior functions will satisfy the smoothness requirements encoded by the kernel used in the GP prior. Therefore, we would hope that proposals in an MCMC scheme would satisfy these smoothness requirements. A simple way to achieve this is to simply draw proposals from the prior itself, resulting in an independent Metropolis scheme. While the proposals will have the correct smoothness properties, independent Metropolis is likely to be incredibly inefficient in this setting. On the other hand, random walk Metropolis makes local proposals, which can be better-informed but may not always satisfy the smoothness requirements. It would therefore be desirable to design a proposal that produces functions with the correct smoothness while also achieving a decent acceptance rate. 

\subsubsection{The pCN Proposal}
To motivate what Cotter et al (2013) refer to as the \textit{pCN} algorithm, let's first consider a simple random walk proposal: 
\[\mathbf{f}^{(t + 1)} = \mathbf{f}^{(t)} + \beta z^{(t)}, \text{ where } z^{(t)} \sim N(0, \mathbf{K}),\]
or equivalently, 
\[\mathbf{f}^{(t + 1)}|\mathbf{f}^{(t)} \sim N(\mathbf{f}^{(t)}, \beta^2 \mathbf{K}) \]
The step size is determined by the scalar $\beta$, which scales the prior covariance $\mathbf{K}$. 

\section{References:}
\begin{itemize}
\item MCMC Algorithms for Gaussian Processes (Titsias, et al)
\item MCMC Methods for Functions: Modifying Old Algorithms to Make Them Faster (Cotter, et al)
\end{itemize}

\bigskip
\noindent
\textbf{Metropolis-Hastings.}
\begin{itemize}
\item Sampling from prior, why this is desirable and also potentially an issue (see Neil Lawrence paper).
\item Sampling from prior but with smaller variance. 
\item Alternative proposal mentioned in Neal and in elliptical slice sampling paper. Is this related to overrelaxation? 
\item De-correlated prior (see project from Youssef's class). 
\item Elliptical slice sampling. 
\item Titsias, et al; ``Gibbs-like algorithm''; sampling from the conditional prior rather than conditional posterior. 
\end{itemize}

\end{document}
