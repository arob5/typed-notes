\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}} % For lines in matrix to represent columns
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}} % For lines in matrix to represent rows

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Bayesian Statistics
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Section
\section{Multivariate Normal Distribution}
\subsection{Known Covariance Matrix} \label{mvn_mean}
Let's assume the model $y_1, \dots, y_n \in \R^d$, $y_i|\mu, \Sigma \overset{iid}{\sim} N_d(\mu, \Sigma)$. To simplify matters for the time being, we assume 
$\Sigma \in \R^{d \times d}$ a known symmetric, positive-definite matrix. Letting $y := (y_1, \dots, y_n)^T$, the likelihood for this model is: 
\begin{align*}
\mathcal{L}(\mu) = p(y|\mu, \Sigma) &= \prod_{i = 1}^{m} \text{det}(2\pi \Sigma)^{-1/2} \exp\left\{-\frac{1}{2}(y_i - \mu)^T \Sigma^{-1}(y_i - \mu)\right\} \\
			  			       &\propto \exp\left\{-\frac{1}{2} \sum_{i = 1}^{n} (y_i - \mu)^T \Sigma^{-1}(y_i - \mu)\right\}
\end{align*}
Since we're considering $\Sigma$ to be fixed, then the first term in the likelihood is a constant with respect to the parameter, hence the second line above. 
When contemplating what prior on $\mu$ might be conjugate to this likelihood we should notice the quadratic form $(y - \mu^T)\Sigma^{-1}(y - \mu)$. When the 
log-likelihood is proportional to a quadratic form of this type, then a multivariate normal prior will provide a conjugate prior. The conjugate relationship 
is derived in the following proposition. 
\begin{prop}
Let $y_1, \dots, y_n \overset{iid}{\sim} N_d(\mu, \Sigma)$, with fixed symmetric, positive definite matrix $\Sigma$ and prior $\mu \sim N_d(\mu_0, \Lambda_0)$.
Then the posterior distribution is $N_d(\mu_n, \Lambda_n)$
where 
\begin{align*}
\mu_n &= \Lambda_n (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \bar{y}) \\
\Lambda_n^{-1} &= \Lambda_0^{-1} + n \Sigma^{-1}
\end{align*}
Here, $\bar{y}$ denotes the mean of $y_1, \dots, y_n$. 
\end{prop}

\begin{proof}
The general strategy here is to multiply out terms, drop constants, combine and simplify as much as possible, and then apply the matrix generalization of completing the square
(proposition \ref{complete_the_square}). We have, 
\begin{align*}
p(\mu|y, \Sigma) &\propto p(y|\mu, \Sigma)p(\mu|\Sigma) \\
			  &\propto \exp\left\{-\frac{1}{2}(y - \mu)^T \Sigma^{-1}(y - \mu)\right\} \exp\left\{-\frac{1}{2}(\mu - \mu_0)^T \Lambda_0^{-1}(\mu - \mu_0)\right\} \\ 
			  &= \exp\left\{-\frac{1}{2} \left[(\mu - \mu_0)^T \Lambda_0^{-1}(\mu - \mu_0) + \sum_{i = 1}^{n} (y_i - \mu)^T \Sigma^{-1}(y_i - \mu) \right]\right\}
\end{align*}
For now I will focus on simplifying the terms in the angled brackets. Expanding the first term we have:
\begin{align*}
(\mu - \mu_0)^T \Lambda_0^{-1}(\mu - \mu_0) &= \mu^T \Lambda_0^{-1} \mu + \mu_0^T \Lambda_0^{-1} \mu_0 - 2\mu^T \Lambda_0^{-1} \mu_0 \\
								       &\propto \mu^T \Lambda_0^{-1} \mu - 2\mu^T \Lambda_0^{-1} \mu_0 \\
\end{align*}
And the second term, 
\begin{align*}
\sum_{i = 1}^{n} (y_i - \mu)^T \Sigma^{-1}(y_i - \mu) &= \mu^T (n\Sigma^{-1})\mu - 2\mu^T(n\Sigma^{-1})\bar{y} + \sum_{i = 1}^{n} y_i^T \Sigma^{-1} y_i \\
									       &\propto \mu^T (n\Sigma^{-1})\mu - 2\mu^T(n\Sigma^{-1})\bar{y} 
\end{align*}
Putting these together, $(\mu - \mu_0)^T \Lambda_0^{-1}(\mu - \mu_0) + \sum_{i = 1}^{n} (y_i - \mu)^T \Sigma^{-1}(y_i - \mu)$
\begin{align*}
 &\propto \mu^T \Lambda_0^{-1} \mu - 2\mu^T \Lambda_0^{-1} \mu_0 + \mu^T (n\Sigma^{-1})\mu - 2\mu^T(n\Sigma^{-1})\bar{y}  \\
																			 &= \mu^T (\Lambda_0^{-1} + n\Sigma^{-1}) \mu - 2\mu^T(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y}) \\
																			 &= \mu^T \Lambda_n^{-1 }\mu - 2(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y})^T \mu
\end{align*}
where $\Lambda_n^{-1} := \Lambda_0^{-1} + n\Sigma^{-1}$. Note that $\Lambda_0$ and $\Sigma$ are both positive definite and thus their inverses are also positive definite. This implies that 
$\Lambda_n^{-1}$ is positive definite and therefore invertible, justifying the definition of this matrix as an inverse. 
Now, notice that this is a quadratic in $\mu$, and we can thus complete the square by applying proposition \ref{complete_the_square}
with $A = \Lambda_n^{-1 }$ and $b = - 2(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y})$. This yields
\[\mu^T \Lambda_n^{-1 }\mu - 2(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y})^T \mu \propto (\mu - \mu_n)^T \Lambda_n^{-1 } (\mu - \mu_n)\]
where $\mu_n = -\frac{1}{2} \Lambda_n \cdot - 2(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y}) = \Lambda_n(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y})$
Finally, we plug this back into the original expression to obtain, 
\[p(\mu|y, \Sigma) \propto \exp\left\{-\frac{1}{2}(\mu - \mu_n)^T \Lambda_n^{-1 } (\mu - \mu_n) \right\} \propto N_d(\mu | \mu_n, \Lambda_n)\]
\end{proof}

% Section: Appendix
\section{Appendix}

\subsection{Matrix Algebra}
\subsubsection{Completing the Square for Quadratic Forms}
A matrix generalization of the ``completing the square'' technique from elementary algebra turns out to be very useful when deriving certain posterior distributions. 
For an example, this comes up when deriving the posterior of a Gaussian mean with Gaussian prior and known covariance matrix (example \ref{mvn_mean}). It will 
be convenient to have a general result we can apply to these types of problems, rather than deriving the formulas ad-hoc on a case-by-case basis. As motivation, 
let's recall the univariate case from elementary algebra. The goal is to write a quadratic
\[ax^2 + bx + c\]
in the form 
\[a(x - d)^2 + e\]
where $a \neq 0$. The task is thus to find the values $d$ and $e$. A straightforward approach is to simply expand this latter expression and compare the terms to the former expression. 
To this end we have 
\[a(x + d)^2 + e = ax^2 + ad^2 - 2adx + e\]
Comparing to the original expression, we see each has a single term with $x$ and thus we set these coefficients to be equal. 
\[b = -2ad \implies d = -\frac{1}{2}a^{-1}b\]
Similarly for $e$ we collect all of the terms that don't depend on $x$,
\[c = ad^2 + e \implies e = c - ad^2 = c - a\left(-\frac{1}{2}a^{-1}b\right)^2 = c - \frac{1}{4}a^{-1}b^2\]
The problem is solved! Summarizing, we have shown that 
\[ax^2 + bx + c = a(x - d)^2 + e\]
where 
\begin{align*}
d &= -\frac{1}{2}a^{-1}b \\
e &= c - \frac{1}{4}a^{-1}b^2 \\
\end{align*}
The following proposition shows that a direct generalization applies to the matrix case. The formulas essentially look identical, just with matrices replacing the scalar values. 
\begin{prop} \label{complete_the_square}
Let $x, b, c \in \R^n$ and $A \in \R^{n \times n}$ be a symmetric, invertible matrix. Then, 
\[x^T A x + b^T x + c = (x - d)^T A(x - d) + e\]
where 
\begin{align*}
d &= -\frac{1}{2}A^{-1}b \\
e &= c - \frac{1}{4}b^T A^{-1}b
\end{align*}
\end{prop}

\begin{proof}
We employ the same exact strategy as in the scalar case: just multiply out and compare terms. To this end, 
\[(x - d)^T A(x - d) + e = x^T A x + d^T Ad - 2d^T A x + e\]
Notice that I've used the fact that $d^T A x$ is symmetric (it's just a scalar) to combine the cross terms into $- 2d^T A x$. Now set the coefficients on the $x$ term to be equal: 
\[b^T = - 2d^T A \implies d = -\frac{1}{2}A^{-1}b\]
using the invertibility of $A$. Now set the constant terms (with respect to $x$) equal: 
\[c = d^T A d + e \implies e = c - d^T A d = c - \left(-\frac{1}{2}A^{-1}b\right)^T A \left(-\frac{1}{2}A^{-1}b\right) = c - \frac{1}{4}b^T A^{-1}b\]
This proves the result. 
\end{proof}


\end{document}

