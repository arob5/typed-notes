\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand*{\vertbar}{\rule[-1ex]{0.5pt}{2.5ex}} % For lines in matrix to represent columns
\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}} % For lines in matrix to represent rows

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
}

\begin{document}

\begin{center}
\Large
Bayesian Statistics
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

% Section: Basics of Bayesian Inference
\section{Basics of Bayesian Inference}
To illustrate the basic ideas of Bayesian inference, we consider the problem of estimating the mean of a normal distribution with known variance. In practice, the variance is almost never known
but this simplified model will be useful in illustrating the core ideas of Bayesian inference. With this in mind, we consider the following model. 
\begin{align*}
&y_1, \dots, y_n \overset{iid}{\sim} N(\theta, \sigma^2) \\
&\theta \sim \pi_0
\end{align*}
where $\pi_0$ is a prior distribution of the mean. We will consider different choices for this prior in the subsequent sections. On a notational note, let $y := (y_1, \dots, y_n)^T$. 

\subsection{Conjugate Prior}
We begin by considering the prior 
\[\theta \sim N(\mu_0, \tau_0^2)\]
As we will see, this prior is conjugate to the normal likelihood, so the posterior distribution is conveniently also Gaussian. We prove this fact below.
\subsubsection{Posterior Distribution.}
\begin{align*}
p(\theta|y) \propto p(y|\theta)p(\theta) &= \left(\prod_{i = 1}^{n} N(y_i|\theta, \sigma^2)\right)N(\theta|\mu_0, \tau_0^2) \\
							 &\propto \exp\left\{-\frac{1}{2\sigma^2} \sum_{i = 1}^{n} (y_i - \theta)^2 \right\} \exp\left\{-\frac{1}{2\tau_0^2}(\theta - \mu_0)^2 \right\} \\
							 &= \exp\left\{-\frac{1}{2} \left[\left(\frac{\theta - \mu_0}{\tau_0}\right)^2 + \sum_{i = 1}^{n} \left(\frac{y_i - \theta}{\sigma}\right)^2 \right]  \right\}
\end{align*}
We now focus our attention on the term inside the square brackets. As I have already spoiled, the posterior is going to be Gaussian; we therefore, want to manipulate this expression to look something like: 
\[\frac{1}{\text{posterior variance}}(\theta - \text{posterior mean})^2\]
where the posterior mean and variance are parameters we will discover by carrying out these algebraic manipulations. Before starting, note that any additive constant in the square bracket expression can be 
pulled out as a multiplicative constant in the original expression (due to the exponential) and therefore just acts as a proportionality constant to the posterior density. Since we know the posterior density is 
Gaussian we are simply dropping proportionality constants, which will make the algebra much nicer. Note that we are assuming the variance is known so $\sigma^2$ is considered a constant. 
Now, to simplify the term in brackets, we multiply out the quadratics and drop constants:  
\begin{align*}
\left(\frac{\theta - \mu_0}{\tau_0}\right)^2 &= \frac{\theta^2}{\tau_0^2} - 2\frac{\theta \mu_0}{\tau_0^2} + \frac{\mu^2}{\tau_0^2} \propto \frac{\theta^2}{\tau^2} - 2\frac{\theta \mu_0}{\tau_0^2} \\
\sum_{i = 1}^{n} \left(\frac{y_i - \theta}{\sigma}\right)^2 &= \sum_{i = 1}^{n}\frac{y_i^2}{\sigma^2} - 2\sum_{i = 1}^{n} \frac{y_i \theta}{\sigma^2} + \sum_{i = 1}^{n} \frac{\theta^2}{\sigma^2}
											\propto \frac{n \theta^2}{\sigma^2} - 2 \frac{n \bar{y} \theta}{\sigma^2}
\end{align*}
where $\bar{y} = \frac{1}{n} \sum_{i = 1}^{n} y_i$. Putting these together we have
\begin{align*}
\left(\frac{\theta - \mu_0}{\tau_0}\right)^2 + \sum_{i = 1}^{n} \left(\frac{y_i - \theta}{\sigma}\right)^2 &\propto \frac{\theta^2}{\tau_0^2} - 2\frac{\theta \mu_0}{\tau_0)^2} + \frac{n \theta^2}{\sigma^2} - 2 \frac{n \bar{y} \theta}{\sigma^2} \\
&= \left(\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}\right)\theta^2 - 2 \left(\frac{\mu_0}{\tau_0^2} + \frac{n \bar{y}}{\sigma^2}\right)\theta
\end{align*}
Now, notice that this is a quadratic function in $\theta$. Indeed, we can write this as 
\[a\theta^2 - 2b\theta\]
where 
\begin{align*}
a &:= \frac{1}{\tau_0^2} + \frac{n}{\sigma^2} \\
b &:= \frac{\mu_0}{\tau_0^2} + \frac{n \bar{y}}{\sigma^2}
\end{align*}
are independent of $\theta$. Recalling that we are seeking the form of a normal density, the natural next step is to complete the square. We can do so as follows. 
\begin{align*}
a\theta^2 - 2b\theta &= a\left(\theta^2 - 2\frac{b}{a} \theta \right) \\
		 	       &= a\left(\theta^2 - 2\frac{b}{a} \theta + \frac{b^2}{a^2} \right) - \frac{b^2}{a} \\
			       &\propto a\left(\theta^2 - 2\frac{b}{a} \theta + \frac{b^2}{a^2} \right) \\
			       &= a\left(\theta - \frac{b}{a}\right)^2
\end{align*}
We can drop the term $\frac{b^2}{a}$ as it does not depend on $\theta$. Plugging back into the original expression we obtain: 
\[p(\theta|y) \propto \exp\left\{-\frac{1}{2a^{-1}} \left(\theta - \frac{b}{a}\right)^2 \right\}\]
In this form, the posterior mean and variance can be deduced, which we find by plugging back in for $a$ and $b$. 
\begin{align*}
\mu_n &:= \frac{b}{a} = \frac{\frac{\mu_0}{\tau_0^2} + \frac{n\bar{y}}{\sigma^2}}{\frac{n}{\sigma^2} + \frac{1}{\tau_0^2}} \\
\tau_n^2 &:= \frac{1}{a} = \frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}
\end{align*}
where the notation for the posterior parameters $\mu_n$ and $\tau_n^2$ indicates the sample size of the data used to calculate the posterior. The interpretation of this result is 
deferred to the next section. For now, let's summarize the result we just derived. 
\begin{prop}
Suppose 
\begin{align*}
&y_1, \dots, y_n \overset{iid}{\sim} N(\theta, \sigma^2) \\
&\theta \sim N(\mu_0, \tau_0^2)
\end{align*}
where $\sigma^2$ is assumed to be known. Then the posterior distribution is given by $\mu|y_1, \dots, y_n \sim N(\mu_n, \tau_n^2)$ where 
\begin{align*}
\mu_n &=\frac{\frac{\mu_0}{\tau_0^2} + \frac{n\bar{y}}{\sigma^2}}{\frac{n}{\sigma^2} + \frac{1}{\tau_0^2}} \\
\tau_n^2 &= \frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}
\end{align*}
\end{prop}

\subsubsection{Interpreting the Posterior Mean and Variance}
Discuss posterior params as weighted averages, compromise between prior and data

\subsection{Non-Informative Prior}



% Section: Multivariate Normal Distribution
\section{Multivariate Normal Distribution}
\subsection{Known Covariance Matrix} \label{mvn_mean}
Let's assume the model $y_1, \dots, y_n \in \R^d$, $y_i|\mu, \Sigma \overset{iid}{\sim} N_d(\mu, \Sigma)$. To simplify matters for the time being, we assume 
$\Sigma \in \R^{d \times d}$ a known symmetric, positive-definite matrix. Letting $y := (y_1, \dots, y_n)^T$, the likelihood for this model is: 
\begin{align*}
\mathcal{L}(\mu) = p(y|\mu, \Sigma) &= \prod_{i = 1}^{m} \text{det}(2\pi \Sigma)^{-1/2} \exp\left\{-\frac{1}{2}(y_i - \mu)^T \Sigma^{-1}(y_i - \mu)\right\} \\
			  			       &\propto \exp\left\{-\frac{1}{2} \sum_{i = 1}^{n} (y_i - \mu)^T \Sigma^{-1}(y_i - \mu)\right\}
\end{align*}
Since we're considering $\Sigma$ to be fixed, then the first term in the likelihood is a constant with respect to the parameter, hence the second line above. 
When contemplating what prior on $\mu$ might be conjugate to this likelihood we should notice the quadratic form $(y - \mu^T)\Sigma^{-1}(y - \mu)$. When the 
log-likelihood is proportional to a quadratic form of this type, then a multivariate normal prior will provide a conjugate prior. The conjugate relationship 
is derived in the following proposition. 
\begin{prop} \label{normal_mean_known_variance}
Let $y_1, \dots, y_n \overset{iid}{\sim} N_d(\mu, \Sigma)$, with fixed symmetric, positive definite matrix $\Sigma$ and prior $\mu \sim N_d(\mu_0, \Lambda_0)$.
Then the posterior distribution is $N_d(\mu_n, \Lambda_n)$
where 
\begin{align*}
\mu_n &= \Lambda_n (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \bar{y}) \\
\Lambda_n^{-1} &= \Lambda_0^{-1} + n \Sigma^{-1}
\end{align*}
Here, $\bar{y}$ denotes the mean of $y_1, \dots, y_n$. 
\end{prop}

\begin{proof}
The general strategy here is to multiply out terms, drop constants, combine and simplify as much as possible, and then apply the matrix generalization of completing the square
(proposition \ref{complete_the_square}). We have, 
\begin{align*}
p(\mu|y, \Sigma) &\propto p(y|\mu, \Sigma)p(\mu|\Sigma) \\
			  &\propto \exp\left\{-\frac{1}{2}(y - \mu)^T \Sigma^{-1}(y - \mu)\right\} \exp\left\{-\frac{1}{2}(\mu - \mu_0)^T \Lambda_0^{-1}(\mu - \mu_0)\right\} \\ 
			  &= \exp\left\{-\frac{1}{2} \left[(\mu - \mu_0)^T \Lambda_0^{-1}(\mu - \mu_0) + \sum_{i = 1}^{n} (y_i - \mu)^T \Sigma^{-1}(y_i - \mu) \right]\right\}
\end{align*}
For now I will focus on simplifying the terms in the angled brackets. Expanding the first term we have:
\begin{align*}
(\mu - \mu_0)^T \Lambda_0^{-1}(\mu - \mu_0) &= \mu^T \Lambda_0^{-1} \mu + \mu_0^T \Lambda_0^{-1} \mu_0 - 2\mu^T \Lambda_0^{-1} \mu_0 \\
								       &\propto \mu^T \Lambda_0^{-1} \mu - 2\mu^T \Lambda_0^{-1} \mu_0 \\
\end{align*}
And the second term, 
\begin{align*}
\sum_{i = 1}^{n} (y_i - \mu)^T \Sigma^{-1}(y_i - \mu) &= \mu^T (n\Sigma^{-1})\mu - 2\mu^T(n\Sigma^{-1})\bar{y} + \sum_{i = 1}^{n} y_i^T \Sigma^{-1} y_i \\
									       &\propto \mu^T (n\Sigma^{-1})\mu - 2\mu^T(n\Sigma^{-1})\bar{y} 
\end{align*}
Putting these together, $(\mu - \mu_0)^T \Lambda_0^{-1}(\mu - \mu_0) + \sum_{i = 1}^{n} (y_i - \mu)^T \Sigma^{-1}(y_i - \mu)$
\begin{align*}
 &\propto \mu^T \Lambda_0^{-1} \mu - 2\mu^T \Lambda_0^{-1} \mu_0 + \mu^T (n\Sigma^{-1})\mu - 2\mu^T(n\Sigma^{-1})\bar{y}  \\
																			 &= \mu^T (\Lambda_0^{-1} + n\Sigma^{-1}) \mu - 2\mu^T(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y}) \\
																			 &= \mu^T \Lambda_n^{-1 }\mu - 2(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y})^T \mu
\end{align*}
where $\Lambda_n^{-1} := \Lambda_0^{-1} + n\Sigma^{-1}$. Note that $\Lambda_0$ and $\Sigma$ are both positive definite and thus their inverses are also positive definite. This implies that 
$\Lambda_n^{-1}$ is positive definite and therefore invertible, justifying the definition of this matrix as an inverse. 
Now, notice that this is a quadratic in $\mu$, and we can thus complete the square by applying proposition \ref{complete_the_square}
with $A = \Lambda_n^{-1 }$ and $b = - 2(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y})$. This yields
\[\mu^T \Lambda_n^{-1 }\mu - 2(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y})^T \mu \propto (\mu - \mu_n)^T \Lambda_n^{-1 } (\mu - \mu_n)\]
where $\mu_n = -\frac{1}{2} \Lambda_n \cdot - 2(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y}) = \Lambda_n(\Lambda_0^{-1} \mu_0 + n\Sigma^{-1} \bar{y})$
Finally, we plug this back into the original expression to obtain, 
\[p(\mu|y, \Sigma) \propto \exp\left\{-\frac{1}{2}(\mu - \mu_n)^T \Lambda_n^{-1 } (\mu - \mu_n) \right\} \propto N_d(\mu | \mu_n, \Lambda_n)\]
\end{proof}

Having derived the form of the posterior distribution, we now turn to the posterior predictive distribution. 
\begin{prop}
Under the same assumptions as \ref{normal_mean_known_variance}, let $\tilde{y} \sim N_d(\mu, \Sigma)$ be a new observation. Then the posterior predictive distribution is given by
\[\tilde{y}|y \sim N(\mu_n, \Sigma + \Lambda_n)\]
\end{prop}

\begin{proof}
We could try to approach this directly via integration, but it is easier to first verify that $\tilde{y}|y$ has a univariate normal distribution and then just solve for the mean and variance. 
To this end, first consider the joint posterior distribution
\begin{align*}
p(\tilde{y}, \mu|y) &= p(\tilde{y}|\mu, y)p(\mu|y) = p(\tilde{y}|\mu)p(\mu|y)
\end{align*}
where the second step follows from the fact that the distribution of $\tilde{y}$ and $y$ are independent, conditional on $\mu$. By assumption $\tilde{y}|\mu$ is multivariate normal, and we have
just derived that the posterior $\mu|y$ is also multivariate normal. It is not difficult to check that a product of multivariate normal densities results in another multivariate normal density. Hence, 
$(\tilde{y}, \mu)|y$ is multivariate normal distributed, which implies that the marginal $\tilde{y}|y$ has a univariate normal distribution. It therefore remains to calculate the mean and variance of this
distribution. For the mean we apply the conditional tower property (\ref{tower_property}): 
\[\E[\tilde{y}|y] = \E[\E(\tilde{y}|\mu, y)|y] = \E[\E(\tilde{y}|\mu)|y] = \E[\mu|y] = \mu_n\]
The second equality again uses the fact that $\tilde{y}$ and $y$ are independent, conditional on $\mu$. We derive the variance similarly using \ref{variance_tower_property}: 
\[\Var(\tilde{y}|y) = \Var[\E(\tilde{y}|\mu, y)|y] + \E[\Var(\tilde{y}|\mu, y)|y] = \Var[\mu|y] + \E[\Sigma|y] = \Lambda_n + \Sigma\]
The final step uses the derivation of the posterior variance in \ref{normal_mean_known_variance}, as well as the assumption that $\Sigma$ is fixed. 
\end{proof}
This result makes intuitive sense. The expectation of the posterior predictive distribution equals the expectation of the posterior distribution, since the parameter of interest here is the mean. 
The posterior predictive variance is a function of both the variance (assumed known) of the underlying process ($\Sigma$), as well as the uncertainty due to the fact that we're estimating the 
mean ($\Lambda_n$). 

% Section: Appendix
\section{Appendix}

\subsection{Important Distributions}

\subsubsection{Wishart and Inverse Wishart Distributions}
The Wishart distribution arises naturally in Bayesian statistics when considering conjugate priors for certain models. The distribution is a multivariate generalization of the Gamma distribution, so 
let's recall some facts about the Gamma distribution before introducing the Wishart. 
\begin{enumerate}
\item Has support $\R_+ = (0, \infty)$ 
\item Has two parameters. It is typically parameterized in terms of shape and scale parameters or shape and rate (i.e. inverse scale) parameters.
\item In either parameterization, both parameters are positive real numbers.  
\end{enumerate}
The Wishart distribution generalizes these properties as follows. 
\begin{enumerate}
\item Has support $S_+^p$, the set of positive-definite $p \times p$ matrices. 
\item Is parameterized by a scale matrix $V$ and the degrees of freedom $n$. 
\item $V$ is a positive-definite matrix and $n$ is a positive integer. 
\end{enumerate}

The Wishart distribution is therefore a distribution over \textit{matrices}, which gives it an important role in random matrix theory and in covariance matrix estimation. 
The following is one method of formally defining the Wishart distribution. 
\begin{definition}
Let $V \in S_+^p$, $n > p- 1$ and suppose $x_1, \dots, x_n \overset{iid}{\sim} N_p(0, V)$. Let $X \in \R^{p \times n}$ be the matrix with columns given by the $x_i$. Then we say that
the outer product $S := XX^T$ has a Wishart distribution with scale $V$ and $n$ degrees of freedom, written $-$
\end{definition}
Note that if a matrix can be written as an outer product, then it is postive-semidefinite, so the support of the distribution is indeed $S_+^p$. Moreover, note that we may also 
write the inner product as $XX^T = \sum_{i = 1}^{n} x_i x_i^T$, which is nice in that it makes the degrees of freedom clear. Written in this way also demonstrates the connection
with covariance matrix estimation; the standard covariance matrix estimator is a sum over outer products. It turns out to be convenient to also define the distribution of the inverse 
of Wishart-distributed matrices. 
\begin{definition}
We say that a matrix $Y \in S_+^p$ has inverse-Wishart distribution with scale matrix $\Sigma \in S_+^p$ and degrees of freedom $n > p - 1$, written $Y \sim \text{Inv-Wishart}(\Sigma, n)$, 
if $Y^{-1} \sim \text{Wishart}(\Sigma^{-1}, n)$. 
\end{definition}

Note that this definition makes sense because positive definite matrices are invertible and have positive definite inverses. 



\subsection{Probability}
\subsubsection{The Tower Property}
The well-known tower property (also called the law of iterated expectations) is typically stated as 
\[\E[Y] = \E[\E(Y|X)]\]
We view the inner expectation $\E(Y|X)$ as a function of $X$, and hence the outer expectation is taken with respect to $X$. Therefore, for clarity we might write
\[\E[Y] = \E_X[\E_Y(Y|X)]\]
In Bayesian statistics, we will find the following generalization of this result especially useful. 
\[\E[Y|X] = \E[\E(Y|X, Z)|X]\] \label{tower_property}
A very nice intuitive explanation of this result is given in \href{https://stats.stackexchange.com/questions/95947/a-generalization-of-the-law-of-iterated-expectations}{this} StackOverflow post. 
I will refer to this generalized form as the \textit{conditional tower property}. We will also make use of an analogous result for the variance. 
\[\Var(Y|X) = \Var(\E[Y|X, Z]|X) + \E[\Var(Y|X, Z)|X]\] \label{variance_tower_property}
which I will refer to as the \textit{conditional law of total variance}. 

\subsection{Matrix Algebra}
\subsubsection{Completing the Square for Quadratic Forms}
A matrix generalization of the ``completing the square'' technique from elementary algebra turns out to be very useful when deriving certain posterior distributions. 
For an example, this comes up when deriving the posterior of a Gaussian mean with Gaussian prior and known covariance matrix (example \ref{mvn_mean}). It will 
be convenient to have a general result we can apply to these types of problems, rather than deriving the formulas ad-hoc on a case-by-case basis. As motivation, 
let's recall the univariate case from elementary algebra. The goal is to write a quadratic
\[ax^2 + bx + c\]
in the form 
\[a(x - d)^2 + e\]
where $a \neq 0$. The task is thus to find the values $d$ and $e$. A straightforward approach is to simply expand this latter expression and compare the terms to the former expression. 
To this end we have 
\[a(x + d)^2 + e = ax^2 + ad^2 - 2adx + e\]
Comparing to the original expression, we see each has a single term with $x$ and thus we set these coefficients to be equal. 
\[b = -2ad \implies d = -\frac{1}{2}a^{-1}b\]
Similarly for $e$ we collect all of the terms that don't depend on $x$,
\[c = ad^2 + e \implies e = c - ad^2 = c - a\left(-\frac{1}{2}a^{-1}b\right)^2 = c - \frac{1}{4}a^{-1}b^2\]
The problem is solved! Summarizing, we have shown that 
\[ax^2 + bx + c = a(x - d)^2 + e\]
where 
\begin{align*}
d &= -\frac{1}{2}a^{-1}b \\
e &= c - \frac{1}{4}a^{-1}b^2 \\
\end{align*}
The following proposition shows that a direct generalization applies to the matrix case. The formulas essentially look identical, just with matrices replacing the scalar values. 
\begin{prop} \label{complete_the_square}
Let $x, b, c \in \R^n$ and $A \in \R^{n \times n}$ be a symmetric, invertible matrix. Then, 
\[x^T A x + b^T x + c = (x - d)^T A(x - d) + e\]
where 
\begin{align*}
d &= -\frac{1}{2}A^{-1}b \\
e &= c - \frac{1}{4}b^T A^{-1}b
\end{align*}
\end{prop}

\begin{proof}
We employ the same exact strategy as in the scalar case: just multiply out and compare terms. To this end, 
\[(x - d)^T A(x - d) + e = x^T A x + d^T Ad - 2d^T A x + e\]
Notice that I've used the fact that $d^T A x$ is symmetric (it's just a scalar) to combine the cross terms into $- 2d^T A x$. Now set the coefficients on the $x$ term to be equal: 
\[b^T = - 2d^T A \implies d = -\frac{1}{2}A^{-1}b\]
using the invertibility of $A$. Now set the constant terms (with respect to $x$) equal: 
\[c = d^T A d + e \implies e = c - d^T A d = c - \left(-\frac{1}{2}A^{-1}b\right)^T A \left(-\frac{1}{2}A^{-1}b\right) = c - \frac{1}{4}b^T A^{-1}b\]
This proves the result. 
\end{proof}


\end{document}

