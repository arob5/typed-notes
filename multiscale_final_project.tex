\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{local-defs-EKI}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./multiscale_paper_figures/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{alg}{Algorithm}
\newtheorem{dynamics}{Artificial Dynamics}

% Title and author
\title{Multiscale Methods for Sampling Posterior Distributions in Bayesian Inverse Problems}
\author{Andrew Roberts}

\begin{document}

\maketitle
\newpage

% Abstract
\section{Abstract}
In this paper, we numerically investigate an algorithm recently proposed for sampling a Bayesian posterior distribution using 
multiscale dynamics. This method joins a growing list of derivative-free Bayesian inversion methods aimed at solving inverse 
problems with expensive forward models. In particular, the multiscale sampling method can be viewed as a competitor to 
a class of algorithms which apply ensemble Kalman techniques to inverse problems. In order to provide context, we begin by 
briefly reviewing the literature on these so-called ensemble Kalman inversion methods. We then describe the multiscale 
method, highlighting its theoretical properties in continuous time, as well as its numerical discretization. The algorithm is 
numerically assessed on two inverse problems: a linear Gaussian example with known posterior, and a non-linear inverse 
problem stemming from a system of ordinary differential equations (ODEs) describing terrestrial carbon dynamics. The algorithm 
is validated against an adaptive random walk Metropolis-Hastings (RWMH) algorithm, a standard baseline for derivative-free posterior 
sampling. Emphasis is placed on evaluating the ability of the algorithms to represent the true posterior using as few forward model 
evaluations as possible. 

% Setting and Literature Review
\section{Setting and Literature Review}
The general setting of interest is non-linear Bayesian inverse problems with Gaussian likelihood and prior
\begin{align}
\dataObs &= \fwd(\spar) + \noise \label{inverse_problem} \\
\noise &\sim \Gaussian(0, \covObsNoise) \nonumber \\
\spar &\sim \Gaussian(\priorMean, \priorCov), \nonumber
\end{align}
where $\spar \in \R^{\Npar}$ is the unknown parameter of interest and $\dataObs \in \R^{\dimData}$ is an observed 
data vector which is assumed to result from a potentially non-linear forward mapping $\fwd: \R^{\Npar} \to \R^{\dimData}$
corrupted by Gaussian noise $\noise$. We consider the Bayesian approach, whereby the parameter is equipped with 
Gaussian prior and the goal is to characterize the posterior distribution $\spar|\dataObs \sim \postMeas$. Letting
$\priorDens$ and $\postDens$ denote the prior and posterior (Lebesgue) densities, we can write the posterior density 
as 
\begin{align*}
&\postDens(\spar) = \frac{1}{Z} \exp\left\{-\SSR_1(\spar) \right\}, &&\SSR_1(\spar) = \SSR(\spar) + \SSR_0(\spar) \\
&\SSR(\spar) := \frac{1}{2}\norm{\dataObs - \fwd(\spar)}_{\covObsNoise}, &&\SSR_0(\spar) := \frac{1}{2} \norm{\spar-\priorMean}_{\priorCov},
\end{align*}
where $Z$ denotes the normalizing constant. 
We utilize the notation $\langle a, b \rangle_{A} := \langle a, A^{-1} b \rangle$ and $\norm{a}_A^2 = \langle a, a \rangle_{A}$, for positive 
definite matrix $A$. 

A wide class of algorithms aims to sample from $\postMeas$ by introducing an artificial stochastic dynamical system 
implying a sequence of probability measures $\postMeas_1, \postMeas_2, \dots$ which converge to the posterior in 
some sense. A subset of these algorithms define this sequence such that it is initialized from the prior and transforms the 
prior into the posterior in a finite sequence of steps. This idea can be realized via the following likelihood tempering 
scheme 
\begin{align*}
\postDens_{\timeStep+1}(\spar) = \frac{1}{Z_{\timeStep+1}} \exp\left(-\frac{1}{\Ntimestep} \SSR(\spar) \right) \postDens_{\timeStep}(\spar),
\end{align*}
which defines a prior-to-posterior in precisely $\Ntimestep$ steps. Particle methods are then used to approximate the updates
$\postMeas_\timeStep \mapsto \postMeas_{\timeStep+1}$. A sequential Monte Carlo (SMC) approach to this problem is explored in \cite{Kantas} 
and ensemble Kalman approximations are discussed in \cite{Iglesias}. A fundamentally different approach to the Bayesian inversion problem is to 
construct an infinite sequence $\postMeas_1, \postMeas_2, \dots$ such that samples are produced in infinite time. Prior initialization is not required 
with this approach, as the methodology relies on the ergodic behavior of the dynamical system. The sequence defined by 
\begin{align}
\postDens_{\timeStep+1}(\spar) = \frac{1}{Z_{\timeStep+1}} \exp\left(-\SSR(\spar) \right) \postDens_{\timeStep}(\spar). 
\end{align}
is explored in \cite{Huang}, where it is shown that the resulting algorithm collapses on the minimizer of $\SSR(\spar)$. Algorithms for approximate posterior sampling, as opposed
to optimization, can be developed by carefully adding noise into the procedure. This is accomplished with the ensemble Kalman sampler (EKS), proposed in 
\cite{Garbuno}, and is improved upon in \cite{Garbuno2} with the affine invariant Langevin dynamics (ALDI) algorithm. Improved formulations and generalized 
perspectives on EKI methods for optimization are discussed in \cite{Huang} and \cite{Chada}. 

% The Multiscale Sampling Method 
\section{The Multiscale Sampling Method}
The method proposed in \cite{Pavliotis} departs from the use of ensemble Kalman methods, instead relying on multiscale dynamics. The method is based on a slow-fast system 
of stochastic differential equations (SDEs); an ensemble $\{\xi_t^{(j)}\}_{j=1}^{J}$ of ``fast exploring'' particles help to guide a single ``distinguished'' particle $\spar_t$, such that the marginal 
distribution of $\spar$ tends to the posterior $\postMeas$ (approximately) in infinite time. This combines the notion of using an ensemble of particles to approximate gradients (as in the 
EKI methods discussed above) with elements of popular Langevin sampling methods, which explore a distribution using a single particle. The method, formulated in continuous-time, is 
based on the following slow-fast system of SDEs. 
\begin{align}
d\spar_t &= -\frac{1}{J\sigma^2} \sum_{j=1}^{J} \left\langle \fwd(\spar_t^{(j)})-\fwd(\spar_t), \fwd(\spar_t)-\dataObs \right\rangle_{\covObsNoise}(\spar_t^{(j)}-\spar_t) - \hat{C}(\xi_t)(\spar_t - \priorMean) + \sqrt{2\hat{C}(\xi_t)} dW_t
\end{align}
where $W_t, \{W_t^{(j)}\}$ are independent Brownian motions, and $\delta$ and $\sigma$ are positive parameters. We write $\xi_t := \{\xi_t^{(j)}\}_{j=1}^{J}$ and $\hat{C}(\xi_t)$ the empirical covariance 
of these fast particles. 

% Numerical Experiments
\section{Numerical Experiments}
We now test the multiscale sampling method, varying its tuning parameters and comparing performance against an adaptive RWMH algorithm \cite{Haario}. All experiments 
are performed using $J=8$ fast particles. For each experiment, all algorithms utilize the same initial condition $u_0$, which is sampled from the prior. 
Posterior approximation is assessed as a function of the iteration number; it will be explicitly noted when burn-in is removed. We note that this comparison may appear to 
unfairly favor the multiscale algorithm over RWMH, as each iteration of the former requires $J+1$ forward model evaluations while RWMH only requires one. However, our 
interest is primarily in wall-clock time, and the $J+1$ evaluations will typically be able to be performed in parallel for reasonably-sized $J$. 

\subsection{Linear Gaussian Model}
We begin with a two-dimensional linear Gaussian model, evaluating the algorithms against the known closed-form Gaussian posterior. In particular, we consider the model 

\begin{align}
\dataObs &= \fwd \spar + \noise \\
\noise &\sim \Gaussian(0, 0.4I) \nonumber \\
\spar &\sim \Gaussian(0, I), \nonumber
\end{align}
where $\fwd \in \R^{100 \times 2}$ is a matrix with rows sampled from a zero-mean Gaussian with unit variances and covariance $0.7$. 
The data $y$ is generated from the exact likelihood with ground-truth parameter $\spar_\star \approx (1.216, 0.331)$. We run all 
algorithms for 5000 iterations. Posterior approximation is assessed as a function of iteration by computing the Kullback-Leibler (KL) divergence 
between the true Gaussian posterior and the Gaussian distribution with mean and covariance set to the empirical mean and covariance 
of all samples produced by the algorithm up through the current iteration. Once the iteration value reaches a reasonable size, this Gaussian 
approximation is reasonable; hence we begin reporting the KL divergence after iteration $100$. 
The results are summarized in figure \ref{fig:lin_Gaus_KL}. While we only test a small number of 
parameter combinations for $\Delta, \delta, \sigma$, in this example the multiscale algorithm is most sensitive to the step size $\Delta$. The smaller 
of the two step sizes tested resulted in slower convergence. The multiscale algorithm is competitive with the RWMH baseline (black line) up until 
about iteration $1000$. At this point, the error of the multiscale algorithm levels out while the RWMH continues to decrease. This is likely reflecting the 
fact that the posterior distribution is only the stationary distribution of the multiscale system in the refinement limit; thus, the multiscale method appears 
to be reaching a level of irreducible error around this point. 

We also plot the RWMH and multiscale algorithm samples overlaid on contours of the true posterior density, each with the first 1000 iterations 
removed as burn-in \ref{fig:lin_Gaus_contour}. The multiscale samples correspond to the algorithm with best-performing parameter choices from 
the previous experiment; in this case, $\Delta=10^{-3}, \sigma=10^{-4}, \delta=10^{-4}$. We see that both algorithms do a good job  of representing 
the true posterior, but the multiscale samples appear to be slightly over-dispersed. Again, this likely corresponds to the fact that the stationary distribution 
of the multiscale system, which approximates the true posterior, is actually a slightly over-dispersed version of the target distribution. Given the the mutliscale 
method faithfully recovers the true posterior mean, this figure illustrates that it is this over-dispersion error in the covariance matrix that is leading to the 
irreducible error in the KL divergence observed in the previous plot. 

\begin{figure}[h] 
\centering
\includegraphics[width=0.9\textwidth]{lin_Gaus_KL.png}
\caption{KL divergence between true and approximate distributions, as a function of iteration. The RWMH error is given by the black line.}
\label{fig:lin_Gaus_KL}
\end{figure}

\begin{figure}[h] 
\centering
\includegraphics[width=0.9\textwidth]{lin_Gaus_contour_rwmh_vs_multiscale.png}
\caption{Samples produced by RWMH (black) and the multiscale method with $\Delta=10^{-3}, \sigma=10^{-4}, \delta=10^{-4}$ (gray) overlaid on contours of the true posterior density. The first 1000 samples
are dropped as burn-in for both algorithms.}
\label{fig:lin_Gaus_contour}
\end{figure}

\subsection{Non-Linear Forward Model}





\begin{thebibliography}{20}
\bibitem{Pavliotis} Pavliotis, Grigorios A. et al. “Derivative-Free Bayesian Inversion Using Multiscale Dynamics.” SIAM J. Appl. Dyn. Syst. 21 (2021): 284-326. 
\bibitem{Kantas} Kantas, Nikolas et al. “Sequential Monte Carlo Methods for High-Dimensional Inverse Problems: A Case Study for the Navier-Stokes Equations.” SIAM/ASA J. Uncertain. Quantification 2 (2013): 464-489. 
\bibitem{Iglesias} Iglesias, Marco and Law, Kody and Stuart, Andrew. (2013). Ensemble Kalman methods for inverse problems. Inverse Problems. 29. 045001. 10.1088/0266-5611/29/4/045001. 
\bibitem{Huang} Huang, Daniel Zhengyu et al. “Iterated Kalman methodology for inverse problems.” J. Comput. Phys. 463 (2021): 111262.
\bibitem{Garbuno} Garbuno Iñigo, Alfredo \& Hoffmann, Franca \& Li, Wuchen \& Stuart, Andrew. (2020). Interacting Langevin Diffusions: Gradient Structure and Ensemble Kalman Sampler. SIAM Journal on Applied Dynamical Systems. 19. 412-441. 10.1137/19M1251655. 
\bibitem{Garbuno2} Garbuno Iñigo, Alfredo \& Nüsken, Nikolas \& Reich, Sebastian. (2019). Affine invariant interacting Langevin dynamics for Bayesian inference. 
\bibitem{Chada} Chada, Neil \& Chen, Yuming \& Sanz-Alonso, Daniel. (2020). Iterative Ensemble Kalman Methods: A Unified Perspective with Some New Variants. 
\bibitem{Haario} Heikki Haario, Eero Saksman, Johanna Tamminen "An adaptive Metropolis algorithm," Bernoulli, Bernoulli 7(2), 223-242, (April 2001)
\end{thebibliography}

\end{document}