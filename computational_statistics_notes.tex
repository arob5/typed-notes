\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

\begin{document}

\begin{center}
\Large
Notes on Computational Statistics/Monte Carlo Methods
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

\section{Sampling}

\subsection{Linchpin Sampling}
Suppose we are interested in sampling from the marginal distribution of a random variable $X$, and we know how to sample from the distributions or $X|Y$ 
and $Y$, for some other random variable $Y$. We can thus exploit the relationship
\[p(X, Y) = p(X|Y)p(Y)\]
to generate samples from the joint distribution of $(X, Y)$. The idea is then to just ``throw away'' the samples from $Y$, leaving us with a random sample from the marginal
distribution of $X$. Here, $Y$ is called the \textbf{linchpin variable}, given that it is vital in allowing us to sample from the marginal of $X$ even though we aren't interested in 
$Y$ itself. The algorithm for generating a single sample from the joint distribution is given below. 
\begin{itemize}
\item Sample $y \sim \pi_Y$
\item Sample $x \sim \pi_{X|Y=y}$
\item Output $(x, y)$
\end{itemize}

The correctness of this algorithm follows almost by definition of what it means to sample from the joint and marginal distributions. Although perhaps a bit overkill, 
I will state it a bit more formally below. 
\begin{prop}
Suppose $(x, y)$ is constructed as above. Then, 
\begin{enumerate}
\item $(x, y) \sim \pi_{X, Y}$
\item $x \sim X$
\end{enumerate}
\end{prop}

\begin{proof}
We begin by proving the first claim. Note that by definition of the algorithm $y \overset{d}{=} Y$ so that $y$ has distribution $\pi_Y$. Now, with Borel sets 
$A \in \mathcal{B}(\mathcal{X})$, $B \in \mathcal{B}(\mathcal{Y})$ we have 
\[\Prob((x, y) \in A \times B) = \Prob(x \in A | y \in B)\Prob(y \in B) = \pi_{X|Y\in B}(A)\pi_Y(B) = \pi_{X, Y}(A, B)\]
which shows $(x, y) \sim \pi_{X, Y}$. 

We now show the second claim, which pretty much follows directly from the joint distribution result above. 
\[\pi_X(A) = \Prob(X \in A) = \Prob(X \in A, Y \in \mathcal{Y}) = \Prob(x \in A, y \in \mathcal{Y}) = \Prob(x \in A)\]
The third equality uses the equality of the joint distributions proved above. 
\end{proof}
This proof is largely overkill; it is probably best just to realize that the result follows trivially from the fact that the joint distribution can be factored as a product
of a conditional and marginal. As a final point, we recall that the marginal distribution of $X$ can be obtained by integrating $Y$ out of the joint distribution. 
\[\pi_X(A) = \int_\mathcal{Y} \pi_{X, Y}(A, y) \pi_Y(dy)\]
This ``integrating out`` step is analogous to the dropping the samples from $Y$ in the above algorithm to obtain marginal samples from $X$. In other words, 
ignoring $Y$ is just like averaging over $Y$. 

\subsection{Rejection Sampling}
\textit{Rejection sampling} or \textit{Accept-Reject sampling} addresses the problem of drawing iid samples from a probability distribution $\pi$, where all we know is the 
density $\pi(x)$, or perhaps even an unnormalized density $\tilde{\pi}$, such that $\pi(x) = \frac{\tilde{\pi}(x)}{Z}$ where $Z = \int \tilde{\pi}(x) dx$ is the normalizing constant which 
does not depend on $x$. The general idea in rejection sampling is to first draw a sample $x^\prime $from some simpler distribution with density $g(x)$ that we already know how to sample from,
 and then strategically accept or reject the sample based on our knowledge of the ratio of densities $\frac{\pi(x^\prime)}{g(x^\prime)}$. Intuitively, if $g(x^\prime)$ is much larger $\pi(x^\prime)$ then 
 in order for the resulting samples to be distributed according $\pi$, we'll have to reject $x^\prime$ with a sizable probability. This idea will be explored in much more depth below, but to start we take 
 a step back and state some more fundamental ideas. 
 
 \subsubsection{The Fundamental Theorem of Simulation}
 We begin with what Robert and Casella term the \textit{Fundamental Theorem of Simulation}. Loosely speaking, this states that drawing a sample from $\pi$ is equivalent to selecting a point
 uniformly at random under the surface $\pi(x)$, and then simply extracting the $x$-coordinate of the resulting sample. To make this more clear, let's first think about what it means to select a point
 uniformly at random underneath the surface $\pi(x)$. The density of this uniform distribution must be $\frac{1}{\text{Vol}(R)}$, where $R$ is the region underneath $\pi(x)$ and $\text{Vol}(R)$ is the 
 volume of this region. However, $\pi(x)$ is a density so necessarily $\text{Vol}(R) = 1$. Therefore, the density is simply $1$ inside $R$ and $0$ everywhere else. Noting that $R$ can more explicitly 
 be described as $R = \{(x, u): 0 \leq u \leq \pi(x)\}$, so we can write the density as 
 \[p(x, u) = \mathbbm{1}\left\{(x, u): 0 \leq u \leq \pi(x)\right\}]\]
We could alternatively arrive at this expression for $p(x, u)$ by considering the two step generative procedure
\begin{enumerate}
\item First sample $x \sim \pi$
\item Next sample $u|x \sim \mathcal{U}(0, \pi(x))$ and return $(x, u)$
\end{enumerate}
Putting these two steps together, we have 
\[p(x, u) = p(u|x)p(x) = \mathcal{U}(u|0, \pi(x))\pi(x) = \frac{1}{\pi(x)}\pi(x) = 1\]
which is the same expression we arrived at previously. This second method shows that the two-step algorithm results in a uniform distribution on $R$ and by definition returns
points $(x, u)$ where marginally $x \sim \pi$. However, to rigorously verify the fundamental theorem of simulation, let's start from the definition of the uniform distribution $p(x, u)$ 
over $R$ and show that the first marginal $p(x)$ is indeed equal to $\pi(x)$. To this end, we have
\begin{align*}
p(x) &= \int_{-\infty}^{\infty} p(x, u) du \\
       &= \int_{-\infty}^{\infty} \mathbbm{1}\left\{(x, u): 0 \leq u \leq \pi(x)\right\} du \\
       &= \int_{0}^{\pi(x)} du \\
       &= \pi(x)
\end{align*}
which verifies the result. So it seems we have replaced the problem of drawing samples from $\pi$ with drawing samples from $p(x, u)$ and then extracting the $x$ portion of the sample. 
This doesn't seem to be any help at all though, since it is not obvious how to draw samples that are uniformly distributed on $R$. Indeed, the obvious two-step sampling procedure described 
above requires sampling from $\pi$ as the first step, which is the original problem we were trying to solve! Despite this impracticality, the basic idea here is fundamental, and we will show how 
to extend it in a way that is actually useful in developing sampling algorithms. Speaking of practicality, we already mentioned that we want algorithms that work even if we can only evaluate the 
unnormalized density  $\tilde{\pi}(x)$, as is often the case in practice. Therefore, as a first step towards practical algorithms, let's generalize the fundamental theorem of simulation to the case 
where the normalization constant $Z$ is unknown. In particular, our claim is that sampling uniformly from the region $\tilde{R} := \left\{(x, u): 0 \leq u \leq \tilde{\pi}(x) \right\}$ and then extracting the 
$x$ component results in a sample distributed according to $\pi(x)$. That is, the idea of sampling from $\pi$ by sampling uniformly under the density surface only requires knowledge of the density 
up to a normalizing constant. We proceed as before, defining the density $p(x, u)$ corresponding to the uniform distribution over $\tilde{R}$ and then show that $\pi(x)$ is a marginal of $p(x, u)$. 
We know that the uniform distribution must have density
\[p(x, u) = \frac{1}{\text{Vol}(\tilde{R})}\mathbbm{1}\left\{(x, u): 0 \leq u \leq \tilde{\pi}(x) \right\}\]
where 
\[ \text{Vol}(\tilde{R}) = \int_{-\infty}^{\infty} \int_{0}^{\tilde{\pi}(x)} du dx = \int_{-\infty}^{\infty} \tilde{\pi}(x) dx = Z \]
by definition of the normalizing constant $Z$. Thus, 
\[p(x, u) = \frac{1}{Z}\mathbbm{1}\left\{(x, u): 0 \leq u \leq \tilde{\pi}(x) \right\}\]
and the marginal of $p(x, u)$ is given by
\begin{align*}
\int_{-\infty}^{\infty} p(x, u) du &= \int_{-\infty}^{\infty} \frac{1}{Z}\mathbbm{1}\left\{(x, u): 0 \leq u \leq \tilde{\pi}(x) \right\} du \\
					     &= \frac{1}{Z} \int_{0}^{\tilde{\pi}(x)} du \\
					     &= \frac{1}{Z} \tilde{\pi}(x) \\
					     &= \frac{1}{Z} Z\pi(x) \\
					     &= \pi(x)
\end{align*}
which verifies the claim. 

\subsection{Slice Sampling}
Recall the fundamental theorem of simulation, which stated that we can sample from $\pi$ by sampling uniformly from $R$, the region under the density surface of $\pi(x)$
 (or from $\tilde{R}$, the region under the the unnormalized density surface
$\tilde{\pi}(x)$). However, this wasn't immediately useful as sampling under the density surface was just as hard as sampling from $\pi$ directly. However, rejection sampling yielded an indirect method
to sample from $\tilde{R}$ (we will focus on the case where we only have access to $\tilde{\pi}(x)$ going forward, as this is more general). Rejection sampling accomplished this by sampling from a region 
that encloses $\tilde{R}$ and then only retaining the sample if it was contained in $\tilde{R}$. This resulted in uniform samples over $\tilde{R}$ (and hence marginal samples from $\pi$), as desired, but this came
at the cost of efficiency--the rejected samples are essentially wasted computation. Metropolis-Hastings type MCMC algorithms presented a very different approach to sampling from $\pi$, but faced the challenge of 
tuning the step-size of the algorithm, which was determined by the variance of the proposal distribution. Gibbs sampling offered an MCMC approach with automatic step-size selection, by alternately sampling from 
the marginal conditional distributions. Slice sampling combines ideas from rejection sampling and Gibbs sampling by directly sampling from the uniform distribution over $\tilde{R} := \left\{(x, u): 0 \leq u \leq \tilde{\pi}(x) \right\}$
using Gibbs sampling. Letting $p(x, u)$ denote the uniform density over $\tilde{R}$, Gibbs sampling proceeds by alternating sampling from $p(x|u)$ and $p(u|x)$. Recall that the density is given by 
\[p(x, u) = \frac{1}{Z}\mathbbm{1}\left\{(x, u): 0 \leq u \leq \tilde{\pi}(x) \right\}\]
where $Z = \int \tilde{\pi}(x) dx$ is the normalizing constant. So the problem becomes how to sample from $p(x|u)$ and $p(u|x)$. The latter is no problem, since
\[u|x \sim \mathcal{U}(0, \tilde{\pi}(x))\]
Sampling from $p(x|u)$ is where things get tricky. Let $S_x := \{x: \hat{\pi}(x) > u\}$, which represents the x-values corresponding to the horizontal ``slice'' of $\tilde{R}$ at fixed height $u$. The other conditional thus has 
distribution
\[x|u \sim \mathcal{U}(S_x)\]
In general, if $\tilde{\pi}(x)$ is unimodal, then $S_x$ might be a disconnected set. This makes uniformly sampling from the slice $S_x$ a challenging problem in general. Algorithms have been proposed to do so, but we will not 
discuss them here. The case where $\tilde{\pi}(x)$ is unimodal is much simpler, since this guarantees that the slice $S_x$ is a connected region (there are no holes or separate pieces). So how to sample uniformly 
from the slice $S_x$ in this simple case? We can use a notion of rejection sampling, trapping $S_x$ in a square region, sampling a point $x^\prime $from this region, and accepting only if $\hat{\pi}(x^\prime) > u$. Each time a sample is rejected, we can rule out all points farther away from $S_x$ than $x^\prime$, thus shrinking the bounding region, and increasing the acceptance probability after each rejection. 

\subsection{Space-Filling Sampling Algorithms}
The purpose of this section is to introduce various sampling algorithms that are often used in the fields of experimental design and computer experiments. I will introduce the 
basic setup in the latter field as the primary motivation here. The setting is as follows: suppose we seek to model a complex process by running a deterministic computer simulation. 
Mathematically, we represent the computer code as a function $f: \mathcal{X} \to \mathcal{Y}$, where $\mathcal{X}$ is a set of valid inputs to the computer program and 
$\mathcal{Y}$ is the set of outputs resulting from running the simulation on a given input value. There are a variety of tasks one might seek to accomplish with such a simulation, with a 
straightforward example being finding an input $x \in \mathcal{X}$, or set of inputs, that produces the ``best'' output $f(x)$. More commonly, we would like to conduct a full \textit{uncertainty analysis}
to gain a sufficient understanding of the variability in the outputs due to the variability in the inputs. We might even want to go a step further and conduct a \textit{sensitivity analysis}, which 
attributes the variability in the outputs to the variability to specific input dimensions. 

As currently described, this problem is completely deterministic. However, in practice we often have some notion as to the quality of certain input variables. We can capture this prior knowledge via a probability distribution $\pi$ over $\mathcal{X}$. If nothing is known about the inputs a priori then $\pi$ may just be the uniform distribution. Assuming a distribution over the input space then induces a distribution over the output space; that is, the simulation is now some unknown transformation of the random inputs $f(X)$, where $X \sim \pi$. Therefore, the
goal of propagating uncertainty as mentioned above is achieved by gaining an understanding of the induced distribution over the outputs. This may come in the form of estimation of the 
distribution function, or we may be content with estimating the mean and variance of $f(X)$.  

\subsection{Two Approaches to Uncertainty Analysis}
\subsubsection{Direct Monte Carlo Approach}
The simplest approach to the uncertainty analysis problem described above 
is simply to draw samples $x_1, \dots, x_n \overset{iid}{\sim} \pi$ and then feed them through the simulator to produce the outputs 
$f(x_1), \dots, f(x_n)$. We can then use this set of outputs to calculate sample statistics of interest; for example, the sample mean
\[\bar{y} := \frac{1}{n} \sum_{i = 1}^{n} y_i\]
where we denote $y_i := f(x_i)$. 
We might also entertain alternative sampling schemes other than the simple random sample considered above that may lead to more efficient estimators. 
However, the high-dimensionality of $\mathcal{X}$ and/or the costly computation time of computing $f(x)$ may render this Monte Carlo approach infeasible, 
even with more clever sampling schemes. It simply might take too much time to run the required number of simulations to achieve reasonable estimates of 
the quantities of interest. The following section addresses these concerns with a new approach. 

\subsubsection{Response Surface Methodology and Surrogate Models}
If the Monte Carlo approach is infeasible, then an alternative is to run the full simulation at a smaller number of so-called \textit{design points}
$x_1, \dots, x_N \in \mathcal{X}$ (where $N$ is much smaller than the $n$ Monte Carlo samples that would be required for estimating the 
quantities of interest). Obtaining the outputs $y_1, \dots, y_n$, we now have a few options. A classical approach is to use local optimization 
methods to construct a \textit{response surface} passing through the design points. This surface acts as an approximation to the true mapping $f$. 
A more modern approach uses Gaussian process regression as a statistical \textit{surrogate} which approximates $f$. Once we have a fitted 
response surface or surrogate model, then (assuming the approximation is adequate and that the surrogate model is fast to run) 
we can use the Monte Carlo approach on the approximation rather than $f$ itself. Note that sampling might play two separate roles in this framework: 
1.) choosing the $N$ design points and 2.) sampling the $n$ Monte Carlo samples to feed through the approximate model. I should emphasize that when 
$f$ is very computationally expensive, then one must be very careful choosing the design points; we want to choose them to maximize the amount of 
information we can get out of a small number of full simulation runs. We would typically not want to simply sample the design points from $\pi$ as we 
are doing for the Monte Carlo samples. The design points are therefore often not drawn directly from $\pi$, but instead chosen using alternative algorithms
that seek to satisfy some useful criteria for what make a set of design points ``good''. 

\subsection{Three Sampling Approaches}
We now consider three sampling algorithms to address the computer experiment problems introduced in the previous sections. Note that these algorithms
are also used in many other contexts as well. Before introducing these new algorithms let's review the basic properties of the random sampling approach, 
and some of its drawbacks in using random sampling for design point selection. A random sample is given by 
\[x_1, \dots, x_n \sim \pi\]
meaning that the $x_i$ are independently sampled from $\pi$. Assuming we know how to sample from $\pi$, this procedure has the benefit of being 
very easy to implement. Moreover, noting that the sample mean, variance, and empirical distribution function can all be written in the form
\[\sum_{i = 1}^{n} \phi(y_i)\]
for some function $\phi$, then the Law of Large Numbers and Central Limit Theorem conveniently apply to these estimators. However, with regard to 
sampling the design points, the random sampling scheme will tend to produce design points that ``clump together'', leaving significant regions 
of $\mathcal{X}$ unexplored. This notion is explored in detail in the following section. A second, related downside is that there may be certain 
regions we'd like to explore, and random sampling gives no guarantee that a design point will be in any given subset of $\mathcal{X}$. 
Assume $\mathcal{X}$ has dimension $d$. Then throughout this section it will be convenient to stack the $d$-dimensional design points in the rows 
of a matrix. We call the resulting matrix $D \in \R^{n \times d}$ the \textit{design matrix}. 

\subsubsection{Choosing Design Points Uniformly at Random}
With the goal of ``spreading points out'' in $\mathcal{X}$, a reasonable first algorithm may simply be to sample $x_1, \dots, x_N$
uniformly at random. However, as mentioned above this approach suffers from ``clumping''. \\[.2cm]
\textbf{TODO: finish this section by proving some clumping results}

\subsubsection{Stratified Sampling}
Stratified sampling is essentially a direct way to address the random sampling and uniform sampling concerns mentioned above. Suppose the input space can be 
partitioned as 
\[\mathcal{X} = P_1 \cup P_2 \cup \cdots \cup P_k\]
where the subsets composing the partition are by definition disjoint. The general stratified sampling algorithm is quite straightforward. 
\begin{itemize}
\item For $j = 1, \dots, k$: 
\item $\qquad$ Take random sample $\{x_{ij}\}_{1 \leq i \leq N_j} \overset{iid}{\sim} \mathcal{U}(P_j)$
\item Return $\{x_{ij}\}_{1 \leq i \leq N_j, 1 \leq j \leq k}$
\end{itemize}
Here I use $\mathcal{U}(P_j)$ to denote the uniform distribution over $P_j$. I have left the algorithm quite general for the time being by allowing an 
arbitrary number of samples $N_j$ from each set $P_j$. Of course, given that the goal is choosing $N$ design points then we require 
$N = \sum_{j = 1}^{k} N_j$. Evidently, the resulting sample $\{x_{ij}\}_{1 \leq i \leq N_j, 1 \leq j \leq k}$ will not be drawn from $\mathcal{U}(\mathcal{X})$
if the $N_j$ are not proportional to the respective probabilities of the $P_j$. For example, consider choosing a large number of samples from a very small
subset of $\mathcal{X}$; the resulting samples will then clearly not be uniformly distribution over $\mathcal{X}$. However, note that setting $k := 1$
and $N_1 := N$ recovers the uniform sampling scheme discussed in the previous section. In the typical case, $N_j = 1$ for all $j$ so that the algorithm samples
a single point from each subset in the partition. 

An immediate benefit of this approach is that we can pick $P_j$ that are of especial interest, thus guaranteeing samples from these regions of interest. 
Since a random sample is taken from each subset, then we can use the typical estimators to estimate quantities of interest for the sub-regions, if this 
is of interest to the practitioner. 
The ability to specify sub-regions of interest is a double-edged sword though, as designing a suitable partition of $\mathcal{X}$ may be very difficult and time-consuming. 
How about estimating the mean, variance, distribution function, etc. of $f(X)$? The key here is to properly weight the summands in the estimator. To illustrate this we consider
the sample mean below. The same weights may be used to adapt the estimators for the other quantities as well. 

\subsection{Truncated and Rectified Distributions}
See: https://stats.stackexchange.com/questions/525894/understanding-the-pdf-of-a-truncated-normal-distribution



\section{Notion of ``Exploration vs. Exploitation'' in Computational Statistics}
Include stochastic gradient based methods, EM, Gibbs sampling here 
In particular, compare (cyclic) coordinate descent and Gibbs (these seem very similar) 

Interesting application of EM algorithm: reinforcement learning: 
http://www.cs.toronto.edu/~fritz/absps/dh97.pdf

\section{Resources}
\begin{itemize}
\item A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code 
\item Latin Hypercube Sampling and the Propagation of Uncertainty in Analyses of Complex Systems (Helton and Davis)
\end{itemize}



\end{document}


