\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Local custom commands. 
\include{local-defs-EKI}
\newcommand{\bphi}{\boldsymbol{\phi}}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Ensemble Kalman Methods for Inversion: Optimization and Sampling}
\author{Andrew Roberts}

\begin{document}

\maketitle
\newpage

\section{Background}

\subsection{Inverse Problems}

\subsection{Filtering Stochastic Dynamical Systems}
We begin by considering a dynamical system which is subject to noise, coupled with an associated observation process. 
We utilize the notation 
\begin{align}
\dstate[\timeStep+1] &= \fwdOne(\dstate) + \indexTimeStep{\noiseDynamic}, 	  		 &&\indexTimeStep{\noiseDynamic} \overset{iid}{\sim} \Gaussian(0, \CovNoiseDynamic) \label{dynamic_model} \\
\ddataObs[\timeStep+1] &= \obsOp(\dstate[\timeStep+1]) + \indexTimeStep[\timeStep+1]{\noiseObs}, &&\indexTimeStep[\timeStep+1]{\noiseObs} \overset{iid}{\sim} \Gaussian(0, \CovNoiseObs) \nonumber \\
\dstate[0] &\sim \Gaussian(\stateMean[0], \stateCov[0]) && \nonumber \dstate[0] \perp \{\indexTimeStep{\noiseDynamic}\} \perp \{\indexTimeStep{\noiseObs}\}, \nonumber 
\end{align}
where $\dstate \in \R^{\dimState}$ is the latent state of interest, described by a deterministic evolution operator $\fwdOne: \R^{\dimState} \to \R^{\dimState}$ which is perturbed by 
Gaussian noise $\indexTimeStep{\noiseDynamic}$. The third line gives a probabilistic initial condition for the dynamical system, also assumed to be Gaussian. 
The second line describes the observation process, in which the observed data $\ddataObs \in \R^{\dimObs}$ is modeled as a
function $\obsOp$ of the underlying state, perturbed by Gaussian observation noise $\noiseObs$. The final line also gives the crucial assumption that the dynamics noise, observation 
noise, and initial condition are all independent. Letting $\dataObsCum := \{\ddataObs[1], \dots, \ddataObs\}$ denote the accumulation of data up through time $\timeStep$, the primary 
target of inference is the \textit{filtering distribution} $\filterMeas$ of the state $\dstate$ given $\dataObsCum$, 
\begin{align}
\dstate|\dataObsCum \sim \filterMeas.
\end{align}
We typically assume the probability measure $\filterMeas$ has a density $\filterDens$ with respect to Lebesgue measure, such that 
\[
\filterMeas(A) = \int_A \filterDens(\state) d\state. 
\]
We now consider how the measure $\filterMeas$ is transformed into $\filterMeas[\timeStep+1]$ one time step in the future. In light of the dynamical model \ref{dynamic_model}, this transformation 
proceeds in two stages. 
\begin{enumerate}
\item \textbf{Prediction:} First the state distribution is propagated through the forward dynamics model. Noting that this model is Markovian, this can be formulated in terms of the associated 
Markov transition kernel $\predictOp$, which we note is time-homogenous under the model assumptions. We denote the distribution resulting from this operation as $\predictMeas[\timeStep+1]$
(with associated density $\predictDens[\timeStep+1]$, when applicable) and refer to it as the \textit{predicted distribution}. This distribution is given by 
\begin{align}
\predictMeas[\timeStep+1](A) := \filterMeas \predictOp(A) &= \int \predictOp(\state, A) \filterMeas(d\state).
\end{align}
We note that the operator $\predictOp$ is linear in the space of measures, and preserves probability measures under convex combinations. Under the model assumptions, this operator 
takes the concrete form 
\begin{align*}
\predictOp(\state, A) &= \int_A \Gaussian(\state^\prime | \state, \CovNoiseDynamic) d\state^\prime
\end{align*}
so that 
\begin{align}
\predictMeas[\timeStep+1](A) &= \int \left[\int_A \Gaussian(\state^\prime | \state, \CovNoiseDynamic) d\state^\prime\right] \filterMeas(d\state). \label{predict_integral}
\end{align}

\item \textbf{Analysis}: In the next stage the predicted distribution $\predictMeas[\timeStep+1]$ is combined with the new observation $\ddataObs[\timeStep+1]$ via Bayes' rule.
This operation can be formalized by the application of an operator $\analysisOp$, which depends on time through the observation $\ddataObs[\timeStep+1]$. Under the independence 
assumptions in \ref{dynamic_model}, this application of Bayes' rule can be written as 
\begin{align*}
\filterMeas[\timeStep+1](A) = \predictMeas[\timeStep+1] \analysisOp(A) = \frac{\int_A \Gaussian(\ddataObs[\timeStep+1]|\obsOp(\state), \CovNoiseObs) \predictMeas[\timeStep+1](d\state)}{\int \Gaussian(\ddataObs[\timeStep+1]|\obsOp(\state), \CovNoiseObs) \predictMeas[\timeStep+1](d\state)}.
\end{align*}
We notice that the numerator defines a linear operator in $\filterMeas[\timeStep+1]$. However, this linear operator is then composed with a non-linear operator which normalizes the measure 
in order to return a probability measure. The operator $\analysisOp$ is thus non-linear. 
\end{enumerate}

We have shown that the filtering distribution update is given by 
\begin{align}
\filterMeas[\timeStep+1] = \predictMeas[\timeStep+1] \analysisOp = \filterMeas \predictOp \analysisOp,
\end{align}
with $\predictOp$ linear Markov operator, and $\analysisOp$ a non-linear likelihood operator. The standard goal in the filtering setting is to approximate these operations in order 
to obtain an approximation of $\filterMeas[\timeStep+1]$. 

\subsection{The Kalman and Ensemble Kalman Filter}

\subsubsection{The Kalman Filter}
Under the additional assumption that the operators $\fwdOne$ and $\obsOp$ in \ref{dynamic_model} are linear, then the filtering distribution update can be performed 
analytically. We show in turn that the operators $\predictOp$ and $\analysisOp$ preserve Gaussianity. Suppose that the current filtering distribution is given by 
$\dstate|\dataObsCum \sim \Gaussian(\stateMean, \stateCov)$. 

\begin{enumerate}
\item \textbf{Prediction}: The prediction update simply follows from noting that 
\begin{align*}
\dstate[\timeStep+1] &= \fwdOne \dstate + \indexTimeStep{\noiseDynamic}, 
&&\dstate \sim \Gaussian(\stateMean, \stateCov), \indexTimeStep{\noiseDynamic} \sim \Gaussian(0, \CovNoiseDynamic)
\end{align*}
under the assumption that $\fwdOne$ is linear implies that $\dstate[\timeStep+1]$ is also Gaussian. Thus, 
\begin{align*}
\dstatePred[\timeStep+1] &\sim \Gaussian(\stateMeanPred[\timeStep+1], \stateCovPred[\timeStep+1]) \\
\stateMeanPred[\timeStep+1] &:= \fwdOne \stateMean \\
\stateCovPred[\timeStep+1]  &:= \fwdOne \stateCov \fwdOne^\top + \CovNoiseDynamic.
\end{align*}

\item \textbf{Analysis}: We write the observation operator as $\obsOpLin \in \R^{\dimObs \times \dimState}$ when it is assumed linear. 
The updated filtering density is given by 
\begin{align*}
\filterDens[\timeStep+1](\dstate[\timeStep+1]) 
&= p(\dstate[\timeStep+1] | \dataObsCum, \ddataObs[\timeStep+1]) \\
&\propto p(\ddataObs[\timeStep+1] | \dstate[\timeStep+1]) \filterDensPred(\dstate[\timeStep+1]) \\
&= \Gaussian(\ddataObs[\timeStep+1] | \obsOpLin \dstate[\timeStep+1], \CovNoiseObs) \Gaussian(\dstate[\timeStep+1] | \stateMeanPred[\timeStep+1], \stateCovPred[\timeStep+1]).
\end{align*}
The analysis update thus reduces to finding the posterior distribution of a standard Gaussian linear regression model. The well-known form of the posterior gives 
\begin{align*}
\dstate[\timeStep+1] | \dataObsCum[\timeStep+1] &\sim \Gaussian(\stateMean[\timeStep+1], \stateCov[\timeStep+1]) \\
\stateCov[\timeStep+1] &= \left[\stateCovPred[\timeStep+1]^{-1} + \obsOpLin^\top \CovNoiseObs^{-1} \obsOpLin \right]^{-1} \\
\stateMean[\timeStep+1] &= \stateCov[\timeStep+1] \left[\stateCovPred[\timeStep+1]^{-1} \stateMeanPred[\timeStep+1] + \obsOpLin^\top \CovNoiseObs^{-1} \ddataObs[\timeStep+1] \right].
\end{align*}  
In applications with high-dimensional state spaces such that $\dimState >> \dimObs$, the inversion of the $\dimState \times \dimState$ matrix in the above update formula may be 
computationally intractable. An application of the Woodbury matrix identity allows these formulae to be re-written in terms of an $\dimObs \times \dimObs$ matrix inverse. 
The equivalent mean and covariance update is given by
\begin{align*}
\stateCov[\timeStep+1] &= \left[\idMat_{\dimState} - \KGain[\timeStep+1] \obsOpLin \right] \stateCovPred[\timeStep+1] \\
\stateMean[\timeStep+1] &= \stateMeanPred[\timeStep+1] + \KGain[\timeStep+1] \left[\ddataObs[\timeStep+1] - \obsOpLin \stateMeanPred[\timeStep+1] \right] \\
\KGain[\timeStep+1] &:= \stateCovPred[\timeStep+1] \obsOpLin^\top \left[\obsOpLin \stateCovPred[\timeStep+1] \obsOpLin^\top + \CovNoiseObs \right]^{-1}
\end{align*}
where the matrix $\KGain[\timeStep+1]$ is referred to as the \textit{Kalman gain}. 
\end{enumerate}


\subsubsection{The Ensemble Kalman Filter}


\subsection{Langevin Diffusions}

\section{Ensemble Kalman Methods for Inverse Problems}
We now turn our attention to the application of ensemble Kalman methods to the solution of Bayesian inverse problems. 
The generic setting of interest is 
\begin{align}
\dataObs &= \fwd(\spar) + \noise \label{inverse_problem} \\
\noise &\sim \Gaussian(0, \covObsNoise) \nonumber \\
\spar &\sim \Gaussian(\priorMean, \priorCov), \nonumber
\end{align}
where $\spar \in \R^{\Npar}$ is the unknown parameter of interest and $\dataObs \in \R^{\dimData}$ is an observed 
data vector which is assumed to result from a potentially non-linear forward mapping $\fwd: \R^{\Npar} \to \R^{\dimData}$
corrupted by Gaussian noise $\noise$. We consider the Bayesian approach, whereby the parameter is equipped with 
Gaussian prior and the goal is to characterize the posterior distribution $\spar|\dataObs \sim \postMeas$. 

We first discuss ensemble Kalman inversion (EKI), which can be understood as an ensemble approach to approximating the 
maximum a posteriori (MAP) estimate of $\postMeas$. We then build on EKI and introduce the ensemble Kalman sampler (EKS), 
which produces approximate samples from $\postMeas$.  

\subsection{Ensemble Kalman Inversion}
While there are many ways to introduce the EKI algorithm, we follow the approach detailed in \textbf{TODO}, which introduces 
artificial dynamics into the inverse problem \ref{inverse_problem} in order to apply an ensemble Kalman approach. We begin 
by noting that \ref{inverse_problem} can be viewed as a rather trivial dynamical system with state vector $\spar$,
\begin{align}
\dpar[\timeStep+1] &\sim \Gaussian(\priorMean, \priorCov) \label{inverse_problem_dynamic1} \\
\indexTimeStep[\timeStep+1]{\dataObs} &= \fwd(\dpar[\timeStep+1]) + \indexTimeStep[\timeStep+1]{\noise}. \nonumber
\end{align}
In this formulation, $\fwd$ is thought of as the observation operator. In order to re-write \ref{inverse_problem_dynamic1}
in a form more suitable to application of the EnKF, we would like the observation operator to be linear. We can achieve this
by introducing a new state
\begin{align}
\dexState := \begin{pmatrix} \dpar & \fwd(\dpar) \end{pmatrix}^\top \in \R^{\Npar + \dimData}.
\end{align}
The observation equation in \ref{inverse_problem_dynamic1} can now be written as 
\begin{align*}
\indexTimeStep[\timeStep+1]{\dataObs} 
&= \begin{pmatrix} 0 & \idMat_{\dimData} \end{pmatrix} \begin{pmatrix} \dpar[\timeStep+1] \\ \fwd(\dpar[\timeStep+1]) \end{pmatrix} + \indexTimeStep[\timeStep+1]{\noise} \\
&= \obsOpLin \dexState[\timeStep+1] + \indexTimeStep[\timeStep+1]{\noise} 
\end{align*}
where we have defined the linear observation operator 
\begin{align}
\obsOpLin := \begin{pmatrix} 0 & \idMat_{\dimData} \end{pmatrix} \in \R^{\dimData \times (\Npar + \dimData)}.
\end{align}
The dynamics in $\dexState$ are trivially given by the identity operator 
\begin{align*}
\dexState[\timeStep+1] &= \idMat \dexState[\timeStep],
\end{align*}
which will imply that the ensemble of particles will only be updated during the analysis step of the EnKF. The complete extended state space
model is thus given by 
\begin{align}
\dexState[\timeStep+1] &= \idMat \dexState[\timeStep] \label{inverse_problem_dynamic} \\
\indexTimeStep[\timeStep+1]{\dataObs} &= \obsOpLin \dexState[\timeStep+1] + \indexTimeStep[\timeStep+1]{\noise} \nonumber.
\end{align}


\end{document} 






