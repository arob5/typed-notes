\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Laws of Large Numbers and Central Limit Theorems}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage


% Introduction
\section{Introduction}
In this writeup I banish all but the simplest proofs to the appendix, in an effort to focus the main body of the text on big picture concepts and intuition. 

% Notions of Convergence
\section{Notions of Convergence}
A random variable is simply a measurable function $X: \Omega \to \R$, where $(\Omega, \mathcal{F}, \mathbb{P})$ is a measure space. Therefore, convergence of random variables
really just boils down to convergence of functions. What differentiates such convergence from that studied in real analysis is that the input space $\Omega$ of the function is \textit{weighted}, 
whereas in classical real analysis there is no weighting scheme that gives more emphasis to certain input values $\omega \in \Omega$ and less to others. Ignoring the weights implied by the 
probability measure, we could simply consider the standard notion of \textit{pointwise convergence} of a sequence of random variables $\{X_n\}$ to some limiting random variable $X$:
\begin{align}
\lim_{n \to \infty} X_n(\omega) = X(\omega), \text{ for all } \omega \in \Omega \label{Pointwise_Convergence}
\end{align}
However, as we already discussed, this completely ignores $\mathbb{P}$. Convergence of random variables should naturally incorporate knowledge of this measure, which intuitively tells us 
how much emphasis should be placed on particular $\omega$. The notion of pointwise convergence [\ref{Pointwise_Convergence}] treats all $\omega$ equally. There are many different reasonable
ways that we might incorporate $\mathcal{P}$ into the definition of a limit, yielding different notions of convergence, each of which have strengths and weaknesses with respect to particular applications. 
We consider the most popular notions of convergence below. 

\subsection{Almost Sure Convergence}
Almost sure convergence incorporates knowledge of $\mathcal{P}$ in the simplest manner possible--it simply ignores the set of $\omega$ with zero probability. This is a quite natural slight adjustment 
of pointwise convergence; if $\mathcal{P}$ assigns zero probability to a set $A \in \mathcal{F}$, then requiring $\{X_n\}$ to converge on this set seems to be overly restrictive. Almost sure convergence 
simply removes this restriction. However, beyond this adjustment it still does not consider any sort of weighting for the set of positive measure. It just partitions the set of positive measure from that of zero 
measure, and ignores the latter. 

\begin{definition}
A sequence of random variables $\{X_n\}$ is said to converge almost surely (a.s.) to a random variable $X$ provided that 
\[\mathbb{P}\left(\lim_{n \to \infty} X_n(\omega) = X(\omega)\right) = 1\]
As shorthand, we write $X_n \overset{a.s.}{\to} X$.
\end{definition}  
To be more explicit, we might write out the definition as 
\[\mathbb{P}\left(\left\{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) = X(\omega)\right\} \right) = 1\]
or equivalently, 
\[\mathbb{P}\left(\left\{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) \neq X(\omega)\right\} \right) = 0\]
Convergence in probability says that by going far enough along in the sequence we can make $X_n(\omega)$ and $X(\omega)$ arbitrarily close for almost all $\omega$.

To dig into this a bit deeper, we recall the definition of the standard pointwise limit $\lim_{n \to \infty} X_n(\omega) = X(\omega)$ from real analysis: 
\begin{align*}
\forall \epsilon > 0, \ \exists N_{\epsilon, \omega} \in \mathbb{N} \text{ s.t. } \forall n \geq N_{\epsilon, \omega}, \ \abs{X_n(\omega) - X(\omega)} < \epsilon 
\end{align*} 
where I utilize the subscripts $N_{\epsilon, \omega}$ to emphasize that $N$ depends on both $\epsilon$ and $\omega$. Almost sure convergence says that existence of such an $N$ for 
every $\epsilon > 0$ holds for almost all $\omega \in \Omega$. It can be shown that this implies for any $\epsilon > 0$ the existence of an $N \in \mathbb{N}$ satisfying 
\[\Prob(\abs{X_n(\omega) - X(\omega)} < \epsilon) = 1, \text{ for all } n \geq N\]
I find this statement to provide the best interpretation of almost sure convergence. Let's call the event $\{X_n(\omega) \text{ is outside of an } \epsilon\text{-ball of } X(\omega)\}$ an \textit{unusual event} 
(where the radius $\epsilon$ is a measure of how unusual). Then $X_n(\omega) \overset{a.s.}{\to} X(\omega)$ means that if we go far enough along in the sequence we can drive the probability of 
an unusual event to \textit{zero}. This is a powerful statement, though it gives no sense of how far along in the sequence one must go to achieve this. This interpretation of almost sure convergence is also 
quite helpful when trying to understand its difference from convergence in probability, which is discussed below. 

\subsection{Convergence in Probability}
Another reasonable way to define convergence that takes the probability measure into account is to consider the probability that $X_n$ is ``far from'' $X$ as a function function of $n$, and require that this 
probability converge to $0$ as $n \to \infty$.  
\begin{definition}
A sequence of random variables $\{X_n\}$ is said to converge in probability to a random variable $X$ provided that for any $\epsilon > 0$,
\[\lim_{n \to \infty} \Prob\left(\abs{X_n(\omega) - X(\omega)} \geq \epsilon \right) = 0\]
As shorthand, we write $X_n \overset{p}{\to} X$.
\end{definition}
This is certainly related almost sure convergence, and I find it quite easy to accidentally conflate the two. Note that in the definition of almost sure convergence the probability measure appears outside of the limit, 
while here it appears inside of the limit. To better understand the difference, we consider manipulating the above definition. 
Plugging in the standard definition of a limit, we obtain the equivalent 
definition that for any $\epsilon > 0$,
\[\forall \epsilon^\prime > 0, \text{ there exists } N \in \mathbb{N} \text{ s.t. for all } n \geq N, \ \Prob\left(\abs{X_n(\omega) - X(\omega)} \geq \epsilon \right) < \epsilon^\prime \]
Now we're in a nice position to compare to almost sure convergence. For $\epsilon > 0$, we found that almost sure convergence guaranteed the existence of an $N \in \mathbb{N}$
satisfying 
\[\Prob(\abs{X_n(\omega) - X(\omega)} \geq \epsilon) = 0, \text{ for all } n \geq N\]
while convergence in probability guarantees the existence of an $N$ satisfying 
\[\Prob(\abs{X_n(\omega) - X(\omega)} \geq \epsilon) < \epsilon^\prime, \text{ for all } n \geq N, \text{ for any } \epsilon^\prime > 0\]
That is, almost sure convergence guarantees a finite number of unusual events--by choosing $N$ large enough the probability of an unusual event may be driven to zero. Convergence in 
probability gives a weaker guarantee; the probability of an unusual event my be driven to be arbitrarily small, but is not guaranteed to ever reach zero. To better understand this distinction, 
we consider a specific example. 

\subsubsection{Example: A sequence that converges in probability but not almost surely}
TODO

% WLLN
\section{Weak Laws of Large Numbers}
This section details a variety of weak laws of large numbers (WLLNs), which concern the convergence of sums of random variables in probability. Recall from the previous section that intuitively we can think 
of these theorems as guaranteeing that the ``error rate'' or probability of an ``unusual event'' becomes arbitrarily small as $N$ increases, but they are weak in the sense that the error probability is never guaranteed 
to reach zero. There are many WLLNs that make different assumptions and conclusions, but they are all generally concerned with the convergence in probability of sums or empirical means of random variables. 

\subsection{Simplest Case: Finite Variance}
We begin with the most basic WLLN, which assumes the random variables are iid with finite variance. The finite variance assumption guarantees that the probability distribution is not too ``spread out'', which implies 
that taking empirical means will reduce the randomness in the distribution and tend toward a single value. With this assumption, the WLLN is quite easy to prove, following from a simple application of Chebyshev's 
inequality. 
\begin{thm} 
Let $\{X_k\}_{k = 1}^{\infty}$ be a sequence of iid random variables with finite mean $\mu := \E X_1 < \infty$ and variance $\sigma^2 := \Var(X_1) < \infty$. Define the partial sum $S_N := \sum_{k = 1}^{N} X_k$. Then 
\[\frac{S_N}{N} \overset{p}{\to} \mu\]
\end{thm}

\begin{proof}
Let $\epsilon > 0$. We must show that 
\[\lim_{N \to \infty} \Prob\left(\abs{\frac{S_N}{N} - \mu} > \epsilon \right) = 0\]
To apply Chebyshev, we will require the variance $\Var(S_N)$. This is easily calculated using the iid assumption and finite variance assumption:
\[\Var(S_N) = \Var\left(\sum_{k = 1}^{N} X_k \right) = \sum_{k = 1}^{N} \Var(X_k) =  \sum_{k = 1}^{N} \sigma^2 = N\sigma^2 \]
By Chebyshev, 
\begin{align*}
\Prob\left(\abs{\frac{S_N}{N} - \mu} > \epsilon \right) &\leq \frac{\Var\left(\frac{S_N}{N}\right)}{\epsilon^2} \\
									       &= \frac{\sigma^2}{N\epsilon^2} \to 0 \text{ as } N \to \infty 
\end{align*}
\end{proof}

\subsection{What happens when variance is infinite?}
Speaking very loosely, we described the finite variance assumption as a guarantee that the distribution of the $X_k$ is not too spread out, allowing the distribution of the partial sum $S_N$ to collapse as $N$ is increased. 
It turns out that this assumption is actually not necessary at all--we can conclude the same exact result without the finite variance assumption. However, the finite mean assumption is still required, which plays a similar role by 
bounding how spread out the distribution can be. The proof of this more general result is longer, but still not too bad. It is included below, as the ``truncated Chebyshev'' technique that is employed is a very common strategy to 
deal with cases of infinite variance. This technique is fairly intuitive: modify $X_k$ (which has infinite variance) by chopping off its values outside of a specified interval, leading to a modified random variable with finite support and 
hence finite variance. We can then apply Chebyshev to this truncated random variable. As we extend the truncation interval, the truncated variables approach the original ones, and we hope that in the limit the partial sums of these 
two respective sequences will have the same limit. A shorter proof using characteristic functions is given in the appendix, but the truncation technique is presented here as it will sometimes be applicable when the 
characteristic function technique is not. 

\begin{thm}
Let $\{X_k\}_{k = 1}^{\infty}$ be a sequence of iid random variables with finite mean $\mu := \E X_1 < \infty$. Define the partial sum $S_N := \sum_{k = 1}^{N} X_k$. Then 
\[\frac{S_N}{N} \overset{p}{\to} \mu\]
\end{thm}

\begin{proof}
Let $\epsilon > 0$. 
We must show that 
\[\lim_{N \to \infty} \Prob\left(\abs{\frac{S_N}{N} - \mu} > \epsilon \right) = 0\]
Define the truncated random variables 
\[Y_{N,k} := X_k \mathbbm{1}\{\abs{X_k} \leq N \epsilon^3\}\]
The $\epsilon^3$ is chosen precisely because it makes the bound come out correctly, as we will see. Therefore, $Y_{N,k}$ agrees with $X_k$ on the set
 $\{\omega \in \Omega: \abs{X(\omega)} \leq N\epsilon^3\}$, and outside of this set $Y_{N, k}$ is set to $0$. Therefore, in absolute value $\abs{Y_{N,k}} \leq \abs{X_{N,k}}$. Let $S_N^\prime := \sum_{k = 1}^{N} Y_{N,k}$
 denote the partial sums of the sequence $\{Y_{N, k}\}$. Note also that, as functions of independent random variables the sequence $\{Y_N\}_{k = 1}^{\infty}$ is independent, for each $N$. 
 
 The strategy here is to apply the triangle inequality 
 \begin{align*}
 \Prob\left(\abs{\frac{S_N}{N} - \mu} > \epsilon \right) &\leq \Prob\left(\abs{\frac{S_N}{N} - \frac{\E S^\prime_N}{N} } + \abs{\frac{\E S^\prime_N}{N}  - \mu} > \epsilon \right) \\
 										&\leq \Prob\left(\left\{\abs{\frac{S_N}{N} - \frac{\E S^\prime_N}{N} } > \frac{\epsilon}{2} \right\} + \left\{\abs{\frac{\E S^\prime_N}{N}  - \mu} > \frac{\epsilon}{2}\right\} \right) \\
										&\leq \Prob\left(\abs{\frac{S_N}{N} - \frac{\E S^\prime_N}{N} } > \frac{\epsilon}{2}\right) + \Prob\left(\abs{\frac{\E S^\prime_N}{N}  - \mu} > \frac{\epsilon}{2}\right)
 \end{align*}
 If we show that 
 \begin{enumerate}
 \item $\frac{\E S_N^\prime}{N} \overset{p}{\to} \mu$
 \item $\frac{S_N}{N} - \frac{\E S_N^\prime}{N} \overset{p}{\to} 0$
 \end{enumerate}
 then we can drive each of the terms to $0$ to prove the result. 
 
\end{proof}


\end{document}









