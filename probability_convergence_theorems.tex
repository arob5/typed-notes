\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Laws of Large Numbers and Central Limit Theorems}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage


% Introduction
\section{Introduction}
In this writeup I banish all but the simplest proofs to the appendix, in an effort to focus the main body of the text on big picture concepts and intuition. 

% Notions of Convergence
\section{Notions of Convergence}
A random variable is simply a measurable function $X: \Omega \to \R$, where $(\Omega, \mathcal{F}, \mathbb{P})$ is a measure space. Therefore, convergence of random variables
really just boils down to convergence of functions. What differentiates such convergence from that studied in real analysis is that the input space $\Omega$ of the function is \textit{weighted}, 
whereas in classical real analysis there is no weighting scheme that gives more emphasis to certain input values $\omega \in \Omega$ and less to others. Ignoring the weights implied by the 
probability measure, we could simply consider the standard notion of \textit{pointwise convergence} of a sequence of random variables $\{X_n\}$ to some limiting random variable $X$:
\begin{align}
\lim_{n \to \infty} X_n(\omega) = X(\omega), \text{ for all } \omega \in \Omega \label{Pointwise_Convergence}
\end{align}
However, as we already discussed, this completely ignores $\mathbb{P}$. Convergence of random variables should naturally incorporate knowledge of this measure, which intuitively tells us 
how much emphasis should be placed on particular $\omega$. The notion of pointwise convergence [\ref{Pointwise_Convergence}] treats all $\omega$ equally. There are many different reasonable
ways that we might incorporate $\mathcal{P}$ into the definition of a limit, yielding different notions of convergence, each of which have strengths and weaknesses with respect to particular applications. 
We consider the most popular notions of convergence below. 

\subsection{Almost Sure Convergence}
Almost sure convergence incorporates knowledge of $\mathcal{P}$ in the simplest manner possible--it simply ignores the set of $\omega$ with zero probability. This is a quite natural slight adjustment 
of pointwise convergence; if $\mathcal{P}$ assigns zero probability to a set $A \in \mathcal{F}$, then requiring $\{X_n\}$ to converge on this set seems to be overly restrictive. Almost sure convergence 
simply removes this restriction. However, beyond this adjustment it still does not consider any sort of weighting for the set of positive measure. It just partitions the set of positive measure from that of zero 
measure, and ignores the latter. 

\begin{definition}
A sequence of random variables $\{X_n\}$ is said to converge almost surely (a.s.) to a random variable $X$ provided that 
\[\mathbb{P}\left(\lim_{n \to \infty} X_n(\omega) = X(\omega)\right) = 1\]
As shorthand, we write $X_n \overset{a.s.}{\to} X$.
\end{definition}  
To be more explicit, we might write out the definition as 
\[\mathbb{P}\left(\left\{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) = X(\omega)\right\} \right) = 1\]
or equivalently, 
\[\mathbb{P}\left(\left\{\omega \in \Omega: \lim_{n \to \infty} X_n(\omega) \neq X(\omega)\right\} \right) = 0\]
Convergence in probability says that by going far enough along in the sequence we can make $X_n(\omega)$ and $X(\omega)$ arbitrarily close for almost all $\omega$.

To dig into this a bit deeper, we recall the definition of the standard pointwise limit $\lim_{n \to \infty} X_n(\omega) = X(\omega)$ from real analysis: 
\begin{align*}
\forall \epsilon > 0, \ \exists N_{\epsilon, \omega} \in \mathbb{N} \text{ s.t. } \forall n \geq N_{\epsilon, \omega}, \ \abs{X_n(\omega) - X(\omega)} < \epsilon 
\end{align*} 
where I utilize the subscripts $N_{\epsilon, \omega}$ to emphasize that $N$ depends on both $\epsilon$ and $\omega$. Almost sure convergence says that existence of such an $N$ for 
every $\epsilon > 0$ holds for almost all $\omega \in \Omega$. It can be shown that this implies for any $\epsilon > 0$ the existence of an $N \in \mathbb{N}$ satisfying 
\[\Prob(\abs{X_n(\omega) - X(\omega)} < \epsilon) = 1, \text{ for all } n \geq N\]
I find this statement to provide the best interpretation of almost sure convergence. Let's call the event $\{X_n(\omega) \text{ is outside of an } \epsilon\text{-ball of } X(\omega)\}$ an \textit{unusual event} 
(where the radius $\epsilon$ is a measure of how unusual). Then $X_n(\omega) \overset{a.s.}{\to} X(\omega)$ means that if we go far enough along in the sequence we can drive the probability of 
an unusual event to \textit{zero}. This is a powerful statement, though it gives no sense of how far along in the sequence one must go to achieve this. This interpretation of almost sure convergence is also 
quite helpful when trying to understand its difference from convergence in probability, which is discussed below. 

\subsection{Convergence in Probability}
Another reasonable way to define convergence that takes the probability measure into account is to consider the probability that $X_n$ is ``far from'' $X$ as a function function of $n$, and require that this 
probability converge to $0$ as $n \to \infty$.  
\begin{definition}
A sequence of random variables $\{X_n\}$ is said to converge in probability to a random variable $X$ provided that for any $\epsilon > 0$,
\[\lim_{n \to \infty} \Prob\left(\abs{X_n(\omega) - X(\omega)} \geq \epsilon \right) = 0\]
As shorthand, we write $X_n \overset{p}{\to} X$.
\end{definition}
This is certainly related almost sure convergence, and I find it quite easy to accidentally conflate the two. Note that in the definition of almost sure convergence the probability measure appears outside of the limit, 
while here it appears inside of the limit. To better understand the difference, we consider manipulating the above definition. 
Plugging in the standard definition of a limit, we obtain the equivalent 
definition that for any $\epsilon > 0$,
\[\forall \epsilon^\prime > 0, \text{ there exists } N \in \mathbb{N} \text{ s.t. for all } n \geq N, \ \Prob\left(\abs{X_n(\omega) - X(\omega)} \geq \epsilon \right) < \epsilon^\prime \]
Now we're in a nice position to compare to almost sure convergence. For $\epsilon > 0$, we found that almost sure convergence guaranteed the existence of an $N \in \mathbb{N}$
satisfying 
\[\Prob(\abs{X_n(\omega) - X(\omega)} \geq \epsilon) = 0, \text{ for all } n \geq N\]
while convergence in probability guarantees the existence of an $N$ satisfying 
\[\Prob(\abs{X_n(\omega) - X(\omega)} \geq \epsilon) < \epsilon^\prime, \text{ for all } n \geq N, \text{ for any } \epsilon^\prime > 0\]
That is, almost sure convergence guarantees a finite number of unusual events--by choosing $N$ large enough the probability of an unusual event may be driven to zero. Convergence in 
probability gives a weaker guarantee; the probability of an unusual event my be driven to be arbitrarily small, but is not guaranteed to ever reach zero. To better understand this distinction, 
we consider a specific example. 

\subsubsection{Example: A sequence that converges in probability but not almost surely}
TODO

% WLLN
\section{Weak Laws of Large Numbers}
This section details a variety of weak laws of large numbers (WLLNs), which concern the convergence of sums of random variables in probability. Recall from the previous section that intuitively we can think 
of these theorems as guaranteeing that the ``error rate'' or probability of an ``unusual event'' becomes arbitrarily small as $N$ increases, but they are weak in the sense that the error probability is never guaranteed 
to reach zero. There are many WLLNs that make different assumptions and conclusions, but they are all generally concerned with the convergence in probability of sums or empirical means of random variables. 

\subsection{Simplest Case: Finite Variance}
We begin with the most basic WLLN, which assumes the random variables are iid with finite variance. The finite variance assumption guarantees that the probability distribution is not too ``spread out'', which implies 
that taking empirical means will reduce the randomness in the distribution and tend toward a single value. With this assumption, the WLLN is quite easy to prove, following from a simple application of Chebyshev's 
inequality. 
\begin{thm} 
Let $\{X_k\}_{k = 1}^{\infty}$ be a sequence of iid random variables with finite mean $\mu := \E X_1 < \infty$ and variance $\sigma^2 := \Var(X_1) < \infty$. Define the partial sum $S_N := \sum_{k = 1}^{N} X_k$. Then 
\[\frac{S_N}{N} \overset{p}{\to} \mu\]
\end{thm}

\begin{proof}
Let $\epsilon > 0$. We must show that 
\[\lim_{N \to \infty} \Prob\left(\abs{\frac{S_N}{N} - \mu} > \epsilon \right) = 0\]
To apply Chebyshev, we will require the variance $\Var(S_N)$. This is easily calculated using the iid assumption and finite variance assumption:
\[\Var(S_N) = \Var\left(\sum_{k = 1}^{N} X_k \right) = \sum_{k = 1}^{N} \Var(X_k) =  \sum_{k = 1}^{N} \sigma^2 = N\sigma^2 \]
By Chebyshev, 
\begin{align*}
\Prob\left(\abs{\frac{S_N}{N} - \mu} > \epsilon \right) &\leq \frac{\Var\left(\frac{S_N}{N}\right)}{\epsilon^2} \\
									       &= \frac{\sigma^2}{N\epsilon^2} \to 0 \text{ as } N \to \infty 
\end{align*}
\end{proof}

\subsection{What happens when variance is infinite?}
Speaking very loosely, we described the finite variance assumption as a guarantee that the distribution of the $X_k$ is not too spread out, allowing the distribution of the partial sum $S_N$ to collapse as $N$ is increased. 
It turns out that this assumption is actually not necessary at all--we can conclude the same exact result without the finite variance assumption. However, the finite mean assumption is still required, which plays a similar role by 
bounding how spread out the distribution can be. The proof of this more general result is longer, but still not too bad. It is included below, as the ``truncated Chebyshev'' technique that is employed is a very common strategy to 
deal with cases of infinite variance. This technique is fairly intuitive: modify $X_k$ (which has infinite variance) by chopping off its values outside of a specified interval, leading to a modified random variable with finite support and 
hence finite variance. We can then apply Chebyshev to this truncated random variable. As we extend the truncation interval, the truncated variables approach the original ones, and we hope that in the limit the partial sums of these 
two respective sequences will have the same limit. In order for the truncated Chebyshev argument to work, intuitively the key will be making sure the tails of the $X_k$ decay sufficiently fast. The finite variance assumption guaranteed
this, but we will show below that the finite mean assumption is actually sufficient as well. 

A shorter proof using characteristic functions is given in the appendix, but the truncation technique is presented here as it will sometimes be applicable when the 
characteristic function technique is not. 

\begin{thm}
Let $\{X_k\}_{k = 1}^{\infty}$ be a sequence of iid random variables with finite mean $\mu := \E X_1 < \infty$. Define the partial sum $S_N := \sum_{k = 1}^{N} X_k$. Then 
\[\frac{S_N}{N} \overset{p}{\to} \mu\]
\end{thm}

\begin{proof}
Let $\epsilon > 0$. 
We must show that 
\[\lim_{N \to \infty} \Prob\left(\abs{\frac{S_N}{N} - \mu} > \epsilon \right) = 0\]
Define the truncated random variables 
\[Y_{N,k} := X_k \mathbbm{1}\{\abs{X_k} \leq N \epsilon^3\}\]
The $\epsilon^3$ is chosen precisely because it makes the bound come out correctly, as we will see. Therefore, $Y_{N,k}$ agrees with $X_k$ on the set
 $\{\omega \in \Omega: \abs{X(\omega)} \leq N\epsilon^3\}$, and outside of this set $Y_{N, k}$ is set to $0$. Therefore, in absolute value $\abs{Y_{N,k}} \leq \abs{X_{N,k}}$. Let $S_N^\prime := \sum_{k = 1}^{N} Y_{N,k}$
 denote the partial sums of the sequence $\{Y_{N, k}\}$. Note also that, for a fixed $N$, then each $Y_{N, k}$ is a fixed function of the iid random variables $X_k$ and hence are themselves iid.
 
 The strategy here is to apply the triangle inequality 
 \begin{align*}
 \Prob\left(\abs{\frac{S_N}{N} - \mu} > \epsilon \right) &\leq \Prob\left(\abs{\frac{S_N}{N} - \frac{\E S^\prime_N}{N} } + \abs{\frac{\E S^\prime_N}{N}  - \mu} > \epsilon \right) \\
 										&\leq \Prob\left(\left\{\abs{\frac{S_N}{N} - \frac{\E S^\prime_N}{N} } > \frac{\epsilon}{2} \right\} \bigcup \left\{\abs{\frac{\E S^\prime_N}{N}  - \mu} > \frac{\epsilon}{2}\right\} \right) \\
										&\leq \Prob\left(\abs{\frac{S_N}{N} - \frac{\E S^\prime_N}{N} } > \frac{\epsilon}{2}\right) + \Prob\left(\abs{\frac{\E S^\prime_N}{N}  - \mu} > \frac{\epsilon}{2}\right)
 \end{align*}
 If we show that 
 \begin{enumerate}
 \item $\frac{\E S_N^\prime}{N} \to \mu$ (typical convergence for deterministic sequences)
 \item $\frac{S_N}{N} - \frac{\E S_N^\prime}{N} \overset{p}{\to} 0$
 \end{enumerate}
 then we can drive each of the terms to $0$ to prove the result. Let's begin with the first item, which is the easier of the two. \\
 
 \bigskip
 \noindent
 \textbf{1. Showing $\frac{\E S_N^\prime}{N} \to \mu$.} \\
 Since $\{Y_{N,k}\}_{k = 1}^{\infty}$ form a sequence of iid random variables, then the expectation of $S_N^\prime$ simplifies to 
 \begin{align*}
 \E S_N^\prime &= N \cdot \E\left[X_1 \mathbbm{1}\{\abs{X_1} \leq N\epsilon^3\} \right]
 \end{align*}
 Therefore, 
 \[\frac{\E S_N^\prime}{N} = \E\left[X_1 \mathbbm{1}\{\abs{X_1} \leq N\epsilon^3\}\right]\]
 which I claim converges in probability to $\mu$ by the dominated convergence theorem. Indeed, we have 
 \[X_1 \mathbbm{1}\{\abs{X_1} \leq N\epsilon^3\} \leq \abs{X_1}\]
 and 
 \[X_1 \mathbbm{1}\{\abs{X_1} \leq N\epsilon^3\} \overset{a.s.}{\to} X_1 \]
 The dominated convergence theorem therefore allows interchanging the limit and expectation, which yields
 \[\frac{\E S_N^\prime}{N} = \E\left[X_1 \mathbbm{1}\{\abs{X_1} \leq N\epsilon^3\}\right] \to \E X_1 = \mu \]
 as desired. 
 
 \bigskip
 \noindent
 \textbf{2. Showing $\frac{S_N}{N} - \frac{\E S_N^\prime}{N} \overset{p}{\to} 0$.} \\
 Note that we cannot immediately apply Chebyshev to bound $\Prob\left(\abs{\frac{S_N}{N} - \frac{\E S_N^\prime}{N}} > \epsilon\right) = \Prob\left(\abs{S_N - \E S_N^\prime} > N\epsilon\right)$ since 
 $S_N$ may have infinite variance. We therefore first utilize a crude bound to write the expression in terms of $S_N^\prime$, which does have finite variance. 
 \begin{align*}
 \Prob\left(\abs{S_N - \E S_N^\prime} > N\epsilon\right) &= \Prob\left(\left[\abs{S_N - \E S_N^\prime} > N\epsilon\right] \cap \left[S_N = S_N^\prime\right] \right) +  \Prob\left(\left[\abs{S_N - \E S_N^\prime} > N\epsilon\right] \cap \left[S_N \neq S_N^\prime\right] \right) \\
 										    &\leq \Prob\left(\abs{S^\prime_N - \E S_N^\prime} > N\epsilon \right) + \Prob(S_N \neq S_N^\prime) \\
										    &= \Prob\left(\abs{S^\prime_N - \E S_N^\prime} > N\epsilon \right) + \Prob\left(\sum_{k = 1}^{N} (X_k - Y_{N,k}) \neq 0\right) \\
										    &\leq \frac{\Var(S_N^\prime)}{N^2 \epsilon^2} + \Prob\left(\bigcup_{k = 1}^{N} \{X_k \neq Y_{N,k}\} \right) \\
										    &\leq \frac{\Var(S_N^\prime)}{N^2 \epsilon^2} + \sum_{k = 1}^{N} \Prob(X_k \neq Y_{N,k}) \\
										    &= \frac{\Var(S_N^\prime)}{N^2 \epsilon^2} + \sum_{k = 1}^{N} \Prob(\abs{X_k} > N\epsilon^3) \\
										    &= \frac{\Var(S_N^\prime)}{N^2 \epsilon^2} + N \cdot \Prob(\abs{X_1} > N\epsilon^3)
 \end{align*}
 The second inequality follows from Chebyshev (left term) and the fact that $\left\{\sum_{k = 1}^{N} (X_k - Y_{N,k}) \neq 0\right\} \subset \bigcup_{k = 1}^{N} \{X_k \neq Y_{N,k}\}$ (right term). The latter says that for the sum to 
 be non-zero, at least one of the terms needs to be non-zero. We continue by bounding the first term $\frac{\Var(S_N^\prime)}{N^2 \epsilon^2}$. 
 \begin{align*}
 \frac{\Var(S_N^\prime)}{N^2 \epsilon^2} &= \frac{N \cdot \Var(Y_{N,1})}{N^2 \epsilon^2} \\
 							      &=  \frac{\Var(Y_{N,1})}{N \epsilon^2} \\
							      &= \frac{\E[Y_{N,1}^2] - \E[Y_{N,1}]^2}{N\epsilon^2} \\
							      &\leq  \frac{\E[Y_{N,1}^2]}{N\epsilon^2} \\
							      &\leq \frac{1}{N\epsilon^2} \int_{\{\abs{X_1} \leq N\epsilon^3\}} X_1^2(\omega) d\Prob(\omega) \\
							      &= \frac{1}{N\epsilon^2} \int_{\{\abs{X_1} \leq N\epsilon^3\}} \abs{X_1(\omega)}\abs{X_1(\omega)} d\Prob(\omega) \\
							      &\leq \frac{1}{N\epsilon^2} \int_{\{\abs{X_1} \leq N\epsilon^3\}} \abs{X_1(\omega)} \cdot N\epsilon^3 d\Prob(\omega) \\
							      &= \epsilon \cdot \E\left[\abs{X_1}\mathbbm{1}\{\abs{X_1} \leq N\epsilon^3\} \right] \\ 
							      &\leq \epsilon \cdot \E\abs{X_1}
 \end{align*}
 Note that for the penultimate inequality, we broke $X_1^2$ into two terms and bounded one of the terms by the maximum value the function can assume on the domain of integration. This achieves a tighter bound 
 then if we had bounded both terms. 
 
 To finish the proof we show that 
 \[\lim \sup_{N \to \infty}  \Prob\left(\abs{S_N - \E S_N^\prime} > N\epsilon\right) = 0\]
 Considering the $\lim \sup$ allows us to not to worry about whether the limit actually exists, and is sufficient to conclude $\frac{S_N}{N} - \frac{\E S_N^\prime}{N} \overset{p}{\to} 0$. To this end, we have
 \begin{align*}
 \lim \sup_{N \to \infty}  \Prob\left(\abs{S_N - \E S_N^\prime} > N\epsilon\right) &= \lim \sup_{N \to \infty} \left[\frac{\Var(S_N^\prime)}{N^2 \epsilon^2}  + N \cdot \Prob(\abs{X_1} > N\epsilon^3) \right] \\
 														       &\leq \epsilon \cdot \E\abs{X_1} + \lim \sup_{N \to \infty} N \cdot \Prob(\abs{X_1} > N\epsilon^3) \\
														       &= \epsilon \cdot \E\abs{X_1}  && \text{See below}
 \end{align*}
 Since $\epsilon > 0$ is arbitrary, then this inequality indeed proves that the limit is $0$. To achieve this, we claimed above that 
 \[\lim \sup_{N \to \infty} N \cdot \Prob(\abs{X_1} > N\epsilon^3) = 0\]
 which essentially is a claim that $\Prob(\abs{X_1} > N\epsilon^3)$ tends to $0$ faster than $\frac{1}{N}$. To show this, consider
 \[N \cdot \Prob(\abs{X_1} > N\epsilon^3) = N \cdot \E\left[N \cdot \mathbbm{1}\{\abs{X_1} > N\epsilon^3\} \right] \leq \E\left[\frac{\abs{X_1}}{\epsilon^3} \mathbbm{1}\{\abs{X_1} > N\epsilon^3\} \right]\]
 The bound follows from the fact that whenever the indicator function is non-zero it follows that $N < \frac{\abs{X_1}}{\epsilon^3}$. We are now in a position to apply the dominated convergence theorem. Indeed, 
 we have 
 \[\frac{\abs{X_1}}{\epsilon^3} \mathbbm{1}\{\abs{X_1} > N\epsilon^3\} \leq \frac{\abs{X_1}}{\epsilon^3}\]
 and
 \[\frac{\abs{X_1}}{\epsilon^3} \mathbbm{1}\{\abs{X_1} > N\epsilon^3\} \overset{a.s.}{\to} 0\]
 Therefore, 
 \[\E\left[\frac{\abs{X_1}}{\epsilon^3} \mathbbm{1}\{\abs{X_1} > N\epsilon^3\} \right] \to \E[0] = 0\]
Note that in each of the dominated convergence theorem applications in this proof, we used the dominating function $\abs{X_1}$ which requires the assumption that $\abs{X_1}$ is integrable. Hence, we are using the finite 
mean assumption here. 
 This verifies that $\frac{S_N}{N} - \frac{\E S_N^\prime}{N} \overset{p}{\to} 0$ and thus completes the proof. 
\end{proof}
Notice that in this proof we were able to replace the random variable $S_N^\prime$ with the deterministic value $\E S_N^\prime$, a common strategy in these types of proofs.  

\subsection{What happens when the mean is infinite?}
In the previous section we showed that even without the finite variance assumption, the tails of the $X_k$ decayed sufficiently fast to still conclude the WLLN. The natural next question is whether the finite mean assumption 
is necessary for this to hold, or if this assumption is also unnecessary? It turns out that the finite mean assumption is indeed necessary to prove the WLLN in general, but that there are examples where the WLLN holds despite 
the mean being infinite. To better understand the implications of infinite mean, we explore to examples: 1.) an example where the mean is infinite and the WLLN is not even close to holding, and 2.) an example where the mean is 
infinite but the WLLN still holds. 

\subsubsection{Example: WLLN Fails with Infinite Mean}
Unsurprisingly, for this example we consider the classic pathological fat-tailed distribution, the Cauchy distribution, which has infinite mean and variance.
 In particular, suppose $X_k \overset{iid}{\sim} \text{Cauchy}(0, 1)$ so that the $X_k$ have characteristic functions
\[\varphi_{X_k}(t) = e^{-\abs{t}}\]
As usual, let $S_N := \sum_{k = 1}^{N} X_k$ denote the partial sums. We will utilize the characteristic function $\phi_{\frac{S_N}{N}}$ to analyze the convergence behavior of the sequence of empirical means $\frac{S_N}{N}$. We can 
find the characteristic function of $\phi_{\frac{S_N}{N}}$ by exploiting the fact that the $X_k$ are iid: 
\begin{align*}
\phi_{\frac{S_N}{N}} &= \E e^{it \frac{S_N}{N}} \\
			       &= \E \exp\left\{i\frac{t}{N} \sum_{k = 1}^{N} X_k \right\} \\
			       &= \E \prod_{k = 1}^{N} \exp\left\{i \frac{t}{N} X_k \right\} \\
			       &= \prod_{k = 1}^{N} \E \exp\left\{i \frac{t}{N} X_k \right\} && \text{Independence} \\
			       &=  \prod_{k = 1}^{N} \varphi_{X_k}(t/N) \\
			       &= \prod_{k = 1}^{N} e^{-\abs{\frac{t}{N}}} \\
			       &= \left[ e^{-\abs{\frac{t}{N}}} \right]^N && \text{Identically distributed} \\
			       &= e^{-\abs{t}}
\end{align*} 
So the characteristic function of the empirical mean is the same as that of the individual $X_k$! Intuitively, the Cauchy distribution is so ``spread out'' that taking empirical means doesn't reduct the randomness at all. Not only does 
$\frac{S_N}{N}$ not converge to $\E X_1$, but it has the same exact distribution for any $N$.  

\subsubsection{Example: WLLN Holds with Infinite Mean}
The Cauchy example doesn't tell the whole story; here we consider a difference distribution with infinite mean where the WLLN still holds. Let $X_k \overset{iid}{\sim} f$ where $f$ is a density defined by 
\[f(x) = \frac{C}{x^2 \log \abs{x}} \mathbbm{1}\{\abs{x} > e\}\]
where $C$ is just a normalizing constant. Note that it can be shown that the distributions with the fattest possible tails subject to finite variance on the real line have densities that look like $\frac{1}{x^3}$. We see that 
the above density replaces one of the $x$ terms with $\log \abs{x}$, resulting in a slower rate of convergence, and therefore immediately indicating that the at least the second moment should be infinite, and maybe also the 
first. However, we're getting ahead of ourselves here. First let's verify that this is indeed a valid density. This is easily verified by 
\[\int_{e}^{\infty} \frac{1}{x^2 \log x} < \int_{e}^{\infty} \frac{1}{x^2} dx < \infty\]
and then applying symmetry for the negative half. However, it can be verified that 
\[\E\abs{X} = 2C \int_{e}^{\infty} \frac{1}{x\log x} dx = \infty\]
so the mean is indeed infinite. It remains to show that the WLLN still holds. 

We will use a truncated Chebyshev approach very similar to that used to prove the WLLN with finite mean. As in that proof, define the truncated random variables $Y_{N,k} := X_k \mathbbm{1}\{\abs{X_k} \leq N\}$ and 
$S_N^\prime := \sum_{k = 1}^{N} Y_{N, k}$. The WLLN with finite mean proof used this strategy, but relied on the fact that $\E\abs{X_1} < \infty$. Therefore, we will need to leverage the additional information at our disposal 
in this specific example in order to draw the same conclusion. We have quite a bit of information in the form of the density function of the $X_k$. Indeed, we see that the $X_k$ are symmetric and hence the $Y_{N, k}$ are 
also symmetric, so that $\E[Y_{N, k}]$ = 0. Now, recall that the triangle inequality argument in the WLLN with finite mean proof required verifying two facts: 
 \begin{enumerate}
 \item $\frac{\E S_N^\prime}{N} \to \mu$ (typical convergence for deterministic sequences)
 \item $\frac{S_N}{N} - \frac{\E S_N^\prime}{N} \overset{p}{\to} 0$
 \end{enumerate}
 The symmetry in this example makes the first item trivial! Indeed, $\frac{\E S_N^\prime}{N} = \frac{N \mu}{N} = \mu$. To establish the second, we once again apply the truncated Chebyshev inequality. 
 \begin{align*}
 \Prob(\abs{S_N - \E S_N^\prime} > N\epsilon) &= \Prob(\abs{S_N} > N\epsilon) && \text{Symmetry} \\
 								      &\leq \frac{\Var(S_N^\prime)}{N^2 \epsilon^2} + N \cdot \Prob(\abs{X_1} > N)
 \end{align*}
 To verify the second line, review the analogous steps in the proof of the WLLN with finite mean; the conclusion (which is the truncated Chebyshev inequality) did not rely on the finite mean assumption. I claim that the limit 
 of both terms is $0$, in which case the claim is established. 


We can interpret this example as an ``edge case'', in which the tails do not converge quite fast enough to yield a finite mean, yet still fast enough such that taking empirical means reduces the randomness in the distribution. 

\subsection{More General Weak Laws}
We have so far looked at sufficient conditions that yield the weak law
\[\frac{S_N}{N} \overset{p}{\to} \mu\]
It is natural to take a step back and wonder if this is just a specific case of a more general phenomenon. Some questions we might ask are:
\begin{enumerate}
\item Are there situations in which a scaling other than $\frac{1}{N}$ yields the proper scaling for a weak law? Can we consider other polynomial rates $\frac{1}{N^r}$? Or even more generally, scalings of the 
form $\frac{1}{b_N}$, where $\{b_N\}$ is an arbitrary increasing sequence? 
\item Can we find sufficient \textit{and} necessary conditions for the weak law to hold? In essence, can we find precisely the correct conditions that correspond to a weak law existing? 
\end{enumerate}
Both of these questions are considered below. 

\subsubsection{Different Scaling Rates: The Marcinkiewiz-Zygmund Weak Law}
We first consider the question of viewing the scaling $\frac{S_N}{N}$ as a special case of a more general polynomial scaling $\frac{S_N}{N^r}$. This generalization is provided by the 
Marcinkiewiz-Zygmund weak Law. I will present this law in two parts, although the theorem can be summarized in a single statement. The first part establishes a weak law in a setting where 
one did not exist before. In particular, if the mean does not exist, but a weaker moment exists than we can still conclude a weak law, but will now need to scale the sum by a larger term to account 
for the extra randomness in the tails. 
\begin{thm} 
Suppose $\{X_k\}$ is an iid sequence with $\E\abs{X_1}^r < \infty$ for some $r \in [0, 1)$. Then 
\[\frac{S_N}{N^{1/r}} \overset{p}{\to} \mu\]
\end{thm}

The second part pertains to a setting in which the mean is finite (and hence the WLLN already holds), but states that stronger assumptions can allow us to conclude stronger results. 
In particular, if we make stronger assumptions 
on the decay of the tails (i.e. higher moments exist) then this translates into a smaller scaling (i.e. the denominator required to properly collapse the sum is now smaller). 
\begin{thm} 
Suppose $\{X_k\}$ is an iid sequence with $\E\abs{X_1}^r < \infty$ for some $r \in [1, 2)$. Moreover, assume $\E X_1 = 0$. Then 
\[\frac{S_N}{N^{1/r}} \overset{p}{\to} \mu\]
\end{thm}
Notice that the $r = 1$ case is simply the standard WLLN. In this second case we require the extra assumption $\E X_1 = 0$ for the proof to work, but it is not limiting in practice as we can 
always de-mean random variables. This assumption would not even be meaningful in the first part of the theorem, as the first part pertains to the setting where the mean does not exist. 

\subsubsection{A General Weak Law with Necessary and Sufficient Conditions}
We now consider a very general weak law that considers arbitrary increasing normalizing sequences $\{b_N\}$, and establishes both necessary and sufficient conditions. This theorem is a bit 
more complicated to state, but this is not surprising as it zeros in on the very particular conditions that are required for a weak law to hold in a very general setting. 
\begin{thm}
Suppose $\{X_k\}$ is an independent (not necessarily iid) sequence of random variables and $\{b_N\}$ is an increasing, non-random sequence of positive reals. Define the cutoff random variables 
\begin{align*}
&Y_{N, k} = X_k \mathbbm{1}\{\abs{X_k} \leq b_N\}, &&S_N^\prime = \sum_{k = 1}^{N} Y_{N, k}
\end{align*}
\begin{enumerate}
\item If
\[\lim_{N \to \infty} \sum_{k = 1}^{N} \Prob(\abs{X_k} > b_n) = 0\]
and 
\[\lim_{N \to \infty} \frac{1}{b_N^2} \sum_{k = 1}^{N} \Var(Y_{N, k}) = 0\]
then
\[\frac{S_N - \E S_N^\prime}{b_N} \overset{p}{\to} 0\]

\item Conversely, if 
\[\frac{S_N - \E S_N^\prime}{b_N} \overset{p}{\to} 0\]
and 
\[\frac{S_N^\prime}{b_N} \to 0\]
then 
\[\lim_{N \to \infty} \sum_{k = 1}^{N} \Prob(\abs{X_k} > b_n) = 0\]
and 
\[\lim_{N \to \infty} \frac{1}{b_N^2} \sum_{k = 1}^{N} \Var(Y_{N, k}) = 0\]
hold. 
\end{enumerate}
\end{thm}

\subsection{A Weak Law for Partial Maxima}
All of the weak laws detailed above conclude that the partial sums $S_N$, when properly normalized by some increasing sequence $\{b_N\}$, converge in probability to the mean $\mu$. Can we establish 
similar results for statistics other than sums? Indeed we can--below we  present a weak law for partial maxima, where the normalized partial maxima converge in probability to $0$. We begin by stating the result
for the more-familiar normalizing sequence of polynomials $\{N^{1/r}\}$ before generalizing to arbitrary increasing sequences $\{b_N\}$. 

\begin{thm}
Suppose $\{X_k\}$ is an iid sequence and define $Y_N := \max_{1 \leq k \leq N} \abs{X_k}$. Then, for $r > 0$, 
\[\frac{Y_N}{N^{1/r}} \overset{p}{\to} 0 \iff N \cdot \Prob(\abs{X} > N^{1/r}) \to 0\]
\end{thm}
Therefore, a weak law for partial maxima exists precisely when the tail  probabilities $\Prob(\abs{X} > N^{1/r})$ decay faster than $\frac{1}{N}$. A larger $r$ indicates lighter tails, in which case the 
normalization need not be as strong. I also emphasize that this tail decay condition is necessary and sufficient. 

Generalizing to arbitrary increasing sequences $\{b_N\}$ is relatively straightforward, but requires a ``holds for all $\epsilon > 0$ condition'', as stated below. 
\begin{thm}
Suppose $\{X_k\}$ is an iid sequence of random variables and $\{b_N\}$ a sequence of non-decreasing positive reals. Define $Y_N := \max_{1 \leq k \leq N} \abs{X_k}$. Then, for $r > 0$, 
\[\frac{Y_N}{b_N} \overset{p}{\to} 0 \iff N \cdot \Prob(\abs{X} > b_N \epsilon) \to 0 \text{ for all } \epsilon > 0\]
\end{thm}

\end{document}









