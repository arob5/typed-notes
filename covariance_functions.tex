\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\zeroVec}{\mathbf{0}}

\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

% Covariance function/kernel. 
\newcommand{\Ker}{K}
\newcommand{\covFun}{C}
\newcommand{\locSpace}{\mathcal{X}}
\newcommand{\loc}{\mathbf{x}}
\newcommand{\locTwo}{\loc^\prime}
\newcommand{\locDum}{\mathbf{u}}
\newcommand{\locLag}{\mathbf{h}}
\newcommand{\logLagDum}{\mathbf{s}}
\newcommand{\inputDim}{D}
\newcommand{\rf}{Y}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Covariance Functions: Stationary and Non-Stationary}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% Introduction to Covariance Functions
\section{Introduction to Covariance Functions}

\subsection{General Definition}

\subsection{Stationary Processes}
Isotropy and anisotropy; Mahalanobis distance to achieve the latter. 

\subsubsection{Positive Definite Functions}
Given that the covariance function for a stationary process can be written as a function only of the 
displacement vector $\locLag := \loc - \locTwo$, then it is common to overload the notation for the covariance function as 
\begin{align}
\covFun(\locLag) := \covFun(\loc - \locTwo) := \covFun(\loc, \locTwo)
\end{align}
Thus, when specifically focusing on stationary processes we will often consider the function $\covFun: \locSpace \to \R_+$ as defined above, 
in addition to the standard two-argument covariance function $\covFun(\cdot, \cdot)$. Notice that the PSD requirement on the latter induces 
an analogous requirement on the former; namely, for all $n \in \mathbb{N}$, $a_1, \dots, a_n \in \R$, and $\loc_1, \dots, \loc_n \in \locSpace$, 
\begin{align}
\sum_{i = 1}^{n} \sum_{j = 1}^{n} a_i a_j \covFun(\loc_i - \loc_j) \geq 0
\end{align} 
Choosing $\loc_1 = \locLag$ and $\loc_2 = 0$ in the above definition, using the fact $\covFun(-\locLag) = \covFun(\locLag)$, and rearranging terms yields the inequality
\begin{align*}
\covFun(\zeroVec) \geq \frac{-2 a_1 a_2 \covFun(\locLag)}{a_1^2 + a_2^2}.
\end{align*}
Choosing $a_1 = 1$ and $a_2 = -1$, we then have 
\begin{align}
\covFun(\zeroVec) \geq \covFun(\locLag),
\end{align}
capturing the intuitive notion that dependence between two locations is greatest when the lag is $\zeroVec$; put differently, covariance between two locations cannot exceed the 
(constant) process variance $\covFun(\zeroVec) = \Cov[\rf(\loc)]$. \textbf{TODO:} is it actually true that $\covFun(-\locLag) = \covFun(\locLag)$?; need to prove this. 



% Common Covariance Functions
\section{Common Covariance Functions}

\subsection{Exponential and Squared Exponential}

\subsection{Matern}


% Spectral Representation 
\section{Spectral Representation}

% Process Convolutions
\section{Process Convolutions}
The idea of \textit{process convolutions} is a popular method in spatial statistics used to constructively generates a covariance function. The main idea is that a random field $\rf(\cdot)$ over an input space 
$\locSpace$ can be defined via a convolution 
\begin{align}
\rf(\loc) &= \int_{\locSpace} \Ker(\loc - \locDum) dW(\locDum) =  \int_{\locSpace} \Ker_{\loc}(\locDum) dW(\locDum), \label{process_convolution}
\end{align}
where $\Ker(\cdot)$ is a (stationary) kernel and $W(\cdot)$ a stochastic process over $\locSpace$ \cite{Risser}. So the idea is to start with a ``simple'' stochastic process $W(\cdot)$ (often a white-noise process) and produce a 
more complicated process $\rf(\cdot)$ via convolution with a kernel $\Ker(\cdot)$. The notation $\Ker_{\loc}(\locDum) = \Ker(\loc - \locDum)$ emphasizes the fact that the properties of the random field $\rf(\loc)$ at location 
$\loc$ are determined by a kernel centered at $\loc$. In essence, the field is constructed by placing a kernel $\Ker_{\loc}$ at every location. Notice that for a fixed location $\loc$, $\Ker_{\loc}(\locDum)$ determines to what degree
the latent process $W(\locDum)$ at location $\locDum$ influences $\rf(\loc)$ at $\loc$. 

As we will see, the kernel convolution approach can be used to construct both stationary and nonstationary covariance functions. In general, the two adjustable parameters to produce different 
covariance structures are the latent process $W(\locDum)$ and \textit{smoothing kernel} $\Ker(\cdot)$. Going forward, we will fix the former to be a GP. 

When $W(\cdot)$ is a Brownian motion or other form of GP, then the resulting stochastic process $\rf(\cdot)$ in \ref{process_convolution} is a 
GP. We can obtain a simpler equivalent representation of \ref{process_convolution} in the Brownian motion case via
\begin{align}
\rf(\loc) &= \int_{\locSpace} \Ker(\loc - \locDum) V(\locDum) d\locDum, \label{process_convolution2}
\end{align}
where $V(\cdot)$ is a Gaussian white-noise process. The covariance function of the process $\rf(\cdot)$ implied by \ref{process_convolution2} is 
\begin{align}
\covFun(\loc, \locTwo) &= \int_{\locSpace} \Ker(\loc - \locDum)\Ker(\locTwo - \locDum) d\locDum =  \int_{\locSpace} \Ker_{\loc}(\locDum)\Ker_{\locTwo}(\locDum) d\locDum. \label{process_convolution_kernel}
\end{align}
Thus, the covariance $\covFun(\loc, \locTwo) = \Cov[\rf(\loc), \rf(\locTwo)]$ is given by the inner product between the local kernels $\Ker_{\loc}$ and $\Ker_{\locTwo}$. 
This makes a lot of sense; the dependence between $\rf(\loc)$ and $\rf(\locTwo)$ is determined by considering the cumulative effect of the influence of location $\loc$ on location $\locTwo$ (and vice versa) 
indirectly through each intermediate location $\locDum$. For example, $\Ker_{\loc}(\locDum)\Ker_{\locTwo}(\locDum)$ will be large if each of the terms in the product is large, meaning that the locations 
$(\loc, \locDum)$ are highly dependent, as are the locations $(\locTwo, \locDum)$. Thus, by a sort of transitivity this will induce dependence between $(\loc, \locTwo)$. Conversely, if 
$\Ker_{\locTwo}(\locDum) \approx 0$, then this will break the link between $\loc$ and $\locTwo$ via the intermediate location $\locDum$, and thus this ``route'' between the two locations will not contribute 
any dependence. The covariance $\covFun(\loc, \locTwo)$ depends on the cumulative effect of such links over all intermediate locations $\locDum$. 

Instead of working through all of the technical details regarding the stochastic integration \ref{process_convolution}, we instead opt for the simpler approach of taking \ref{process_convolution_kernel}
as our starting point. The first order of business is to ensure that the integral \ref{process_convolution_kernel} exists so that $\covFun(\cdot, \cdot)$ is well-defined. 


Therefore, we need only verify that $\covFun(\cdot, \cdot)$ is PSD in order to establish that it is a valid covariance function \cite{Paciorek}. 

\begin{prop} 
Let $\Ker: \locSpace \to \R_+$ be a stationary kernel. Then the function $\covFun: \locSpace \times \locSpace \to \R_+$ defined in \ref{process_convolution_kernel} is symmetric PSD. 
\end{prop}

\begin{proof} 
The symmetry $\covFun(\loc, \locTwo) = \covFun(\locTwo, \loc)$ is clearly observed in formula \ref{process_convolution_kernel}. To show positive semi-definiteness, consider arbitrary 
$n \in \mathbb{N}$, $a_1, \dots, a_n \in \R$, and $\loc_1, \dots, \loc_n \in \locSpace$. Then, 
\begin{align*}
\sum_{i = 1}^{n} \sum_{j = 1}^{n} a_i a_j \covFun(\loc_i, \loc_j) 
&= \sum_{i = 1}^{n} \sum_{j = 1}^{n} a_i a_j \int_{\locSpace} \Ker_{\loc_i}(\locDum) \Ker_{\loc_j}(\locDum) d\locDum \\
&= \int_{\locSpace} \left(\sum_{i = 1}^{n} \sum_{j = 1}^{n} a_i a_j  \Ker_{\loc_i}(\locDum) \Ker_{\loc_j}(\locDum)\right) d\locDum \\
&=  \int_{\locSpace} \left(\sum_{i = 1}^{n} a_i \Ker_{\loc_i}(\locDum)\right)  \left(\sum_{j = 1}^{n} a_j \Ker_{\loc_j}(\locDum)\right) d\locDum \\
&=  \int_{\locSpace} \left(\sum_{i = 1}^{n} a_i \Ker_{\loc_i}(\locDum)\right)^2 d\locDum \\
&\geq 0
\end{align*}
which establishes that $\covFun(\cdot, \cdot)$ is PSD. 
\end{proof} 

It is not immediately obvious whether the covariance function \ref{process_convolution_kernel} is stationary. The following result gives a condition under which this is always the case. 

\begin{prop} 
Suppose the smoothing kernel satisfies $\Ker(\locLag) = \Ker(-\locLag)$ for all $\locLag \in \locSpace$. Then the covariance function given in \ref{process_convolution_kernel} is stationary. 
\end{prop}

\begin{proof} 
This follows directly from applying the change of variables $\logLagDum := $ in the 
\end{proof}





\subsection{Paciorek and Schervish Generalization}
See Paciorek thesis, as well as \cite{Paciorek}. 

\subsection{Non-Euclidean Domains}
See Sam Baugh thesis


% Bibliography
\begin{thebibliography}{20}
\bibitem{Risser} Risser, Mark. (2016). Review: Nonstationary Spatial Modeling, with Emphasis on Process Convolution and Covariate-Driven Approaches. 
\bibitem{Guinness} Joseph Guinness, Montserrat Fuentes (2016). Isotropic covariance functions on spheres: Some properties and modeling considerations, Journal of Multivariate Analysis, Volume 143
Pages 143-152, ISSN 0047-259X.
\bibitem{Porcu} Porcu, Emilio \& Bevilacqua, Moreno \& Schaback, Robert \& Oates, Chris. (2023). The Mat\'ern Model: A Journey through Statistics, Numerical Analysis and Machine Learning. 10.48550/arXiv.2303.02759. 
\bibitem{Higdon} Higdon, David M.. “Space and Space-Time Modeling using Process Convolutions.” (2002).
\bibitem{Higdon2} Higdon, David M. et al. “Non-Stationary Spatial Modeling.” (2022).
\bibitem{Paciorek} Paciorek CJ, Schervish MJ. Spatial Modelling Using a New Class of Nonstationary Covariance Functions. Environmetrics. 2006;17(5):483-506. doi: 10.1002/env.785. PMID: 18163157; PMCID: PMC2157553.
\bibitem{Higdon} Higdon, D. A process-convolution approach to modelling temperatures in the North Atlantic Ocean. Environmental and Ecological Statistics 5, 173–190 (1998). https://doi.org/10.1023/A:1009666805688
\bibitem{Genton} Marc G. Genton. 2002. Classes of kernels for machine learning: a statistics perspective. J. Mach. Learn. Res. 2 (3/1/2002), 299–312.
\bibitem{} 
\end{thebibliography}

\end{document}


