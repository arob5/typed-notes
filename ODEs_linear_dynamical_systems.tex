\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\be}{\mathbf{e}}

\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}


\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Linear Dynamical Systems and Ordinary Differential Equations}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% An Introduction to ODEs
\section{An Introduction to ODEs}


% First-Order ODEs
\section{First-Order ODEs}

\subsection{Solving Scalar ODEs}

\subsubsection{Guessing the solution}

\subsubsection{Integration}
See e.g. Sarkka SDEs book, first chapter. Motivates the matrix exponential solution for vector systems. 

\subsection{Vector ODEs}

\subsubsection{Introduction and Interpretation}
See Stanford linear dynamical systems lectures. 

\subsubsection{The Matrix Exponential}
Briefly discuss different ways to compute it; i.e. Fourier/Laplace/Taylor series

\subsubsection{Integrating Factors}


% Higher-Order Linear ODEs
\section{Higher-Order Linear ODEs}
Linear ODEs are generally of the form 
\begin{align}
a_0(t)x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dots + a_n(t) x^{(n)} = b(t) \label{linear_ODE}
\end{align}
This equation is said to be of order $n$ due to the fact that $n$ derivatives of the state vector $x(t) \in \R^P$ appear in the equation. 
The equation is linear in the state vector $x$ and its $n$ derivatives, $\{x, \dot{x}, \ddot{x}, \dots, x^{(n)}\}$. The coefficients 
$a_1(t), \dots, a_n(t)$ may be non-linear functions of $t$. In the case when the coefficients are simply constants 
\begin{align*}
a_0 x + a_1 \dot{x} + a_2 \ddot{x} + \dots + a_n x^{(n)} = b(t)
\end{align*}
the equation is known as an $n^{\text{th}}$ order linear ODE with constant coefficients. The emphasis here is not on finding analytical solutions 
to such systems, but rather understanding the underlying structure. In particular, I will try to emphasize the application of linear algebra in studying 
such systems far more than I emphasize arguments based on calculus. In this spirit, I start by exploring the general structure of the solutions to 
$n^{\text{th}}$ order linear ODEs by taking a linear operator viewpoint. 

\subsection{Reduction to a First-Order System}
I now establish another connection between linear ODEs \ref{linear_ODE} and linear systems. In particular, I show that \ref{linear_ODE} can be written as a first order linear 
equation 
\[\dot{\bx} = A \bx + L \bb \]
by extending the state-space $x$ to a vector of states $\bx$. In the homogenous case this reduces to 
\[\dot{\bx} = A \bx\]
which is the basic vector-valued first-order linear ODE studied in the previous section. For clarity of notation, we consider a third-order system,
\[a_0(t) x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dddot{x} = b(t)\]
but the same exact reasoning applies to equations of general order. To keep the algebra clean we have also assumed $a_3(t) = 1$, which can be achieved by dividing both 
sides of the equation by the coefficient on the highest-order term. I define the extended state vector 
\begin{align*}
\bx :=  \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} := \begin{pmatrix} x \\ \dot{x} \\ \ddot{x} \end{pmatrix}
\end{align*}
The dimension of $\bx$ in general matches the order of the ODE, as $\bx$ includes the original state $x$ plus all of its derivatives except for the last one $x^{(n)}$. 
The time derivative of $\bx$ is then given by 
\begin{align*}
\dot{\bx} =  \begin{pmatrix} \dot{x}_1 \\ \dot{x}_2 \\ \dot{x}_3 \end{pmatrix} = \begin{pmatrix} \dot{x} \\ \ddot{x} \\ \dddot{x} \end{pmatrix} &=  
\begin{pmatrix} x_2 \\ x_3 \\ b(t) -  a_0(t) x_1 - a_1(t) x_2 - a_2(t) x_3 \end{pmatrix} \\
&= \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ -a_0(t) & -a_1(t) & -a_2(t) \end{pmatrix}  \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} + \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} b(t) \\
&=: A\bx + L b
\end{align*}
The resulting forcing term $L b$ is quite simple, since $L$ is simply a vector and $b = b(t)$ a scalar. This reduction of higher order linear ODEs to a first order matrix system allows 
us to apply theory and algorithms concerning the latter to equations like \ref{linear_ODE}.

\subsection{Linear Operator Viewpoint and the Solution Space}
We can view the left side of \ref{linear_ODE} as a linear operator that maps between function spaces. In particular, let $L: C^n(a, b) \to C(a, b)$
\footnote{$C^n(a, b)$ denotes the space of functions on $(a, b)$ with n continuous derivatives. $C(a, b)$ is the space of continuous functions on $(a, b)$} be the operator 
that maps $x(t)$ to $a_0(t)x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dots + a_n(t) x^{(n)}$. The equation \ref{linear_ODE} can now be written 
\begin{align}
Lx &= b \label{linear_ODE_operator_form}
\end{align}
where I note that $x = x(t)$ and $b = b(t)$ are both functions of $t$, though this is suppressed in the notation to emphasize the fact that we are viewing $x$ and 
$b$ abstractly as vectors. The operator $L$ is often called a \textit{differential operator} and is often written 
\begin{align*}
L &= a_0(t) + a_1(t) \frac{d}{dt} + a_2(t) \frac{d^2}{dt^2} + \dots + a_n(t) \frac{d^n}{dt^n}
\end{align*}
Note that $L$ is indeed a linear operator (i.e. it satisfies $L(\alpha x_1 + \beta x_2) = \alpha L x_1 + \beta L x_2$) due to the fact that differentiation 
is a linear operation. By writing the linear ODE as \ref{linear_ODE_operator_form}, we see that solving linear ODEs can be viewed as solving a linear equation, though the unknown 
$x$ in the equation is infinite-dimensional. 

With this powerful viewpoint, we can immediately establish what solutions of \ref{linear_ODE} must look like. First, suppose we have already obtained a \textit{particular solution}
$x_p$ such that $L x_p = b$. My claim is that the space of solutions of \label{linear_ODE} can be written as $x_p + \text{Null}(L)$, where $\text{Null}(L)$ 
is the null space of the linear operator $L$. If $x_n \in \text{Null}(L)$ then $L x_n = 0$ (where the righthand side is the zero function) so 
\[L(x_p + x_n) = L x_p + L x_n = b + 0 = b\]
so indeed $x_p + x_n$ is a solution. To show that all solutions are of this form, suppose that $x$ is some other solution such that $Lx = b$. Then we can write $x$ as 
$x = x_p + (x - x_p)$ where $L(x - x_p) = Lx - L x_p = b - b = 0$ so $x - x_p \in \text{Null}(L)$. This establishes that all solutions are of the form $x_p + \text{Null}(L)$. 

\subsubsection{Finding the Particular Solution}
See MathTheBeautiful videos. 

\subsubsection{Characterizing the Null Space}
The big result here is that the dimension of $\text{Null}(L)$ is finite-dimensional, and in particular equal to $n$, the order of the equation. This means that the general solution 
$x_p + \text{Null}(L)$ consists of a linear subspace of dimension $n$. In practice, this means that we require $n$ initial conditions in order to solve the equation: 
$x(t_0), \dot{x}(t_0), \dots, x^{(n-1)}(t_0)$. 

I now prove that indeed $\text{dim } \text{Null}(L) = n$. The proof will require Caratheodory's existence theorem, which under mild conditions guarantees a unique solution to the initial value problem (IVP) 
\begin{align*}
\dot{\bx} = f(\bx, t), && \bx(t_0) = \bx_0
\end{align*}
where $t_0 \in (a, b)$. Note that this result applies to first-order ODEs; however, from the previous section we know that we can reduce an ODE of order $n$ to a first-order ODE 
by extending the state space. Applying that result, we can re-write 
\begin{align*}
Lx = a_0(t)x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dots + a_n(t) x^{(n)} = 0
\end{align*}
as 
\begin{align*}
\dot{\bx} &= A\bx, &&\bx = \begin{pmatrix} x & \dot{x} & \hdots & x^{(n-1)} \end{pmatrix}^\top
\end{align*}
Now consider the $n$ separate IVPs for some $t_0 \in (a, b)$:
\begin{align*}
\dot{\bx} &= A\bx, \ \bx(t_0) := \be_i, &&i = 1, \dots, n
\end{align*}
where $\be_i \in \R^n$ denotes the $i^{\text{th}}$ standard basis vector. Caratheodory's existence theorem guarantees unique solutions 
$\bx^{(1)}, \dots, \bx^{(n)}$ for each respective IVP. I claim that these $n$ functions form a basis for $\text{Null}(L)$, which will establish that $\text{Null}(L)$ has dimension $n$.
To show that they are linearly independent, simply note that $\bx^{(i)}(t_0) = \be_i$ so that the vectors $\{\bx^{(i)}(t_0)\}_{i = 1}^{n}$ are linearly independent. Thus, the functions 
 $\{\bx^{(i)}\}_{i = 1}^{n}$ must also be independent. It remains to show that these functions span $\text{Null}(L)$. To this end, suppose $\tilde{x} \in \text{Null}(L)$ so that $L\tilde{x} = 0$, 
 which can be reduced to $\dot{\tilde{\bx}} = A\tilde{\bx}$ as above. 
 
 Since the initial conditions $\{\bx^{(i)}(t_0)\}_{i = 1}^{n} = \{\be_i\}_{i = 1}^{n}$ span $\R^n$ then $\tilde{\bx}(t_0) = \sum_{i = 1}^{n} c_i \bx_i(t_0)$ for scalars $c_i$ (clearly, $c_i$ is simply 
 the $i^{\text{th}}$ entry of $\tilde{\bx}$). Now, consider the IVP:
 \begin{align}
 \dot{\bx} &= A\bx, && \bx(t_0) = \sum_{i = 1}^{n} c_i \bx^{(i)}(t_0) \label{IVP_tilde}
 \end{align}
 By construction we have that $\tilde{\bx}$ is a solution to \ref{IVP_tilde}. I claim that $\sum_{i = 1}^{n} c_i \bx^{(i)}$ is also a solution, in which case by uniqueness it must be that 
 $\tilde{\bx} = \sum_{i = 1}^{n} c_i \bx^{(i)}$ and hence $\tilde{\bx} \in \text{Span}\left(\bx^{(1)}, \dots, \bx^{(in)} \right)$. This is clearly true since $\sum_{i = 1}^{n} c_i \bx^{(i)}$ satisfies the initial 
 condition and 
 \begin{align*}
 \frac{d}{dt} \sum_{i = 1}^{n} c_i \bx^{(i)} &= \sum_{i = 1}^{n} c_i \dot{\bx}^{(i)} = \sum_{i = 1}^{n} c_i A\bx^{(i)} = A \sum_{i = 1}^{n} c_i \bx^{(i)} 
 \end{align*}
 We have shown that $\bx^{(1)}, \dots, \bx^{(n)}$ is a basis for the set of solutions to $\dot{\bx} = A\bx$. Recall that this matrix system encodes the original problem $Lx = 0$. Therefore, it 
 immediately follows that $\bx^{(1)}_1, \dots, \bx^{(n)}_1$ form a basis for $\text{Null}(L)$. The fact that these functions span the null space is immediate, and also note that 
 each vector $\bx^{(i)}$ contains the original state and its derivatives. Thus, for the vectors to be independent the states $x(t)$ must also be independent; otherwise, the derivatives 
 would follow the same dependence relation as the original states. 


% Numerical Methods for Solving General ODEs
\section{Numerical Methods for Solving General ODEs}

\subsection{Forward Euler}
Introduce in the intuitive way of simply stepping in the direction of the gradient. Also emphasize the alternative derivation based on discretizing the integral used to 
step forward in time. 


\end{document}


