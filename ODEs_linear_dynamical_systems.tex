\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\be}{\mathbf{e}}

% Differential Equations
\newcommand{\state}{x}
\newcommand{\stateb}{\mathbf{x}}


\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}


\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Linear Dynamical Systems and Ordinary Differential Equations}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% An Introduction to ODEs
\section{An Introduction to ODEs}

\subsection{General Definitions}

\subsection{Examples}

\subsection{Solving First-Order Scalar ODEs}

\subsubsection{Guessing the solution}

\subsubsection{Integration}
See e.g. Sarkka SDEs book, first chapter. Motivates the matrix exponential solution for vector systems. 



% Matrix Systems
\section{Matrix Systems}
The focus on this section is an in-depth study of the first-order matrix system of ODEs
\begin{align}
\dot{\bx} = A\bx \label{matrix_system}
\end{align}
where 
\begin{align*}
&\bx(t) = \begin{pmatrix} x_1(t) & x_2(t) & \hdots & x_n(t) \end{pmatrix}^\top \in \R^n, &&A \in \R^{n \times n}
\end{align*}
The amount of time we spend studying this system is justified by the following points. 
\begin{enumerate}
\item Higher-order linear ODEs can be reduced to a matrix system of the form \ref{matrix_system}, and thus characterizing the solution 
of \ref{matrix_system} immediately gives a characterization of the broad class of higher-order linear ODEs. 
\item Such linear systems are quite general and show up all over the place in applications. 
\item The study of non-linear systems often boils down to extending techniques from the linear case, or reducing to the 
linear case via linearization. 
\end{enumerate}

\subsection{Big Picture and Interpretation}
See Stanford linear dynamical systems lectures. Also mention view as a system of coupled dynamics. 

\subsection{Solving $\dot{\bx} = A\bx$}
\subsubsection{Eigenvalues and Eigenvectors}



\subsubsection{The Matrix Exponential}
Briefly discuss different ways to compute it; i.e. Fourier/Laplace/Taylor series

\subsection{Matrix System with Forcing}
\subsubsection{Integrating Factors}
See Saarka SDEs book


% Higher-Order Linear ODEs
\section{Higher-Order Linear ODEs}
Linear ODEs are generally of the form 
\begin{align}
a_0(t)x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dots + a_n(t) x^{(n)} = b(t) \label{linear_ODE}
\end{align}
This equation is said to be of order $n$ due to the fact that $n$ derivatives of the state vector $x(t) \in \R^P$ appear in the equation. 
The equation is linear in the state vector $x$ and its $n$ derivatives, $\{x, \dot{x}, \ddot{x}, \dots, x^{(n)}\}$. The coefficients 
$a_1(t), \dots, a_n(t)$ may be non-linear functions of $t$. In the case when the coefficients are simply constants 
\begin{align*}
a_0 x + a_1 \dot{x} + a_2 \ddot{x} + \dots + a_n x^{(n)} = b(t)
\end{align*}
the equation is known as an $n^{\text{th}}$ order linear ODE with constant coefficients. The emphasis here is not on finding analytical solutions 
to such systems, but rather understanding the underlying structure. In particular, I will try to emphasize the application of linear algebra in studying 
such systems far more than I emphasize arguments based on calculus. In this spirit, I start by exploring the general structure of the solutions to 
$n^{\text{th}}$ order linear ODEs by taking a linear operator viewpoint. 

\subsection{Reduction to a First-Order System}
I now establish another connection between linear ODEs \ref{linear_ODE} and linear systems. In particular, I show that \ref{linear_ODE} can be written as a first order linear 
equation 
\[\dot{\bx} = A \bx + L \bb \]
by extending the state-space $x$ to a vector of states $\bx$. In the homogenous case this reduces to 
\[\dot{\bx} = A \bx\]
which is the basic vector-valued first-order linear ODE studied in the previous section. For clarity of notation, we consider a third-order system,
\[a_0(t) x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dddot{x} = b(t)\]
but the same exact reasoning applies to equations of general order. To keep the algebra clean we have also assumed $a_3(t) = 1$, which can be achieved by dividing both 
sides of the equation by the coefficient on the highest-order term. I define the extended state vector 
\begin{align*}
\bx :=  \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} := \begin{pmatrix} x \\ \dot{x} \\ \ddot{x} \end{pmatrix}
\end{align*}
The dimension of $\bx$ in general matches the order of the ODE, as $\bx$ includes the original state $x$ plus all of its derivatives except for the last one $x^{(n)}$. 
The time derivative of $\bx$ is then given by 
\begin{align*}
\dot{\bx} =  \begin{pmatrix} \dot{x}_1 \\ \dot{x}_2 \\ \dot{x}_3 \end{pmatrix} = \begin{pmatrix} \dot{x} \\ \ddot{x} \\ \dddot{x} \end{pmatrix} &=  
\begin{pmatrix} x_2 \\ x_3 \\ b(t) -  a_0(t) x_1 - a_1(t) x_2 - a_2(t) x_3 \end{pmatrix} \\
&= \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ -a_0(t) & -a_1(t) & -a_2(t) \end{pmatrix}  \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} + \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} b(t) \\
&=: A\bx + L b
\end{align*}
The resulting forcing term $L b$ is quite simple, since $L$ is simply a vector and $b = b(t)$ a scalar. This reduction of higher order linear ODEs to a first order matrix system allows 
us to apply theory and algorithms concerning the latter to equations like \ref{linear_ODE}.

\subsection{Linear Operator Viewpoint and the Solution Space}
We can view the left side of \ref{linear_ODE} as a linear operator that maps between function spaces. In particular, let $L: C^n(a, b) \to C(a, b)$
\footnote{$C^n(a, b)$ denotes the space of functions on $(a, b)$ with n continuous derivatives. $C(a, b)$ is the space of continuous functions on $(a, b)$} be the operator 
that maps $x(t)$ to $a_0(t)x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dots + a_n(t) x^{(n)}$. The equation \ref{linear_ODE} can now be written 
\begin{align}
Lx &= b \label{linear_ODE_operator_form}
\end{align}
where I note that $x = x(t)$ and $b = b(t)$ are both functions of $t$, though this is suppressed in the notation to emphasize the fact that we are viewing $x$ and 
$b$ abstractly as vectors. The operator $L$ is often called a \textit{differential operator} and is often written 
\begin{align*}
L &= a_0(t) + a_1(t) \frac{d}{dt} + a_2(t) \frac{d^2}{dt^2} + \dots + a_n(t) \frac{d^n}{dt^n}
\end{align*}
Note that $L$ is indeed a linear operator (i.e. it satisfies $L(\alpha x_1 + \beta x_2) = \alpha L x_1 + \beta L x_2$) due to the fact that differentiation 
is a linear operation. By writing the linear ODE as \ref{linear_ODE_operator_form}, we see that solving linear ODEs can be viewed as solving a linear equation, though the unknown 
$x$ in the equation is infinite-dimensional. 

With this powerful viewpoint, we can immediately establish what solutions of \ref{linear_ODE} must look like. First, suppose we have already obtained a \textit{particular solution}
$x_p$ such that $L x_p = b$. My claim is that the space of solutions of \label{linear_ODE} can be written as $x_p + \text{Null}(L)$, where $\text{Null}(L)$ 
is the null space of the linear operator $L$. If $x_n \in \text{Null}(L)$ then $L x_n = 0$ (where the righthand side is the zero function) so 
\[L(x_p + x_n) = L x_p + L x_n = b + 0 = b\]
so indeed $x_p + x_n$ is a solution. To show that all solutions are of this form, suppose that $x$ is some other solution such that $Lx = b$. Then we can write $x$ as 
$x = x_p + (x - x_p)$ where $L(x - x_p) = Lx - L x_p = b - b = 0$ so $x - x_p \in \text{Null}(L)$. This establishes that all solutions are of the form $x_p + \text{Null}(L)$. 

\subsubsection{Finding the Particular Solution}
I will not spend much time on particular solution methods, as the emphasis here is on big picture ideas. However, it is helpful to reason through one simple example. 
Consider,  
\[\ddot{x} - 6\dot{x} + 8x = 3e^{-5t}\]
an example I took from \href{https://www.youtube.com/watch?v=3eFiwF8pUR4&list=PLlXfTHzgMRUK56vbQgzCVM9vxjKxc8DCr&index=5}{MathTheBeautiful's} excellent 
YouTube channel. We are seeking a function $x(t)$ such that taking a linear combination of itself and its derivatives yields $3e^{-5t}$. Since the derivative of the exponential 
is proportional to itself, then a good guess here would be $x(t) \propto e^{-5t}$. Let's plug this in and see what we get. 
\begin{align*}
L e^{-5t} &= 25e^{-5t} + 30 e^{-5t} + 8 e^{-5t} = 63 e^{-5t}
\end{align*}
So we have overshot the mark by a factor of $21$. We can easily correct for this by dividing through by $21$, yielding the particular solution 
\[x_p(t) = \frac{1}{21} e^{-5t}\]
I should note that this example was engineered to be very simple to reason through. The constant coefficients and fact that the forcing function is an exponential make this 
equation very easy to solve. Progressing to more complicated examples depends more sophisticated solution techniques (in the case that analytical solutions even exist). 
Working through these particular solution methods are not the emphasis of these notes. 

\subsubsection{Characterizing the Null Space}
The big result here is that the dimension of $\text{Null}(L)$ is finite-dimensional, and in particular equal to $n$, the order of the equation. This means that the general solution 
$x_p + \text{Null}(L)$ consists of a linear subspace of dimension $n$. In practice, this means that we require $n$ initial conditions in order to solve the equation: 
$x(t_0), \dot{x}(t_0), \dots, x^{(n-1)}(t_0)$. It is also worth emphasizing that the null space of $L$ has nothing to do with $b(t)$, the driving term in the inhomogenous equation. 
The particular solution $x_p$ of course depends on $b(t)$, but once a particular solution is found then finding the remaining solutions has nothing to do with $b(t)$. 

I now prove that indeed $\text{dim } \text{Null}(L) = n$. The proof will require Caratheodory's existence theorem, which under mild conditions guarantees a unique solution to the initial value problem (IVP) 
\begin{align*}
\dot{\bx} = f(\bx, t), && \bx(t_0) = \bx_0
\end{align*}
where $t_0 \in (a, b)$. Note that this result applies to first-order ODEs; however, from the previous section we know that we can reduce an ODE of order $n$ to a first-order ODE 
by extending the state space. Applying that result, we can re-write 
\begin{align*}
Lx = a_0(t)x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dots + a_n(t) x^{(n)} = 0
\end{align*}
as 
\begin{align*}
\dot{\bx} &= A\bx, &&\bx = \begin{pmatrix} x & \dot{x} & \hdots & x^{(n-1)} \end{pmatrix}^\top
\end{align*}
Now consider the $n$ separate IVPs for some $t_0 \in (a, b)$:
\begin{align*}
\dot{\bx} &= A\bx, \ \bx(t_0) := \be_i, &&i = 1, \dots, n
\end{align*}
where $\be_i \in \R^n$ denotes the $i^{\text{th}}$ standard basis vector. Caratheodory's existence theorem guarantees unique solutions 
$\bx^{(1)}, \dots, \bx^{(n)}$ for each respective IVP. I claim that these $n$ functions form a basis for $\text{Null}(L)$, which will establish that $\text{Null}(L)$ has dimension $n$.
To show that they are linearly independent, simply note that $\bx^{(i)}(t_0) = \be_i$ so that the vectors $\{\bx^{(i)}(t_0)\}_{i = 1}^{n}$ are linearly independent. Thus, the functions 
 $\{\bx^{(i)}\}_{i = 1}^{n}$ must also be independent. It remains to show that these functions span $\text{Null}(L)$. To this end, suppose $\tilde{x} \in \text{Null}(L)$ so that $L\tilde{x} = 0$, 
 which can be reduced to $\dot{\tilde{\bx}} = A\tilde{\bx}$ as above. 
 
 Since the initial conditions $\{\bx^{(i)}(t_0)\}_{i = 1}^{n} = \{\be_i\}_{i = 1}^{n}$ span $\R^n$ then $\tilde{\bx}(t_0) = \sum_{i = 1}^{n} c_i \bx_i(t_0)$ for scalars $c_i$ (clearly, $c_i$ is simply 
 the $i^{\text{th}}$ entry of $\tilde{\bx}$). Now, consider the IVP:
 \begin{align}
 \dot{\bx} &= A\bx, && \bx(t_0) = \sum_{i = 1}^{n} c_i \bx^{(i)}(t_0) \label{IVP_tilde}
 \end{align}
 By construction we have that $\tilde{\bx}$ is a solution to \ref{IVP_tilde}. I claim that $\sum_{i = 1}^{n} c_i \bx^{(i)}$ is also a solution, in which case by uniqueness it must be that 
 $\tilde{\bx} = \sum_{i = 1}^{n} c_i \bx^{(i)}$ and hence $\tilde{\bx} \in \text{Span}\left(\bx^{(1)}, \dots, \bx^{(in)} \right)$. This is clearly true since $\sum_{i = 1}^{n} c_i \bx^{(i)}$ satisfies the initial 
 condition and 
 \begin{align*}
 \frac{d}{dt} \sum_{i = 1}^{n} c_i \bx^{(i)} &= \sum_{i = 1}^{n} c_i \dot{\bx}^{(i)} = \sum_{i = 1}^{n} c_i A\bx^{(i)} = A \sum_{i = 1}^{n} c_i \bx^{(i)} 
 \end{align*}
 We have shown that $\bx^{(1)}, \dots, \bx^{(n)}$ is a basis for the set of solutions to $\dot{\bx} = A\bx$. Recall that this matrix system encodes the original problem $Lx = 0$. Therefore, it 
 immediately follows that $\bx^{(1)}_1, \dots, \bx^{(n)}_1$ form a basis for $\text{Null}(L)$. The fact that these functions span the null space is immediate, and also note that 
 each vector $\bx^{(i)}$ contains the original state and its derivatives. Thus, for the vectors to be independent the states $x(t)$ must also be independent; otherwise, the derivatives 
 would follow the same dependence relation as the original states. $\blacksquare$

\bigskip
Let us consider the third order homogenous equation with constant coefficients. 
\begin{align*}
a_0 x + a_1 \dot{x} + a_2 \ddot{x} + a_3 \dddot{x} = 0 
\end{align*}
That is, we seek the null space of the differential operator 
\[L = a_0 + a_1 \frac{d}{dt} + a_2 \frac{d^2}{dt^2} + a_3 \frac{d^3}{dt^3} \]
To solve this equation, we may use the following logic: we know that the terms on the left-hand side must sum to zero. Thus, we immediately 
know that $x$ cannot be something like a polynomial $x^p$ since taking derivatives would yield lower order polynomials which wouldn't cancel with 
$x^p$. Thus, the derivative operations must not change the general form of the function in order for the terms to cancel. We know that a function with 
this property is the exponential $x(t) = e^{\lambda t}$. Thus, let us guess that this is the solutions, leaving $\lambda$ unspecified for now so that we can 
determine what $\lambda$ would have to be in the case that the solution really has this form. Plugging in the guess, we have 
\begin{align*}
L e^{\lambda t} &= a_0 e^{\lambda t} + a_1 \lambda e^{\lambda t} +  a_2 \lambda^2 e^{\lambda t} +  a_3 \lambda^3 e^{\lambda t} = 0
\end{align*} 
We can factor out $e^{\lambda t}$
\begin{align*}
e^{\lambda t} \left(a_0 + a_1 \lambda + a_2 \lambda^2 + a_3 \lambda^3 \right) = 0
\end{align*} 
and since $e^{\lambda t} > 0$ we can divide both sides through by this term to obtain 
\begin{align*}
a_0 + a_1 \lambda + a_2 \lambda^2 + a_3 \lambda^3 = 0
\end{align*}
Note that another way to express this idea is to say that $e^{\lambda t}$ is an \textit{eigenfunction} of the differential operator. Indeed, we just showed that 
$L e^{\lambda t} = ce^{\lambda t}$, where $c = a_0 + a_1 \lambda + a_2 \lambda^2 + a_3 \lambda^3$. 

The same reasoning of course extends to general equations of order $n$, in which case we arrive at the polynomial equation
\begin{align*}
a_0 + a_1 \lambda + a_2 \lambda^2 + a_3 \lambda^3  + \dots + a_n \lambda^n= 0 \label{characteristic_polynomial}
\end{align*}
The righthand side of this equation is called the \textit{characteristic polynomial}. 
We know from the fundamental theorem of algebra that \ref{characteristic_polynomial} has exactly $n$ complex solutions, counted with multiplicity. 
Each distinct solution of $\lambda$ yields an independent vector in $\text{Null}(L)$ since $e^{\lambda_1 t}$ and $e^{\lambda_2 t}$ are linearly 
independent when $\lambda_1 \neq \lambda_2$. 

\bigskip
\noindent
\textbf{Example.} Let us return to the example
\[\ddot{x} - 6\dot{x} + 8x = 3e^{-5t}\]
for which we already found a particular solution $x_p = \frac{1}{21} e^{-5t}$. We now seek the general solution, which requires finding a basis 
for the null space of $L$. We know from above that this comes down to solving 
\[8 - 6\lambda + \lambda^2 = 0\]
In this case the solutions are easily seen to be $\lambda = 2, 4$ so $\{e^{2t}, e^{4t}\}$ is a basis for the null space. Note that the dimension of the basis
is indeed equal to the order of the equation, as we proved in general above. Therefore, the general solution to this (very simple) ODE is 
\[\frac{1}{21} e^{-5t} + \text{Span}\left(e^{2t}, e^{4t}\right)\]
The convention is to just write this as 
\[\frac{1}{21} e^{-5t} + C_1 e^{2t} + C_2 e^{4t}  \]
\textbf{TODO}: add some initial conditions. 

% Harmonic Oscillator
\subsection{The Harmonic Oscillator}
While we have mostly been focusing on big ideas, we take a bit of time here to discuss solutions to a specific second order ODE, the harmonic oscillator. 
Just as $\dot{\state} = \lambda \state$ is the canonical first order ODE with closed-form solutions, the simple harmonic oscillator enjoys a similar status 
among second order ODEs.To motivate this system from a physical perspective, consider a mass $m$ attached to a horizontal (so that we can ignore 
gravity) spring. We let the state $\state$ denote the horizontal displacement from the steady-state position of rest, with a positive sign indicating displacement 
to the right. Hooke's law states the empirical observation that the displacement of the mass is proportional to the force applied,
\[F \propto -\state.\]
The negative sign captures the intuitive notion that the restoring force of the spring is in the opposite direction of the displacement. 
This proportionality is typically expressed by introducing a constant $k$ such that $F = -k \state$, where the \textit{spring constant} $k$ depends on the stiffness 
properties of the spring. Applying Newton's second law, we obtain 
\[F = ma = m\ddot{x} = -k \state\]
We thus arrive at the differential equation 
\begin{align}
m\ddot{x} + k \state = 0.
\end{align}
Here we will often simplify matters by considering unit spring constant $k = 1$. We also add some boundary conditions by assuming the initial 
displacement is $\state_0$ and that the mass starts at rest. These assumptions yields the initial value problem  
\begin{align}
&m \ddot{x} + \state = 0, && \state_0 := \state(0), v_0 := \dot{\state}(0) = 0. \label{harmonic_oscillator}
\end{align}
We could have also motivated this equation in many other ways, for example by considering the motion of pendulums (with small period of oscillation) instead of springs. 
As hinted at, this second order ODE has a closed-form solution. Given the fundamental importance of this equation, we consider multiple solution methods. 

\subsubsection{Closed-Form Solution}

\textbf{Method 1: Guess sinusoids.} Using some intuition is a good place to start. Given our physical intuition that the spring will oscillate, it is reasonable to think that the solution
will involve sines and cosines. This also aligns with out mathematical intuition, since \ref{harmonic_oscillator} tells us that the solution $\state(t)$ must be a function that 
whose (scaled) second derivative cancels out the function itself. Now, should the solution be a pure sine wave, pure cosine wave, or a mixture of the two. Well, if 
$\state_0 = 0$ then we know the sine wave satisfies the initial condition since $\sin(0) = 0$. However, if the $\state_0 \neq 0$ then we must consider a cosine way to 
achieve the non-zero initial condition.
 Let's consider the more general latter case. To satisfy the initial displacement condition we must consider $\state_0 \cos(t)$.
 Thus, proposing the solution $\state(t) = \state_0 \cos(t)$ and plugging into the lefthand side of 
\ref{harmonic_oscillator} yields $\state_0 (1 - m)\cos(t)$, which is non-zero except when $m = 1$. To correct for the mass constant, we instead posit the solution 
$\state(t) = \state_0 \cos\left(\frac{t}{\sqrt{m}}\right)$. Plugging this in to the lefthand side of \ref{harmonic_oscillator} now gives zero, as required. All that is left is to 
check the initial velocity condition. We have 
\[
\dot{\state}(0) = - \state_0 \sin\left(0\right) = 0, 
\]
which satisfies the initial condition. We have thus found the solution to the initial value problem to be
\[
\state(t) = \state_0 \cos\left(\frac{t}{\sqrt{m}}\right).
\]
Note that we can easily generalize this solution to the case of non-unit spring constant $k$. Assuming $k \neq 0$ we can divide through by $k$ to obtain the ODE
\[
\frac{m}{k} \ddot{x} + \state = 0,
\]
which is in the same form as the problem we just solved. Thus, the solution in this more general case is 
\[
\state(t) = \state_0 \cos\left(\sqrt{\frac{k}{m}} t \right).
\]
The $\sqrt{\frac{k}{m}}$ term here is the \textit{natural frequency}, which depends on the stiffness of the spring and the mass of the object. The system will 
complete one full oscillation in $2\pi \frac{m}{k}$ units of time. Note that $\omega := \frac{m}{k}$ is often referred to as the \textit{angular frequency}. 
Finally, note that this cosine solution cannot accommodate any non-zero initial velocity $v_0 := \dot{\state}(0)$. To generalize to this case, a natural guess would 
be that we must introduce a sine wave. Indeed, this is the case and the general solution can be seen to be
\[
\state(t) = \state_0 \cos\left(\sqrt{\frac{k}{m}} t \right) + v_0  \sqrt{\frac{m}{k}} \sin\left(\sqrt{\frac{k}{m}} t  \right)
\]

\bigskip
\noindent
\textbf{Method 2: Guess exponential.} We know that another reasonable guess when we want derivatives of functions to look like the functions themselves is 
the exponential. However, we know that this system experiences oscillation, not exponential growth or decay; thus, the exponential under consideration here 
will be of the complex variety. Since Euler's formula states
\[
e^{it} = \cos(t) + i \sin(t),
\]
the complex exponential is essentially a different way to package the cosine and sine terms we discovered using the previous method. To this end, let us 
suppose the solution of
\begin{align*}
&\ddot{\state} + \state = 0, && \state(0) = x_0, \dot{\state}(0) = v_0 
\end{align*}
is of the form $\state(t) = e^{\lambda t}$. Again, we are anticipating $\lambda$ to be a complex number here. Note that we are again taking $m = k = 1$ for simplicity, 
but this can easily be generalized. Computing the first two derivatives of $\state(t)$ and plugging into the ODE yields 
\[
\lambda^2 e^{\lambda t} + e^{\lambda t} = 0, 
\]
or $\lambda^2 = -1$. Thus, we have imaginary solutions $\lambda = \pm i$. Since this ODE is linear, then by superposition, linear combinations of solutions remain 
solutions. Thus, the solution is of the form 
\[
\state(t) = C_1 e^{it} + C_2 e^{-it},
\]
for some, potentially complex, constants $C_1 = a_1 + b_1 i$, $C_2 = a_2 + b_2 i$. It remains to find $C_1$ and $C_2$ by plugging in the initial conditions. 
Note that since the ODE solution is real, then the imaginary part of the solution must necessarily be zero. We will see how this plays out in the algebra. 
Initial position and velocity conditions give 
\begin{align*}
\state(0) &= C_1 + C_2 &&\implies &&&C_1 + C_2 = \state_0 \\
\dot{\state}(0) &= i C_1 - i C_2 &&\implies &&&i(C_2 - C_1) = v_0.
\end{align*}
To solve the system of equations on the righthand side we equate the real and imaginary parts. This gives another system of equations 
\begin{align*}
(a_1 + a_2) + (b_1 + b_2)i &= x_0 \\
(a_1 - a_2)i - (b_1 - b_2) &= v_0.
\end{align*}
Since the imaginary term must vanish to achieve a real solution, we have the additional constraints $b_1 + b_2 = 0$ and $a_1 - a_2 = 0$. We now have four independent 
linear equations in four unknowns, which can be solved to obtain 
\begin{align*}
&a_1 = \frac{x_0}{2}, a_2 = \frac{x_0}{2} \\
&b_1 = -\frac{v_0}{2}, b_2 = \frac{v_0}{2}.
\end{align*}
With the constants determined, we have solved the problem. Let us now apply Euler's formula to write the solution in terms of sines and cosines in order to see that it 
is equivalent to the solution previously found. We have 
\begin{align*}
\state(t) &= C_1 e^{it} + C_2 e^{-it} \\
	    &= \left[C_1 + C_2\right] \cos(t) + \left[ C_1 - C_2\right]i \sin(t) \\
	    &= \state_0 \cos(t) + v_0 \sin(t),
\end{align*}
which agrees with the previous solution! 

\bigskip
\noindent
\textbf{Method 3: Suspend Variables.} For the final approach, we consider extending the state space to include the velocity $v$ as an additional state. 




% Numerical Methods for Solving General ODEs
\section{Numerical Methods for Solving General ODEs}

\subsection{Forward Euler}
Introduce in the intuitive way of simply stepping in the direction of the gradient. Also emphasize the alternative derivation based on discretizing the integral used to 
step forward in time. 

% Prep for Multiscale methods course
\section{Prep for Multiscale Methods Course}
\begin{itemize}
\item Generic solution of ODE as an integral (see Brunton and Saarka)
\item Multiple ways to solve $\dot{x} = ax$. 
\item Fourier series solution of the heat equation (see Stanford course notes). 
\item Basic numerical solution of ODEs. 
\item Strogatz lecture on WKB; motivating example of simple harmonic oscillator
\end{itemize}





\end{document}


