\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\usepackage{bbm}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\be}{\mathbf{e}}

\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}


\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% For embedding images
\graphicspath{ {./images/} }

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}

% Title and author
\title{Linear Dynamical Systems and Ordinary Differential Equations}
\author{Andrew Roberts}

\begin{document}

\maketitle
\tableofcontents
\newpage

% An Introduction to ODEs
\section{An Introduction to ODEs}

\subsection{General Definitions}

\subsection{Examples}

\subsection{Solving First-Order Scalar ODEs}

\subsubsection{Guessing the solution}

\subsubsection{Integration}
See e.g. Sarkka SDEs book, first chapter. Motivates the matrix exponential solution for vector systems. 



% Matrix Systems
\section{Matrix Systems}
The focus on this section is an in-depth study of the first-order matrix system of ODEs
\begin{align}
\dot{\bx} = A\bx \label{matrix_system}
\end{align}
where 
\begin{align*}
&\bx(t) = \begin{pmatrix} x_1(t) & x_2(t) & \hdots & x_n(t) \end{pmatrix}^\top \in \R^n, &&A \in \R^{n \times n}
\end{align*}
The amount of time we spend studying this system is justified by the following points. 
\begin{enumerate}
\item Higher-order linear ODEs can be reduced to a matrix system of the form \ref{matrix_system}, and thus characterizing the solution 
of \ref{matrix_system} immediately gives a characterization of the broad class of higher-order linear ODEs. 
\item Such linear systems are quite general and show up all over the place in applications. 
\item The study of non-linear systems often boils down to extending techniques from the linear case, or reducing to the 
linear case via linearization. 
\end{enumerate}

\subsection{Big Picture and Interpretation}
See Stanford linear dynamical systems lectures. Also mention view as a system of coupled dynamics. 

\subsection{Solving $\dot{\bx} = A\bx$}
\subsubsection{Eigenvalues and Eigenvectors}



\subsubsection{The Matrix Exponential}
Briefly discuss different ways to compute it; i.e. Fourier/Laplace/Taylor series

\subsection{Matrix System with Forcing}
\subsubsection{Integrating Factors}
See Saarka SDEs book


% Higher-Order Linear ODEs
\section{Higher-Order Linear ODEs}
Linear ODEs are generally of the form 
\begin{align}
a_0(t)x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dots + a_n(t) x^{(n)} = b(t) \label{linear_ODE}
\end{align}
This equation is said to be of order $n$ due to the fact that $n$ derivatives of the state vector $x(t) \in \R^P$ appear in the equation. 
The equation is linear in the state vector $x$ and its $n$ derivatives, $\{x, \dot{x}, \ddot{x}, \dots, x^{(n)}\}$. The coefficients 
$a_1(t), \dots, a_n(t)$ may be non-linear functions of $t$. In the case when the coefficients are simply constants 
\begin{align*}
a_0 x + a_1 \dot{x} + a_2 \ddot{x} + \dots + a_n x^{(n)} = b(t)
\end{align*}
the equation is known as an $n^{\text{th}}$ order linear ODE with constant coefficients. The emphasis here is not on finding analytical solutions 
to such systems, but rather understanding the underlying structure. In particular, I will try to emphasize the application of linear algebra in studying 
such systems far more than I emphasize arguments based on calculus. In this spirit, I start by exploring the general structure of the solutions to 
$n^{\text{th}}$ order linear ODEs by taking a linear operator viewpoint. 

\subsection{Reduction to a First-Order System}
I now establish another connection between linear ODEs \ref{linear_ODE} and linear systems. In particular, I show that \ref{linear_ODE} can be written as a first order linear 
equation 
\[\dot{\bx} = A \bx + L \bb \]
by extending the state-space $x$ to a vector of states $\bx$. In the homogenous case this reduces to 
\[\dot{\bx} = A \bx\]
which is the basic vector-valued first-order linear ODE studied in the previous section. For clarity of notation, we consider a third-order system,
\[a_0(t) x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dddot{x} = b(t)\]
but the same exact reasoning applies to equations of general order. To keep the algebra clean we have also assumed $a_3(t) = 1$, which can be achieved by dividing both 
sides of the equation by the coefficient on the highest-order term. I define the extended state vector 
\begin{align*}
\bx :=  \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} := \begin{pmatrix} x \\ \dot{x} \\ \ddot{x} \end{pmatrix}
\end{align*}
The dimension of $\bx$ in general matches the order of the ODE, as $\bx$ includes the original state $x$ plus all of its derivatives except for the last one $x^{(n)}$. 
The time derivative of $\bx$ is then given by 
\begin{align*}
\dot{\bx} =  \begin{pmatrix} \dot{x}_1 \\ \dot{x}_2 \\ \dot{x}_3 \end{pmatrix} = \begin{pmatrix} \dot{x} \\ \ddot{x} \\ \dddot{x} \end{pmatrix} &=  
\begin{pmatrix} x_2 \\ x_3 \\ b(t) -  a_0(t) x_1 - a_1(t) x_2 - a_2(t) x_3 \end{pmatrix} \\
&= \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ -a_0(t) & -a_1(t) & -a_2(t) \end{pmatrix}  \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} + \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} b(t) \\
&=: A\bx + L b
\end{align*}
The resulting forcing term $L b$ is quite simple, since $L$ is simply a vector and $b = b(t)$ a scalar. This reduction of higher order linear ODEs to a first order matrix system allows 
us to apply theory and algorithms concerning the latter to equations like \ref{linear_ODE}.

\subsection{Linear Operator Viewpoint and the Solution Space}
We can view the left side of \ref{linear_ODE} as a linear operator that maps between function spaces. In particular, let $L: C^n(a, b) \to C(a, b)$
\footnote{$C^n(a, b)$ denotes the space of functions on $(a, b)$ with n continuous derivatives. $C(a, b)$ is the space of continuous functions on $(a, b)$} be the operator 
that maps $x(t)$ to $a_0(t)x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dots + a_n(t) x^{(n)}$. The equation \ref{linear_ODE} can now be written 
\begin{align}
Lx &= b \label{linear_ODE_operator_form}
\end{align}
where I note that $x = x(t)$ and $b = b(t)$ are both functions of $t$, though this is suppressed in the notation to emphasize the fact that we are viewing $x$ and 
$b$ abstractly as vectors. The operator $L$ is often called a \textit{differential operator} and is often written 
\begin{align*}
L &= a_0(t) + a_1(t) \frac{d}{dt} + a_2(t) \frac{d^2}{dt^2} + \dots + a_n(t) \frac{d^n}{dt^n}
\end{align*}
Note that $L$ is indeed a linear operator (i.e. it satisfies $L(\alpha x_1 + \beta x_2) = \alpha L x_1 + \beta L x_2$) due to the fact that differentiation 
is a linear operation. By writing the linear ODE as \ref{linear_ODE_operator_form}, we see that solving linear ODEs can be viewed as solving a linear equation, though the unknown 
$x$ in the equation is infinite-dimensional. 

With this powerful viewpoint, we can immediately establish what solutions of \ref{linear_ODE} must look like. First, suppose we have already obtained a \textit{particular solution}
$x_p$ such that $L x_p = b$. My claim is that the space of solutions of \label{linear_ODE} can be written as $x_p + \text{Null}(L)$, where $\text{Null}(L)$ 
is the null space of the linear operator $L$. If $x_n \in \text{Null}(L)$ then $L x_n = 0$ (where the righthand side is the zero function) so 
\[L(x_p + x_n) = L x_p + L x_n = b + 0 = b\]
so indeed $x_p + x_n$ is a solution. To show that all solutions are of this form, suppose that $x$ is some other solution such that $Lx = b$. Then we can write $x$ as 
$x = x_p + (x - x_p)$ where $L(x - x_p) = Lx - L x_p = b - b = 0$ so $x - x_p \in \text{Null}(L)$. This establishes that all solutions are of the form $x_p + \text{Null}(L)$. 

\subsubsection{Finding the Particular Solution}
I will not spend much time on particular solution methods, as the emphasis here is on big picture ideas. However, it is helpful to reason through one simple example. 
Consider,  
\[\ddot{x} - 6\dot{x} + 8x = 3e^{-5t}\]
an example I took from \href{https://www.youtube.com/watch?v=3eFiwF8pUR4&list=PLlXfTHzgMRUK56vbQgzCVM9vxjKxc8DCr&index=5}{MathTheBeautiful's} excellent 
YouTube channel. We are seeking a function $x(t)$ such that taking a linear combination of itself and its derivatives yields $3e^{-5t}$. Since the derivative of the exponential 
is proportional to itself, then a good guess here would be $x(t) \propto e^{-5t}$. Let's plug this in and see what we get. 
\begin{align*}
L e^{-5t} &= 25e^{-5t} + 30 e^{-5t} + 8 e^{-5t} = 63 e^{-5t}
\end{align*}
So we have overshot the mark by a factor of $21$. We can easily correct for this by dividing through by $21$, yielding the particular solution 
\[x_p(t) = \frac{1}{21} e^{-5t}\]
I should note that this example was engineered to be very simple to reason through. The constant coefficients and fact that the forcing function is an exponential make this 
equation very easy to solve. Progressing to more complicated examples depends more sophisticated solution techniques (in the case that analytical solutions even exist). 
Working through these particular solution methods are not the emphasis of these notes. 

\subsubsection{Characterizing the Null Space}
The big result here is that the dimension of $\text{Null}(L)$ is finite-dimensional, and in particular equal to $n$, the order of the equation. This means that the general solution 
$x_p + \text{Null}(L)$ consists of a linear subspace of dimension $n$. In practice, this means that we require $n$ initial conditions in order to solve the equation: 
$x(t_0), \dot{x}(t_0), \dots, x^{(n-1)}(t_0)$. It is also worth emphasizing that the null space of $L$ has nothing to do with $b(t)$, the driving term in the inhomogenous equation. 
The particular solution $x_p$ of course depends on $b(t)$, but once a particular solution is found then finding the remaining solutions has nothing to do with $b(t)$. 

I now prove that indeed $\text{dim } \text{Null}(L) = n$. The proof will require Caratheodory's existence theorem, which under mild conditions guarantees a unique solution to the initial value problem (IVP) 
\begin{align*}
\dot{\bx} = f(\bx, t), && \bx(t_0) = \bx_0
\end{align*}
where $t_0 \in (a, b)$. Note that this result applies to first-order ODEs; however, from the previous section we know that we can reduce an ODE of order $n$ to a first-order ODE 
by extending the state space. Applying that result, we can re-write 
\begin{align*}
Lx = a_0(t)x + a_1(t) \dot{x} + a_2(t) \ddot{x} + \dots + a_n(t) x^{(n)} = 0
\end{align*}
as 
\begin{align*}
\dot{\bx} &= A\bx, &&\bx = \begin{pmatrix} x & \dot{x} & \hdots & x^{(n-1)} \end{pmatrix}^\top
\end{align*}
Now consider the $n$ separate IVPs for some $t_0 \in (a, b)$:
\begin{align*}
\dot{\bx} &= A\bx, \ \bx(t_0) := \be_i, &&i = 1, \dots, n
\end{align*}
where $\be_i \in \R^n$ denotes the $i^{\text{th}}$ standard basis vector. Caratheodory's existence theorem guarantees unique solutions 
$\bx^{(1)}, \dots, \bx^{(n)}$ for each respective IVP. I claim that these $n$ functions form a basis for $\text{Null}(L)$, which will establish that $\text{Null}(L)$ has dimension $n$.
To show that they are linearly independent, simply note that $\bx^{(i)}(t_0) = \be_i$ so that the vectors $\{\bx^{(i)}(t_0)\}_{i = 1}^{n}$ are linearly independent. Thus, the functions 
 $\{\bx^{(i)}\}_{i = 1}^{n}$ must also be independent. It remains to show that these functions span $\text{Null}(L)$. To this end, suppose $\tilde{x} \in \text{Null}(L)$ so that $L\tilde{x} = 0$, 
 which can be reduced to $\dot{\tilde{\bx}} = A\tilde{\bx}$ as above. 
 
 Since the initial conditions $\{\bx^{(i)}(t_0)\}_{i = 1}^{n} = \{\be_i\}_{i = 1}^{n}$ span $\R^n$ then $\tilde{\bx}(t_0) = \sum_{i = 1}^{n} c_i \bx_i(t_0)$ for scalars $c_i$ (clearly, $c_i$ is simply 
 the $i^{\text{th}}$ entry of $\tilde{\bx}$). Now, consider the IVP:
 \begin{align}
 \dot{\bx} &= A\bx, && \bx(t_0) = \sum_{i = 1}^{n} c_i \bx^{(i)}(t_0) \label{IVP_tilde}
 \end{align}
 By construction we have that $\tilde{\bx}$ is a solution to \ref{IVP_tilde}. I claim that $\sum_{i = 1}^{n} c_i \bx^{(i)}$ is also a solution, in which case by uniqueness it must be that 
 $\tilde{\bx} = \sum_{i = 1}^{n} c_i \bx^{(i)}$ and hence $\tilde{\bx} \in \text{Span}\left(\bx^{(1)}, \dots, \bx^{(in)} \right)$. This is clearly true since $\sum_{i = 1}^{n} c_i \bx^{(i)}$ satisfies the initial 
 condition and 
 \begin{align*}
 \frac{d}{dt} \sum_{i = 1}^{n} c_i \bx^{(i)} &= \sum_{i = 1}^{n} c_i \dot{\bx}^{(i)} = \sum_{i = 1}^{n} c_i A\bx^{(i)} = A \sum_{i = 1}^{n} c_i \bx^{(i)} 
 \end{align*}
 We have shown that $\bx^{(1)}, \dots, \bx^{(n)}$ is a basis for the set of solutions to $\dot{\bx} = A\bx$. Recall that this matrix system encodes the original problem $Lx = 0$. Therefore, it 
 immediately follows that $\bx^{(1)}_1, \dots, \bx^{(n)}_1$ form a basis for $\text{Null}(L)$. The fact that these functions span the null space is immediate, and also note that 
 each vector $\bx^{(i)}$ contains the original state and its derivatives. Thus, for the vectors to be independent the states $x(t)$ must also be independent; otherwise, the derivatives 
 would follow the same dependence relation as the original states. $\blacksquare$

\bigskip
Let us consider the third order homogenous equation with constant coefficients. 
\begin{align*}
a_0 x + a_1 \dot{x} + a_2 \ddot{x} + a_3 \dddot{x} = 0 
\end{align*}
That is, we seek the null space of the differential operator 
\[L = a_0 + a_1 \frac{d}{dt} + a_2 \frac{d^2}{dt^2} + a_3 \frac{d^3}{dt^3} \]
To solve this equation, we may use the following logic: we know that the terms on the left-hand side must sum to zero. Thus, we immediately 
know that $x$ cannot be something like a polynomial $x^p$ since taking derivatives would yield lower order polynomials which wouldn't cancel with 
$x^p$. Thus, the derivative operations must not change the general form of the function in order for the terms to cancel. We know that a function with 
this property is the exponential $x(t) = e^{\lambda t}$. Thus, let us guess that this is the solutions, leaving $\lambda$ unspecified for now so that we can 
determine what $\lambda$ would have to be in the case that the solution really has this form. Plugging in the guess, we have 
\begin{align*}
L e^{\lambda t} &= a_0 e^{\lambda t} + a_1 \lambda e^{\lambda t} +  a_2 \lambda^2 e^{\lambda t} +  a_3 \lambda^3 e^{\lambda t} = 0
\end{align*} 
We can factor out $e^{\lambda t}$
\begin{align*}
e^{\lambda t} \left(a_0 + a_1 \lambda + a_2 \lambda^2 + a_3 \lambda^3 \right) = 0
\end{align*} 
and since $e^{\lambda t} > 0$ we can divide both sides through by this term to obtain 
\begin{align*}
a_0 + a_1 \lambda + a_2 \lambda^2 + a_3 \lambda^3 = 0
\end{align*}
Note that another way to express this idea is to say that $e^{\lambda t}$ is an \textit{eigenfunction} of the differential operator. Indeed, we just showed that 
$L e^{\lambda t} = ce^{\lambda t}$, where $c = a_0 + a_1 \lambda + a_2 \lambda^2 + a_3 \lambda^3$. 

The same reasoning of course extends to general equations of order $n$, in which case we arrive at the polynomial equation
\begin{align*}
a_0 + a_1 \lambda + a_2 \lambda^2 + a_3 \lambda^3  + \dots + a_n \lambda^n= 0 \label{characteristic_polynomial}
\end{align*}
The righthand side of this equation is called the \textit{characteristic polynomial}. 
We know from the fundamental theorem of algebra that \ref{characteristic_polynomial} has exactly $n$ complex solutions, counted with multiplicity. 
Each distinct solution of $\lambda$ yields an independent vector in $\text{Null}(L)$ since $e^{\lambda_1 t}$ and $e^{\lambda_2 t}$ are linearly 
independent when $\lambda_1 \neq \lambda_2$. 

\bigskip
\noindent
\textbf{Example.} Let us return to the example
\[\ddot{x} - 6\dot{x} + 8x = 3e^{-5t}\]
for which we already found a particular solution $x_p = \frac{1}{21} e^{-5t}$. We now seek the general solution, which requires finding a basis 
for the null space of $L$. We know from above that this comes down to solving 
\[8 - 6\lambda + \lambda^2 = 0\]
In this case the solutions are easily seen to be $\lambda = 2, 4$ so $\{e^{2t}, e^{4t}\}$ is a basis for the null space. Note that the dimension of the basis
is indeed equal to the order of the equation, as we proved in general above. Therefore, the general solution to this (very simple) ODE is 
\[\frac{1}{21} e^{-5t} + \text{Span}\left(e^{2t}, e^{4t}\right)\]
The convention is to just write this as 
\[\frac{1}{21} e^{-5t} + C_1 e^{2t} + C_2 e^{4t}  \]
\textbf{TODO}: add some initial conditions. 

% Numerical Methods for Solving General ODEs
\section{Numerical Methods for Solving General ODEs}

\subsection{Forward Euler}
Introduce in the intuitive way of simply stepping in the direction of the gradient. Also emphasize the alternative derivation based on discretizing the integral used to 
step forward in time. 


\end{document}


