\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Notes on Linear Programming
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

\section{High Level Overview}
Linear programming, or linear optimization, simply deals with the optimization of linear functions subject to a set of linear constraints: 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax \leq b \\
\end{align*}
where $A \in \R^{m \times n}$, $b \in \R^m$, and $c, x \in \R^n$. Each row $a_i$ of $A$ defines a constraint $a_i^T x \leq b$ and thus this problem
has $m$ constraints. We call a vector $x \in \R^n$ a \textit{feasible point} if it satisfies all of the constraints. Note that since linear functions are convex
(and concave) then this problem is actually a specific case of convex optimization and hence we can apply all of the nice convex optimization theory
(duality, etc.) in this case as well. 

The basic story of linear programming can actually be summarized quite succinctly: the constraints define a \textit{feasible set} in $\R^n$ and due to the fact that the objective function is linear then 
the minimum must occur at one of the ``corners'' of this set. This is really the fundamental theorem of linear programming; it reduces this continuous optimization
problem to a selection problem over a discrete, finite set of points. So, we can just enumerate over these points and we're done, right? In theory, sure, but consider 
why this might be a problem in practice. Consider the following $2n$ inequality constraints which define the $n$-dimensional hypercube as the feasible set in $\R^n$:
\[0 \leq x_i \leq 1, \ \forall i = 1, \dots, n\]
How many corners does this feasible set have? Clearly in $\R^2$ a square has 4 corners, in $\R^3$ a cube has 8, and the pattern continues: in $\R^n$ these $2n$
constraints yield a feasible set with $2^n$ corners. Therefore, the number of potential solutions we'd have to check grows exponentially in the dimension of the 
problem. Moreover, the brute force approach wouldn't just require iterating over these points; we'd have to actually \textit{find} these points by solving linear systems 
of equations. For large $n$ this quickly becomes infeasible. Therefore, the crux of linear programming is to find efficient algorithms that do not require exploring every
corner of the feasible set. There are two primary methods that have emerged: 
\begin{enumerate} 
\item The Simplex Method: Dating back to the 1940s, this is the older of the two methods. It can be thought of as a greedy steepest descent algorithm over the edges of the feasible
set; that is, the algorithm moves from one corner to the adjacent corner that causes the largest decrease in the objective. 
\item Interior Point Methods: Discovered in the late 1960s but popularized in the mid-80s, this is a more modern approach. The general idea is that, despite the fact that the optimum 
must occur at a corner, it may be useful to consider points in the interior of the feasible set. This is due to the fact that in the interior we can actually calculate derivatives and use 
calculus to help determine where to go next; incorporating gradient information can lead to more intelligent choices about which points to consider next. This is kind of like an 
``exploration vs. exploitation'' type idea; although we are intentionally spending time looking at points that we know cannot possibly be the solution, we are gaining much more 
information that can hopefully lead to faster convergence in the long run. 
\end{enumerate}
Although the Simplex Method is older, it is still in use for solving LP problems. However, in more complicated settings (semi-definite programming, quadratic programming) interior 
point methods tend to be the primary tool. 


\section{Examples and Tricks for Representing Problems as LPs}

\subsection{Absolute Values}
Although absolute values introduce non-linearity in some situations we can take advantage of the general form of LPs to be able to represent constraints and/or objective functions
containing absolute values. Straightforward absolute value constraints are actually quite simple; say we want to constraint a decision variable as 
\[\abs{x_i} \leq b_i\]
Well, this is just equivalent to 
\[-b_i \leq x_i \leq b_i\]
which constitutes two linear inequality constraints. Absolute values in the objective function are a bit trickier. Consider, 
\begin{align*}
&\min \sum_{i = 1}^{n} \abs{x_i} \\
&\text{s.t. } Ax \leq b \\
\end{align*}

To represent this as an LP, the idea is to introduce new variables $z_i, i = 1, \dots, n$ which we constrain to be non-negative. These variables are intended to represent $\abs{x_i}$ so 
we must constrain them as $-z_i \leq x_i \leq z_i$, or $\abs{x_i} \leq z_i$. This leads to the formulation, 
\begin{align*}
&\min \sum_{i = 1}^{n} z_i \\
&\text{s.t. } Ax \leq b \\
&\;\;\;\;\;\; -z_i \leq x_i \leq z_i \\
&\;\;\;\;\;\; z_i \geq 0
\end{align*}
Since $\abs{x_i} \leq z_i$ then pushing down $z_i$ will also push down $\abs{x_i}$ and clearly the minimum of the two will coincide. 




\section{Polytopes and Extreme Points}
This section develops the basic geometry of linear programs, and introduces terminology and concepts that will be crucial when considering algorithms for solving linear programs
later on. 

\subsection{Polytopes and Extreme Points}
In the introduction, I have spoken loosely about ``corners'' of feasible sets. To actually begin developing the theory we must put this notion on firm mathematical ground. 
First, we must state a few basic definitions. 
\begin{definition}
A set defined by linear inequality constraints $\{x \in \R^n: Ax \leq b\}$ is called a \textbf{polyhedron}. If it is bounded we call it a \textbf{polytope}. 
\end{definition}

\begin{definition}
A set of linear inequality constraints is said to be \textbf{independent} if the vectors $a_i$ defining the constraints are linearly independent. A constraint is said to be 
\textbf{tight} (or binding or active) if it is satisfied with equality.  
\end{definition}

Now we seek a rigorous definition of ``corners'' of polytopes. Some linear programming texts (e.g. Bertsimas) may spend a whole section introducing a few different characterizations
and then in a dramatic conclusion prove that they are all equivalent. I'm going to skip over this exposition and just state  the result that the three following definitions are all equivalent. 
These are thus all equivalent characterizations of what I was previously calling a ``corner''. 
\begin{definition}
Let $P$ be a polytope. 
\begin{enumerate} 
\item Extreme Point (Convex analysis definition): $x \in P$ is an \textbf{extreme point} of $P$ provided that there does not exist vectors $y \neq z$ in $P$ and scalar $\lambda \in (0, 1)$
such that $x = \lambda y + (1 - \lambda)z$. 
\item Vertex (Optimization definition): $x \in P$ is a \textbf{vertex} of $P$ provided that there exists $c \in \R^n$ satisfying $c^T x < c^T y$ for all $y \in P$, $y \neq x$. This says that there
exists a linear objective such that $x$ is the strictly optimal solution to the constrained optimization problem: $\min_{y \in P} c^T y$. 
\item Basic Feasible Solution (Linear algebra definition): $x \in P$ is a \textbf{basic feasible solution} (BFS) provided that at least $n$ of the constraints are independent and tight. 
\end{enumerate}
\end{definition}
I already gave away that the big result here is that extreme points, vertices, and BFS are all equivalent. While the first definition should be intuitive (draw a picture) the latter two may be 
less so. For the third definition, recall the hypercube example from above. In $\R^2$ the corners of the cube form at the intersection of 2 constraints, and in $\R^3$ at the intersection 
of 3 constraints, etc. This definition thus says that this result is not unique to this example, that corners of polyhedra in $\R^n$ must occur at points where (at least) $n$ constraints 
are active. 

\subsection{Standard Form Polyhedra/Linear Programs}
As a reminder, we defined a linear program as the optimization problem: 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax \leq b \\
\end{align*}

I claim that any problem of this form can be written in the following form: 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax = b \\
&\;\;\;\;\;\;\; x \geq 0
\end{align*}
We call this the \textbf{standard form} representation. Note that $A$, $b$, and $c$ may not be equal to their counterparts in the non-standard form problem. 
The proof is fairly straightforward. 
\begin{proof}
We introduce a vector of \textit{slack variables} $s \in \R^n$, $s \geq 0$: 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax + s = b \\
&\;\;\;\;\;\;\; s \geq 0
\end{align*}
Clearly the optimal solutions to these two problems coincide; the non-negativity constraints on $s$ ensure that any feasible solution in the first 
formulation is also feasible in the second. Also note that the variable $s$ does not show up in the objective of the second problem and thus will
not affect the optimal solution. Next we must show that we can re-write this such that all variables are subject to non-negativity constraints. 
To do this we can simply define new variables that are the positive and negative parts of the existing variables. Let 
$x_i^+ := \max\{x_i, 0\}$ and $x_i^- := \max\{-x_i, 0\}$. Thus $x_i = x_i^+ - x_i^-$ with $x_i^+, x_i^- \geq 0$ so we may re-write the problem as: 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax_i^+ - Ax_i^- + s = b \\
&\;\;\;\;\;\;\; s \geq 0 \\
&\;\;\;\;\;\;\; x_i^+ \geq 0 \\
&\;\;\;\;\;\;\; x_i^- \geq 0
\end{align*}
To be absolutely clear that we have accomplished our goal of writing this as a standard form problem, we can define $\tilde{A} := \begin{pmatrix} A & -A & I \end{pmatrix} \in \R^{m \times 3n}$, 
$\tilde{x} := \begin{pmatrix} x^+ \\ x^- \\ s \end{pmatrix} \in \R^{3n}$, and $\tilde{c} := \begin{pmatrix} c \\ 0 \end{pmatrix} \in \R^{3n}$. Then the above problem can be written: 
\begin{align*}
&\min \tilde{c}^T \tilde{x} \\
&\text{s.t. } \tilde{A}\tilde{x} = b \\
&\;\;\;\;\;\;\; \tilde{x} \geq 0
\end{align*}

\end{proof}

Recall that $A$ is an $m \times n$ matrix. When discussing problems in standard form it is customary to make the assumption $m \leq n$ for the following reason. It can be shown that 
linearly dependent constraints (that is, when $A$ has linearly dependent columns) are in fact redundant. Without loss of generality we can then assume that $A$ has linearly independent 
rows. Since each row of $A$ is in $\R^n$ then this assumption implies the condition $m \leq n$.  

Standard form problems have a very nice interpretation. If we let $A_i$ denote column $i$ of $A$ then the constraints can be written: 
\begin{align*}
&\sum_{i = 1}^{n} A_i x_i = b\\
&\;\;\;\;\;\;\; x \geq 0
\end{align*}
So we may view the columns $A_i$ as ``resource vectors'' and the $x_i$ are non-negative amounts of each resource, subject to the constraint that the total amount of resources must
sum to $b$. The task is thus to minimize the cost subject to this resource constraint. 

In general both the general form and standard forms of a linear program are useful
to work with. The general form (that is, with constraint $Ax \leq b$) is often convenient when developing the theory of linear programming. However, the standard form often tends to be 
more computationally convenient and hence is typically used for the design of algorithms. 
	
\section{LP Duality}
Duality is a very important concept in optimization; more generally, it is a property of convex programming but here we consider duality in the special case of linear programming. Duality 
is a special characterization of optimality in LPs, and thus something that can be exploited to help design optimization algorithms. Before immediately introducing duality, we start by asking 
a different question that will lead us on the path to duality: when is a LP feasible? In other words, when in the feasible set non-empty? Now we see the convenience of the standard form LP, since 
the feasible set is simply defined by a linear system of equations (plus the non-negativity constraints, but let's ignore that for now). Thus, the equivalent question is: does $Ax = b$ have a solution. 
From linear algebra, we already know how to answer this; just use Gaussian elimination! The results from this method are summarized in the following theorem. 
\begin{thm}
Let $A \in \R^{m \times n}$ and $b \in \R^m$, exactly one and only one of the following statements holds: 
\begin{enumerate}
\item $x \in \R^n$ is a solution to $Ax = b$.
\item There exists $y \in \R^m$ such that $y^T A = 0$ but $y^T b \neq 0$. 
\end{enumerate}
When the system does not have a solution then a vector $y \in \R^m$ satisfying the second statement is called a \textbf{certificate of infeasibility}. 
\end{thm}
The second statement might look a bit weird at first glance, but it is actually very intuitive. Suppose we have a system of two equations $E_1$ and $E_2$. We learn in elementary math 
that one strategy to solve such a system is to add the equations together and then solve the resulting single equation. More generally, we might take a linear combination of the two equations
such as $y_1 E_1 + y_2 E_2$ for some $y_1, y_2 \in \R$. If this results in an equation of the form $0 = d$, where $d \neq 0$ we know this is impossible and hence the equation does not have a 
solution. That's all this second statement says; the vector $y$ produces a linear combination of the system of equations and if the result is unsolvable then so is the original system. 

As I briefly alluded to before, the above theorem by itself is not that useful for our current problem as it ignores the non-negativity constraints $x \geq 0$. A result known as \textit{Farkas' Lemma}
gives the analogous result in this case. 
\begin{thm}
Let $A \in \R^{m \times n}$ and $b \in \R^m$, exactly one and only one of the following statements holds: 
\begin{enumerate}
\item $x \in \R^n$ is a solution to $Ax = b$, $x \geq 0$.
\item There exists $y \in \R^m$ such that $y^T A \leq 0$ but $y^T b > 0$. 
\end{enumerate}
\end{thm}
This result is exactly what we would have hoped for; it is basically a direct analog of the previous well-known result from linear algebra but adapted to consider the non-negativity constraints. 

Now we begin to consider duality. Recall that the ultimate goal is to design methods and algorithms to solve LPs. We have seen one way to attack this problem; namely, the Simplex Method. 
Duality theorem will give us a different and powerful avenue of attack for certain problems. The basic idea is to first find upper and lower bounds on the optimal solution. If we are able to tighten 
these bounds then we can get closer and closer to the solution. Ideally we could continue this logic and find the \textit{least upper bound} and \textit{greatest lower bound}, which would then yield
the true solution. The \textit{primal-dual} method does exactly this. 

\bigskip

\textbf{Finding upper bounds:} The naive approach here is simply to guess solutions (i.e. try to pick a point in the feasible set) and then evaluate the solution in some way to determine its 
quality. In particular, if we correctly guess a feasible point and then evaluate the objective function using this point, then the result will be an upper bound on the true minimum. If we are able 
to guess multiple feasible points then we can keep doing this and refining this upper bound. However, it is not obvious how to guess such solutions. For high-dimensional problems especially
this seems like a very challenging task. 

\bigskip

\textbf{Finding lower bounds:} Consider a nice example courtesy of Aaron Singer's lecture notes (see resources). 
\begin{align*}
\min \ &x_1 + 2x_2 + 4x_3 \\
 \text{s.t. } &x_1 + x_2 + 2x_3 = 5 \\
 &2x_1 + x_2 + 3x_3 = 8 \\
 &x_1, x_2, x_3 \geq 0
\end{align*}
Recall that any solution to this system of equations must also solve linear combinations of these equations. So here's an idea: Consider taking twice the first 
constraint equation minus the second; that is
\[\begin{pmatrix} 2 & -1\end{pmatrix}^T A = \begin{pmatrix} 2 & -1\end{pmatrix}^Tb\]
Carrying this out yields the equation: 
\[0x_1 + x_2 + x_3 = 2\]
So the optimal solution, whatever it may be, \textit{must} satisfy this equation. But take a look at the objective function
\[x_1 + 2x_2 + 4x_3\]
We notice that each of the values scaling the objective function is strictly larger than those in the above equation that the solution must satisfy 
(1 > 0, 2 > 1, 4 > 1). Since the $x_i$ are constrained to be non-negative this means that the true minimum value can be no smaller than 2. We conclude 
that 2 is a lower bound on the true minimum. If we happened to pick a linear combination of the constraints and a feasible point that resulted in lower and 
upper bounds of the same value, then we would know we found the optimal solution. This seems very unlikely, but the ideas explored here turn out to 
be powerful. 

\subsection{Primal-Dual Theory}
We will call the standard LP we have been working with the \textbf{primal problem}. 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax = b \\
&\;\;\;\;\;\;\; x \geq 0
\end{align*}

The \textbf{dual problem} seeks to solve the same problem by maximizing a lower bound on the optimal solution in the primal problem. The definition of the dual simply 
follows from generalizing the lower bound example presented above. Recall what we did: we took a linear combination of the constraint equations $y^T A = y^T b$, where 
the vector $y$ encodes the coefficients in the linear combinations. Then we concluded that $y^T b$ was a lower bound on the optimal solution provided that the coefficients
resulting from $y^T A$ were smaller than the respective coefficients in the cost vector $c$; that is, $y^T A \leq c^T$. The dual problem simply considers the maximum of lower 
bounds of this form. 
\begin{align*}
&\min y^T b \\
&\text{s.t. } y^T A \leq c \\
\end{align*}

From the preceding example, we suspect that these two problems should give the same solution. We proceed to make this intuition rigorous. We begin with a slightly less
ambitious goal, simply proving that the dual formulation provides a lower bound on the optimal solution. This is known as \textbf{weak duality}. 

\begin{thm}
Let $x \in \R^n$ and $y \in \R^m$ be solutions to the primal and dual problems, respectively. Then $y^T b \leq c^T x$. 
\end{thm}

\begin{proof}
This simply uses the feasibility of $x$ and $y$ in the respective problems. 
\begin{align*}
y^T b &= y^T(Ax) && Ax = b \text{ from primal} \\
	 &= (y^T A)x \\
	 &\leq c^T x && y^T A \leq c^T \text{ from dual and } x \geq 0 \text{ from primal} \\
\end{align*}
\end{proof}

We now state the major result in primal-dual theory, that the problems yield the same solution. 
\begin{thm}
If either the primal or dual problem is feasible, then they share the same optimal solution. 
\end{thm}

\begin{proof}
\textbf{TODO}
\end{proof}

\textbf{TODO}: Include proof that dual of the dual is the primal. Discuss how infeasibility can be concluded between the problems, etc.  

\subsection{Applications of Duality}
The primal-dual theory is very nice and all, but how do we actually use it? This section details applications to game theory and learning theory. 

\subsubsection{Two-Player Games and the Minimax Theorem}
We consider the basic setup of two-player games from game theory. Suppose players A and B have sets of strategies $S_A = \{1, \dots, m\}$ and 
$S_B = \{1, \dots, n\}$, respectively. We also suppose the two players have respective \textit{payoff matrices} $A \in \R^{m \times n}$ and 
$B \in \R^{m \times n}$. These matrices are defined such that player 1 gets payoff $A_{ij}$ and player 2 gets payoff $B_{ij}$ when 
the players choose respective strategies $i \in S_A$, $j \in S_B$. We will assume the players are rational in the sense that they are utility-maximizers. 
This leads us to the notion of a \textbf{Nash Equilibrium}. Intuitively, a Nash Equilibrium is a situation in which, given the other players strategy, neither one
would be better off transitioning to a new strategy. It is in this sense that the game is in equilibrium. 
\begin{definition}
 A Nash Equilibrium is a strategy profile $(i, j) \in S_A \times S_B$ such that each player's strategy is optimal given the other player's strategy; that is, 
 \begin{itemize}
 \item $A_{ij} \geq A_{kj} \text{ for all } k \in S_A$ 
 \item $B_{ij} \geq B_{i\ell} \text{ for all } \ell \in S_B$
 \end{itemize}
\end{definition}


	
\section{TODO}
\begin{itemize}
\item Shortest path LP (lecture 18)
\end{itemize}
	
	
\end{document}
	
	