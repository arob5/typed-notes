\documentclass[12pt]{article}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=1.8}
\usepackage{tikz}

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Ker}{\mathrm{Ker}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\def\R{\mathbb{R}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}

% Specifically for paper formatting 
\renewcommand{\baselinestretch}{1.2} % Spaces manuscript for easy reading

% Formatting definitions, propositions, etc. 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{notation}{Notation}

\begin{document}

\begin{center}
\Large
Notes on Linear Programming
\end{center}

\begin{flushright}
Andrew Roberts
\end{flushright} 

\section{High Level Overview}
Linear programming, or linear optimization, simply deals with the optimization of linear functions subject to a set of linear constraints: 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax \leq b \\
\end{align*}
where $A \in \R^{m \times n}$, $b \in \R^m$, and $c, x \in \R^n$. Each row $a_i$ of $A$ defines a constraint $a_i^T x \leq b$ and thus this problem
has $m$ constraints. We call a vector $x \in \R^n$ a \textit{feasible point} if it satisfies all of the constraints. Note that since linear functions are convex
(and concave) then this problem is actually a specific case of convex optimization and hence we can apply all of the nice convex optimization theory
(duality, etc.) in this case as well. 

The basic story of linear programming can actually be summarized quite succinctly: the constraints define a \textit{feasible set} in $\R^n$ and due to the fact that the objective function is linear then 
the minimum must occur at one of the ``corners'' of this set. This is really the fundamental theorem of linear programming; it reduces this continuous optimization
problem to a selection problem over a discrete, finite set of points. So, we can just enumerate over these points and we're done, right? In theory, sure, but consider 
why this might be a problem in practice. Consider the following $2n$ inequality constraints which define the $n$-dimensional hypercube as the feasible set in $\R^n$:
\[0 \leq x_i \leq 1, \ \forall i = 1, \dots, n\]
How many corners does this feasible set have? Clearly in $\R^2$ a square has 4 corners, in $\R^3$ a cube has 8, and the pattern continues: in $\R^n$ these $2n$
constraints yield a feasible set with $2^n$ corners. Therefore, the number of potential solutions we'd have to check grows exponentially in the dimension of the 
problem. Moreover, the brute force approach wouldn't just require iterating over these points; we'd have to actually \textit{find} these points by solving linear systems 
of equations. For large $n$ this quickly becomes infeasible. Therefore, the crux of linear programming is to find efficient algorithms that do not require exploring every
corner of the feasible set. There are two primary methods that have emerged: 
\begin{enumerate} 
\item The Simplex Method: Dating back to the 1940s, this is the older of the two methods. It can be thought of as a greedy steepest descent algorithm over the edges of the feasible
set; that is, the algorithm moves from one corner to the adjacent corner that causes the largest decrease in the objective. 
\item Interior Point Methods: Discovered in the late 1960s but popularized in the mid-80s, this is a more modern approach. The general idea is that, despite the fact that the optimum 
must occur at a corner, it may be useful to consider points in the interior of the feasible set. This is due to the fact that in the interior we can actually calculate derivatives and use 
calculus to help determine where to go next; incorporating gradient information can lead to more intelligent choices about which points to consider next. This is kind of like an 
``exploration vs. exploitation'' type idea; although we are intentionally spending time looking at points that we know cannot possibly be the solution, we are gaining much more 
information that can hopefully lead to faster convergence in the long run. 
\end{enumerate}
Although the Simplex Method is older, it is still in use for solving LP problems. However, in more complicated settings (semi-definite programming, quadratic programming) interior 
point methods tend to be the primary tool. 


\section{Examples and Tricks for Representing Problems as LPs}

\subsection{Absolute Values}
Although absolute values introduce non-linearity in some situations we can take advantage of the general form of LPs to be able to represent constraints and/or objective functions
containing absolute values. Straightforward absolute value constraints are actually quite simple; say we want to constraint a decision variable as 
\[\abs{x_i} \leq b_i\]
Well, this is just equivalent to 
\[-b_i \leq x_i \leq b_i\]
which constitutes two linear inequality constraints. Absolute values in the objective function are a bit trickier. Consider, 
\begin{align*}
&\min \sum_{i = 1}^{n} \abs{x_i} \\
&\text{s.t. } Ax \leq b \\
\end{align*}

To represent this as an LP, the idea is to introduce new variables $z_i, i = 1, \dots, n$ which we constrain to be non-negative. These variables are intended to represent $\abs{x_i}$ so 
we must constrain them as $-z_i \leq x_i \leq z_i$, or $\abs{x_i} \leq z_i$. This leads to the formulation, 
\begin{align*}
&\min \sum_{i = 1}^{n} z_i \\
&\text{s.t. } Ax \leq b \\
&\;\;\;\;\;\; -z_i \leq x_i \leq z_i \\
&\;\;\;\;\;\; z_i \geq 0
\end{align*}
Since $\abs{x_i} \leq z_i$ then pushing down $z_i$ will also push down $\abs{x_i}$ and clearly the minimum of the two will coincide. 




\section{Polytopes and Extreme Points}
This section develops the basic geometry of linear programs, and introduces terminology and concepts that will be crucial when considering algorithms for solving linear programs
later on. 

\subsection{Polytopes and Extreme Points}
In the introduction, I have spoken loosely about ``corners'' of feasible sets. To actually begin developing the theory we must put this notion on firm mathematical ground. 
First, we must state a few basic definitions. 
\begin{definition}
A set defined by linear inequality constraints $\{x \in \R^n: Ax \leq b\}$ is called a \textbf{polyhedron}. If it is bounded we call it a \textbf{polytope}. 
\end{definition}

\begin{definition}
A set of linear inequality constraints is said to be \textbf{independent} if the vectors $a_i$ defining the constraints are linearly independent. A constraint is said to be 
\textbf{tight} (or binding or active) if it is satisfied with equality.  
\end{definition}

Now we seek a rigorous definition of ``corners'' of polytopes. Some linear programming texts (e.g. Bertsimas) may spend a whole section introducing a few different characterizations
and then in a dramatic conclusion prove that they are all equivalent. I'm going to skip over this exposition and just state  the result that the three following definitions are all equivalent. 
These are thus all equivalent characterizations of what I was previously calling a ``corner''. 
\begin{definition}
Let $P$ be a polytope. 
\begin{enumerate} 
\item Extreme Point (Convex analysis definition): $x \in P$ is an \textbf{extreme point} of $P$ provided that there does not exist vectors $y \neq z$ in $P$ and scalar $\lambda \in (0, 1)$
such that $x = \lambda y + (1 - \lambda)z$. 
\item Vertex (Optimization definition): $x \in P$ is a \textbf{vertex} of $P$ provided that there exists $c \in \R^n$ satisfying $c^T x < c^T y$ for all $y \in P$, $y \neq x$. This says that there
exists a linear objective such that $x$ is the strictly optimal solution to the constrained optimization problem: $\min_{y \in P} c^T y$. 
\item Basic Feasible Solution (Linear algebra definition): $x \in P$ is a \textbf{basic feasible solution} (BFS) provided that at least $n$ of the constraints are independent and tight. 
\end{enumerate}
\end{definition}
I already gave away that the big result here is that extreme points, vertices, and BFS are all equivalent. While the first definition should be intuitive (draw a picture) the latter two may be 
less so. For the third definition, recall the hypercube example from above. In $\R^2$ the corners of the cube form at the intersection of 2 constraints, and in $\R^3$ at the intersection 
of 3 constraints, etc. This definition thus says that this result is not unique to this example, that corners of polyhedra in $\R^n$ must occur at points where (at least) $n$ constraints 
are active. 

\subsection{Standard Form Polyhedra/Linear Programs}
As a reminder, we defined a linear program as the optimization problem: 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax \leq b \\
\end{align*}

I claim that any problem of this form can be written in the following form: 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax = b \\
&\;\;\;\;\;\;\; x \geq 0
\end{align*}
We call this the \textbf{standard form} representation. Note that $A$, $b$, and $c$ may not be equal to their counterparts in the non-standard form problem. 
The proof is fairly straightforward. 
\begin{proof}
We introduce a vector of \textit{slack variables} $s \in \R^n$, $s \geq 0$: 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax + s = b \\
&\;\;\;\;\;\;\; s \geq 0
\end{align*}
Clearly the optimal solutions to these two problems coincide; the non-negativity constraints on $s$ ensure that any feasible solution in the first 
formulation is also feasible in the second. Also note that the variable $s$ does not show up in the objective of the second problem and thus will
not affect the optimal solution. Next we must show that we can re-write this such that all variables are subject to non-negativity constraints. 
To do this we can simply define new variables that are the positive and negative parts of the existing variables. Let 
$x_i^+ := \max\{x_i, 0\}$ and $x_i^- := \max\{-x_i, 0\}$. Thus $x_i = x_i^+ - x_i^-$ with $x_i^+, x_i^- \geq 0$ so we may re-write the problem as: 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax_i^+ - Ax_i^- + s = b \\
&\;\;\;\;\;\;\; s \geq 0 \\
&\;\;\;\;\;\;\; x_i^+ \geq 0 \\
&\;\;\;\;\;\;\; x_i^- \geq 0
\end{align*}
To be absolutely clear that we have accomplished our goal of writing this as a standard form problem, we can define $\tilde{A} := \begin{pmatrix} A & -A & I \end{pmatrix} \in \R^{m \times 3n}$, 
$\tilde{x} := \begin{pmatrix} x^+ \\ x^- \\ s \end{pmatrix} \in \R^{3n}$, and $\tilde{c} := \begin{pmatrix} c \\ 0 \end{pmatrix} \in \R^{3n}$. Then the above problem can be written: 
\begin{align*}
&\min \tilde{c}^T \tilde{x} \\
&\text{s.t. } \tilde{A}\tilde{x} = b \\
&\;\;\;\;\;\;\; \tilde{x} \geq 0
\end{align*}

\end{proof}

Recall that $A$ is an $m \times n$ matrix. When discussing problems in standard form it is customary to make the assumption $m \leq n$ for the following reason. It can be shown that 
linearly dependent constraints (that is, when $A$ has linearly dependent columns) are in fact redundant. Without loss of generality we can then assume that $A$ has linearly independent 
rows. Since each row of $A$ is in $\R^n$ then this assumption implies the condition $m \leq n$.  

Standard form problems have a very nice interpretation. If we let $A_i$ denote column $i$ of $A$ then the constraints can be written: 
\begin{align*}
&\sum_{i = 1}^{n} A_i x_i = b\\
&\;\;\;\;\;\;\; x \geq 0
\end{align*}
So we may view the columns $A_i$ as ``resource vectors'' and the $x_i$ are non-negative amounts of each resource, subject to the constraint that the total amount of resources must
sum to $b$. The task is thus to minimize the cost subject to this resource constraint. 

In general both the general form and standard forms of a linear program are useful
to work with. The general form (that is, with constraint $Ax \leq b$) is often convenient when developing the theory of linear programming. However, the standard form often tends to be 
more computationally convenient and hence is typically used for the design of algorithms. 
	
\section{LP Duality}
Duality is a very important concept in optimization; more generally, it is a property of convex programming but here we consider duality in the special case of linear programming. Duality 
is a special characterization of optimality in LPs, and thus something that can be exploited to help design optimization algorithms. Before immediately introducing duality, we start by asking 
a different question that will lead us on the path to duality: when is a LP feasible? In other words, when in the feasible set non-empty? Now we see the convenience of the standard form LP, since 
the feasible set is simply defined by a linear system of equations (plus the non-negativity constraints, but let's ignore that for now). Thus, the equivalent question is: does $Ax = b$ have a solution. 
From linear algebra, we already know how to answer this; just use Gaussian elimination! The results from this method are summarized in the following theorem. 
\begin{thm}
Let $A \in \R^{m \times n}$ and $b \in \R^m$, exactly one and only one of the following statements holds: 
\begin{enumerate}
\item $x \in \R^n$ is a solution to $Ax = b$.
\item There exists $y \in \R^m$ such that $y^T A = 0$ but $y^T b \neq 0$. 
\end{enumerate}
When the system does not have a solution then a vector $y \in \R^m$ satisfying the second statement is called a \textbf{certificate of infeasibility}. 
\end{thm}
The second statement might look a bit weird at first glance, but it is actually very intuitive. Suppose we have a system of two equations $E_1$ and $E_2$. We learn in elementary math 
that one strategy to solve such a system is to add the equations together and then solve the resulting single equation. More generally, we might take a linear combination of the two equations
such as $y_1 E_1 + y_2 E_2$ for some $y_1, y_2 \in \R$. If this results in an equation of the form $0 = d$, where $d \neq 0$ we know this is impossible and hence the equation does not have a 
solution. That's all this second statement says; the vector $y$ produces a linear combination of the system of equations and if the result is unsolvable then so is the original system. 

As I briefly alluded to before, the above theorem by itself is not that useful for our current problem as it ignores the non-negativity constraints $x \geq 0$. A result known as \textit{Farkas' Lemma}
gives the analogous result in this case. 
\begin{thm}
Let $A \in \R^{m \times n}$ and $b \in \R^m$, exactly one and only one of the following statements holds: 
\begin{enumerate}
\item $x \in \R^n$ is a solution to $Ax = b$, $x \geq 0$.
\item There exists $y \in \R^m$ such that $y^T A \leq 0$ but $y^T b > 0$. 
\end{enumerate}
\end{thm}
This result is exactly what we would have hoped for; it is basically a direct analog of the previous well-known result from linear algebra but adapted to consider the non-negativity constraints. 

Now we begin to consider duality. Recall that the ultimate goal is to design methods and algorithms to solve LPs. We have seen one way to attack this problem; namely, the Simplex Method. 
Duality theorem will give us a different and powerful avenue of attack for certain problems. The basic idea is to first find upper and lower bounds on the optimal solution. If we are able to tighten 
these bounds then we can get closer and closer to the solution. Ideally we could continue this logic and find the \textit{least upper bound} and \textit{greatest lower bound}, which would then yield
the true solution. The \textit{primal-dual} method does exactly this. 

\bigskip

\textbf{Finding upper bounds:} The naive approach here is simply to guess solutions (i.e. try to pick a point in the feasible set) and then evaluate the solution in some way to determine its 
quality. In particular, if we correctly guess a feasible point and then evaluate the objective function using this point, then the result will be an upper bound on the true minimum. If we are able 
to guess multiple feasible points then we can keep doing this and refining this upper bound. However, it is not obvious how to guess such solutions. For high-dimensional problems especially
this seems like a very challenging task. 

\bigskip

\textbf{Finding lower bounds:} Consider a nice example courtesy of Aaron Singer's lecture notes (see resources). 
\begin{align*}
\min \ &x_1 + 2x_2 + 4x_3 \\
 \text{s.t. } &x_1 + x_2 + 2x_3 = 5 \\
 &2x_1 + x_2 + 3x_3 = 8 \\
 &x_1, x_2, x_3 \geq 0
\end{align*}
Recall that any solution to this system of equations must also solve linear combinations of these equations. So here's an idea: Consider taking twice the first 
constraint equation minus the second; that is
\[\begin{pmatrix} 2 & -1\end{pmatrix}^T A = \begin{pmatrix} 2 & -1\end{pmatrix}^Tb\]
Carrying this out yields the equation: 
\[0x_1 + x_2 + x_3 = 2\]
So the optimal solution, whatever it may be, \textit{must} satisfy this equation. But take a look at the objective function
\[x_1 + 2x_2 + 4x_3\]
We notice that each of the values scaling the objective function is strictly larger than those in the above equation that the solution must satisfy 
(1 > 0, 2 > 1, 4 > 1). Since the $x_i$ are constrained to be non-negative this means that the true minimum value can be no smaller than 2. We conclude 
that 2 is a lower bound on the true minimum. If we happened to pick a linear combination of the constraints and a feasible point that resulted in lower and 
upper bounds of the same value, then we would know we found the optimal solution. This seems very unlikely, but the ideas explored here turn out to 
be powerful. 

\subsection{Primal-Dual Theory}
We will call the standard LP we have been working with the \textbf{primal problem}. 
\begin{align*}
&\min c^T x \\
&\text{s.t. } Ax = b \\
&\;\;\;\;\;\;\; x \geq 0
\end{align*}

The \textbf{dual problem} seeks to solve the same problem by maximizing a lower bound on the optimal solution in the primal problem. The definition of the dual simply 
follows from generalizing the lower bound example presented above. Recall what we did: we took a linear combination of the constraint equations $y^T A = y^T b$, where 
the vector $y$ encodes the coefficients in the linear combinations. Then we concluded that $y^T b$ was a lower bound on the optimal solution provided that the coefficients
resulting from $y^T A$ were smaller than the respective coefficients in the cost vector $c$; that is, $y^T A \leq c^T$. The dual problem simply considers the maximum of lower 
bounds of this form. 
\begin{align*}
&\min y^T b \\
&\text{s.t. } y^T A \leq c \\
\end{align*}

From the preceding example, we suspect that these two problems should give the same solution. We proceed to make this intuition rigorous. We begin with a slightly less
ambitious goal, simply proving that the dual formulation provides a lower bound on the optimal solution. This is known as \textbf{weak duality}. 

\begin{thm}
Let $x \in \R^n$ and $y \in \R^m$ be solutions to the primal and dual problems, respectively. Then $y^T b \leq c^T x$. 
\end{thm}

\begin{proof}
This simply uses the feasibility of $x$ and $y$ in the respective problems. 
\begin{align*}
y^T b &= y^T(Ax) && Ax = b \text{ from primal} \\
	 &= (y^T A)x \\
	 &\leq c^T x && y^T A \leq c^T \text{ from dual and } x \geq 0 \text{ from primal} \\
\end{align*}
\end{proof}

We now state the major result in primal-dual theory, that the problems yield the same solution. 
\begin{thm}
If either the primal or dual problem is feasible, then they share the same optimal solution. 
\end{thm}

\begin{proof}
\textbf{TODO}
\end{proof}

\textbf{TODO}: Include proof that dual of the dual is the primal. Discuss how infeasibility can be concluded between the problems, etc.  

\subsection{Applications of Duality}
The primal-dual theory is very nice and all, but how do we actually use it? This section details applications to game theory and learning theory. 

\subsubsection{Two-Player Games and the Minimax Theorem}
We consider the basic setup of two-player games from game theory. Suppose players A and B have sets of strategies $S_A = \{1, \dots, m\}$ and 
$S_B = \{1, \dots, n\}$, respectively. We also suppose the two players have respective \textit{payoff matrices} $A \in \R^{m \times n}$ and 
$B \in \R^{m \times n}$. These matrices are defined such that player 1 gets payoff $A_{ij}$ and player 2 gets payoff $B_{ij}$ when 
the players choose respective strategies $i \in S_A$, $j \in S_B$. We will assume the players are rational in the sense that they are utility-maximizers. 
This leads us to the notion of a \textbf{Nash Equilibrium}. Intuitively, a Nash Equilibrium is a situation in which, given the other players strategy, neither one
would be better off transitioning to a new strategy. It is in this sense that the game is in equilibrium. 
\begin{definition}
 A Nash equilibrium is a strategy profile $(i, j) \in S_A \times S_B$ such that each player's strategy is optimal given the other player's strategy; that is, 
 \begin{itemize}
 \item $A_{ij} \geq A_{kj} \text{ for all } k \in S_A$ 
 \item $B_{ij} \geq B_{i\ell} \text{ for all } \ell \in S_B$
 \end{itemize}
\end{definition}

We me generalize the idea of two-player games by considering that each players strategy is picked at random. That is, for player A we associate their 
set of strategies $S_A$ with a probability vector $p_A \in \Delta_A$ such that $(p_A)_i$ is the probability that player A employs strategy $i \in S_A$. 
$\Delta_A$ is the set of all possible probability vectors for player $A$,
\[ \Delta_A := \left\{p = (p_1, \dots, p_m) : p_i \in [0, 1] \text{ and } \sum_{i = 1}^{m} p_i = 1\right\} \]
While in the non-random setting, player A picked a strategy $i \in S_A$, in this setting player A picks a probability vector $p_A \in \Delta_A$; that is, the player 
gets to choose the probability of each strategy but not the strategy itself. 
The vector $p_B$ is defined analogously for player B. 
$p_A$ is called the \textit{mixed strategy} for player $A$ and the pair $p_A, p_B$ is called a \textit{mixed strategy profile}. Notice that the mixed strategy, say for A, is just 
a probability distribution over the strategies $S_A =\{1, \dots, m\}$. For clarity, we refer to the elements of $S_A$ as \textit{pure strategies} with the mixed strategies 
then being a probability distribution over pure strategies. 

In the mixed strategy setting the notion of payoff is replaced with \textit{expected payoff}. The expected payoff for 
player A is: 
\[\sum_{(i, j) \in S_A \times S_B} A_{ij} (p_A)_i (p_B)_j = (p_A)^T A p_B\]
The expected payoff is conveniently written as the above quadratic form. We may now generalize the notion of Nash equilibrium to this random setting. The 
definition is basically identical to the previous case, just replacing \text{payoff} with \textit{expected payoff}. 
\begin{definition}
A mixed Nash equilibrium is a mixed strategy profile $p_A \in \Delta_A, p_B \in \Delta_B$ such that the expected payoff for each player is optimal given the other 
players mixed strategy; that is, 
\begin{itemize}
\item $(p_A)^T A p_B \geq (p^\prime_A)^T A p_B$ \text{ for all } $p^\prime_A \in \Delta_A$
\item $(p_A)^T B p_B \geq (p_A)^T B p^\prime_B$ \text{ for all } $p^\prime_B \in \Delta_B$
\end{itemize} 
\end{definition}

Now, one final definition to complete our brief foray into game theory. 
\begin{definition}
A two-player game is called a \textbf{zero-sum game} if the payoff matrices of the players satisfy $A = -B$. 
\end{definition}
This simply means that a gain for one player results in a loss of equal size for the other player, and vice versa; that is, cooperation if futile. 

Now, the question is: how does this all connect back to LPs? Well, the major result in the theory of two-player games is that every two-player, zero-sum
game has a mixed Nash equilibrium. The proof of this famous result relies on another result known as the minimax theorem, which in turn is proved using
strong duality. We should also emphasize that the minimax theorem is very important in and of itself and generalizations of this theorem have found 
many applications in machine learning, statistics, decision theory, etc. 

\begin{thm}
(Von Neumann's Minimax Theorem) For any matrix $A \in \R^{\abs{M} \times \abs{N}}$, 
\[ \max_{x \in \Delta_A} \min_{y \in \Delta_B} x^T A y = \min_{y \in \Delta_B} \max_{x \in \Delta_A} x^T A y \]
\end{thm}
Before the proof, a couple comments. First, note that I have switched notation to $x := p_A$, $y := p_B$ to reduce clutter. Now let's break down what this is saying.

\bigskip

\textbf{Lefthand side:}
\begin{itemize}
\item $L_x := \min_{y \in \Delta_B} x^T A y$: For a fixed mixed strategy for player $A$, this is the minimum expected payoff for A across all possible mixed strategies for player B.  
In other words, this is a lower bound on expected payoff for player $A$ given player A's mixed strategy $x$. 
\item $\max_{x \in \Delta_A} L_x$: This maximizes the above lower bound over all possible mixed strategies for player A.  That is, we can think of player A as choosing a mixed strategy 
$x$ that takes into account all possible strategies for player B and chooses $x$ such that player A's expected payoff in the worst-case scenario is maximized. 
\end{itemize}

\bigskip

\textbf{Righthand side:}
\begin{itemize}
\item $U_y := \max_{x \in \Delta_A} x^T A y$: For a fixed mixed strategy for player B, this is the maximum expected payoff for player A across all possible strategies for player A. In other words, 
this is an upper bound on A's expected payoff given a fixed strategy for B. 
\item $\min_{y \in \Delta_B} U_y$: Here we can think of player B as trying to minimize A's maximum expected payoff across all strategies for B. 
\end{itemize}

So what this theorem is telling us is that the following two approaches by the players A and B will result in the same outcome. 
\begin{itemize}
\item Player A chooses a strategy that maximizes their worst-case expected payoff across all possible strategies for player B. 
\item Player B chooses a strategy that minimizes player A's best-case expected payoff across all possible strategies for player A. 
\end{itemize}

We can actually make this even more intuitive. Suppose the game is sequential (one player plays a strategy, then the second player responds). The question is: does it matter 
which player goes first? Let's see what happens. 

\bigskip

\textbf{Player B goes first}. Suppose B starts by playing $\hat{y} \in \Delta_B$. Then naturally A will respond by playing $\hat{x} \in \argmax_{x \in \Delta_A} x^T A \hat{y}$; of course A will just 
choose the strategy to maximize their own payoff as best they can given what B already played. But B knows in advance that A will choose this! 
So $B's$ initial strategy will then naturally be: $\hat{y} \in \argmin_{y \in \Delta_B} \max_{x \in \Delta_A} x^T A y$. That is, B will choose the strategy that minimizes A's payoff. Since this is 
a zero-sum game this is equivalent to maximizing B's payoff. This implies A's expected payoff is $\min_{y \in \Delta_B} \max_{x \in \Delta_A} x^T A y$ when B goes first. 

\bigskip

\textbf{Player A goes first}: The same reasoning works here. Player A knows that player B will respond with a strategy that minimizes A's payoff given the strategy A played. So with this knowledge
A will choose the strategy to maximize their own payoff. Thus, A will 
play $\hat{x} \in \argmax_{x \in \Delta_A} \min_{y \in \Delta_B} x^T A y$. This implies A's expected payoff is $\max_{x \in \Delta_A} \min_{y \in \Delta_B} x^T A y$ when A goes first. 
Note that we've phrased all of these statements in terms of A's payoff; we could of course have done the same thing with B, 
but due to the zero-sum game assumption we can easily switch between the two formulations since $A = -B$. 

\bigskip

\textbf{So does the order of play matter?} Well, going second certainly couldn't hurt (that is, having more information can't be worse) so
\[\min_{y \in \Delta_B} \max_{x \in \Delta_A} x^T A y \geq \max_{x \in \Delta_A} \min_{y \in \Delta_B} x^T A y\]
This just says A's expected payoff going second is at least as large as when A goes first. So going second can't hurt. But can going second actually help? The minimax theorem can be viewed
as an answer to this question, with the answer being no! Viewed in this sequential game interpretation, this is quite a nice result to have to build a theory of games. It means (under these assumptions 
at least) we don't need to consider different cases for which player goes first. The same result will occur either way. 


Now let's proceed with the proof. 
\begin{proof}
The basic idea here is to re-write both sides of the equation as LPs such that one is the dual of the other, and then apply strong duality. To this end, let's first consider the lefthand side. The first 
step is to re-write
\[\min_{y \in \Delta_B} x^T A y\]
Recall that $y$ must satisfy the constraints of a probability mass function. Thus to minimize $x^T A y$ the trick is to consider the smallest value in the vector $x^T A$ and then put all of the probability
mass into the component of $y$ corresponding to that smallest value. This yields, 
\[\min_{y \in \Delta_B} x^T A y = \min_{j \in S_B} (x^T A)_j = \min_{j \in [n]} (x^T A)_j\]
where the second equality follows from $S_B = \{1, \dots, n\} =: [n]$. Thus, the lefthand side can be written as: 
\begin{align*}
&\max_{x \in \Delta_A}  \min_{j \in [n]} (x^T A)_j \\
\end{align*}
We can introduce the variable $t := \min_{j \in [n]} (x^T A)_j$ and then re-write this as
\begin{align*}
&\max_{x \in \Delta_A}  t \\
&\text{s.t. } t \leq (x^T A)_j \ \forall j \in [n]  \\
\end{align*}
Then since $x \in \Delta_A$ is simply constraining $x$ to be a valid probability mass function then we can write these constraints out explicitly. 
\begin{align*}
\max & \ t \\
\text{s.t. } &t \leq (x^T A)_j \ \forall j \in [n]  \\
&\sum_{j = 1}^{n} x_j = 1 \\
&x \geq 0
\end{align*}
Note that we do not need to explicitly constraint $x \in [0, 1]$ since requiring the $x_i$ to be non-negative and to sum to $1$ implicitly implies this constraint. This is a linear 
program. Now for the righthand side. The reasoning is essentially exactly the same so we will go through this more quickly. We have 
\[ \max_{x \in \Delta_A} x^T A y = \max_{i \in [m]} (Ay)_i\]
We consider a new variable $s := \max_{i \in [m]} (Ay)_i$, which then yields: 
\begin{align*}
\min & \ s \\
\text{s.t. } &s \geq (Ay)_i \ \forall i \in [m]  \\
&\sum_{i = 1}^{m} y_i = 1 \\
&y \geq 0
\end{align*}
These two problems can shown to be dual to each other (consider what the values of $c, b$, etc. are for each LP to see this). Thus by strong duality the theorem is proved. 
\end{proof}

Now to the big result: the proof of existence of mixed Nash Equilibria in zero-sum games. Note that it was John Nash who originally proved this theorem by other means, but John von Neumann
proved the minimax theorem and showed that this result implied the Nash equilibrium result. 

\begin{thm}
Every zero-sum game has a mixed Nash Equilibrium. 
\end{thm}

\begin{proof}
I claim that $(x_*, y_*)$ is a mixed NE, where:
\begin{align*}
x_* &:= \argmax_{x \in \Delta_A} \min_{y \in \Delta_B} x^T A y \\
y_* &:= \argmin_{y \in \Delta_B} \max_{x \in \Delta_A} x^T A y
\end{align*}

Let's recall what a mixed NE is; we're trying to show: 
\begin{align*}
x_*^T A y_* &\geq x^T A y_* \text{ for all }  x \in \Delta_A \\
x_*^T B y_* &\geq x_*^T B y \text{ for all } y \in \Delta_B 
\end{align*}
Since this is a zero-sum game $A = -B$ so we can write the second line in terms of A: 
\[x_*^T A y_* \leq x_*^T A y \text{ for all } y \in \Delta_B \]
This is just expressing the fact the B maximizes their expected payoff by minimizing A's expected payoff. We can re-write these requirements for a mixed NE as
\begin{align*}
x_*^T A y_* &= \max_{x \in \Delta_A} x^T A y_* \\
x_*^T A y_* &= \min_{y \in \Delta_B} x_*^T B y 
\end{align*}
To show this, consider the following. 
\[\max_{x \in \Delta_A} \min_{y \in \Delta_B} x^T A y = \min_{y \in \Delta_B} x_*^T A y \leq x_*^T A y_* \leq \max_{x \in \Delta_A} x^T A y_* = \min_{y \in \Delta_B} \max_{x \in \Delta_A} x^T A y \]
The inner inequalities follow directly from the definition of $\min$ and $\max$, while the outer equalities follow from the definition of $x_*, y_*$ at the beginning of the proof. Now, from the minimax 
theorem we known that 
\[\max_{x \in \Delta_A} \min_{y \in \Delta_B} x^T A y = \min_{y \in \Delta_B} \max_{x \in \Delta_A} x^T A y \]
Since the other terms are sandwiched between these expressions we then know that the inequalities must be equalities; that is, 
\[\min_{y \in \Delta_B} x_*^T A y = x_*^T A y_* = \max_{x \in \Delta_A} x^T A y_*\]
which is precisely what we wanted to show. 
\end{proof}


\subsubsection{Learning Theory and Boosting}
We now consider a second application of duality, particularly the notion of binary classification in theoretical machine learning. As setup, suppose we seek to classify objects from a set
$\mathcal{X}$. We observe data which is sampled from an unknown distribution $q$ on $\mathcal{X}$. To address the classification problem, the first step is to establish the class of 
models we will be using. Formally, we call this the \textit{hypothesis class} $\mathcal{H} = \{h | h: \mathcal{X} \to \{0, 1\}\}$. To keep things simple we assume both $\mathcal{X}$ and 
$\mathcal{H}$ are finite sets. 

In the hope of developing any theory at all, we must make some standing 
assumptions. What if the problem is simply impossible in the sense that no classifier could ever do better than chance? We want to restrict such a possibility, so we make the \textit{weak learning assumption}, 
which intuitively says that there exists some $h \in \mathcal{H}$ which is correct at least half the time (at least a little better than chance). For $x \in \mathcal{X}$ let $c(x)$ denote the correct class. Then the 
assumption formally states: 
\[\exists \gamma > 0 \text{ s.t. } \forall q \ \exists h: \mathcal{X} \to \{0, 1\} \text{ such that } \Prob_{x \sim q}\left(h(x) \neq c(x)\right) \leq \frac{1 - \gamma}{2}\]
We can also re-write this a second way. If we let $W_{h}(x) = \mathbb{I}\{h(x) \neq c(x)\}$ be the indicator random variable that assumes the value 1 if $h$ incorrectly classifies $x$ and $0$ otherwise, then 
the weak learning assumption simply says there exists $h \in \mathcal{H}$ with $\E_{x \sim q} [W_h(x)] \leq \frac{1 - \gamma}{2}$. We can also write out this expectation explicitly (which will turn out to be useful
in the below proof): 
\[\E_{x \sim q} [W_h(x)] = \sum_{i = 1}^{\abs{\mathcal{X}}} q(x_i) W_h(x_i) \leq \frac{1 - \gamma}{2}\] 

It turns out that even under this very mild assumption we can actually prove some powerful theoretical results. The basic idea is as follows: even if there is just a single classifier that barely classifies better than 
chance, we can exploit this by taking many independent 

\begin{thm}
Suppose $\mathcal{H}$ satisfies the weak learning assumption. Then there exists a distribution $p$ on $\mathcal{H}$ such that the weighted majority classifier 
\[c_p(x) = \begin{cases} 
      1 & \sum_{h \in \mathcal{H}} p(x)h(x) \geq \frac{1}{2} \\
      0 & \text{otherwise}
   \end{cases}
\]
is always correct: $c_p(x) = c(x) \ \forall x \in \mathcal{X}$
\end{thm}

\begin{proof}
Recall from above that the weak learning assumption implies the existence of a classifier $h_j \in \mathcal{H}$ satisfying 
\[\sum_{i = 1}^{\abs{\mathcal{X}}} q(x_i) W_{h_i}(x_i) \leq \frac{1 - \gamma}{2}\]
Define the matrix $M_{ij} \in \{-1, +1\}^{\abs{X} \times \abs{H}}$ by 
\[M_{ij}  = \begin{cases} 
      +1 & h_j(x_i) \neq c(x_i) \\
      -1 & h_j(x_i) = c(x_i)
   \end{cases}
\]
We then have the relationship $M_{ij} = 2W_{h_j}(x_i) - 1$. With this in mind, let's re-write the above inequality. 
\[\sum_{i = 1}^{\abs{\mathcal{X}}} \frac{1}{2}q(x_i) \left[2W_{h_i}(x_i) - 1\right] + \sum_{i = 1}^{\abs{\mathcal{X}}} q(x_i) \leq \frac{1 - \gamma}{2}\]
Since $\sum_{i = 1}^{\abs{\mathcal{X}}} q(x_i) = 1$, and plugging in the above expression for $M_{ij}$, this can be re-written as
\[\sum_{i = 1}^{\abs{\mathcal{X}}} q(x_i)M_{ij} \leq -\gamma \]
If we abuse notation and consider $q$ to be a vector with $q_i := q(x_i)$ then we may write this as $qM_j \leq -\gamma$, where $M_j$ is column $j$ of $M$. Alternatively, 
\[qMe_j \leq -\gamma\]
where $e_j$ is the $i^{\text{th}}$ standard basis vector of $\R^{\abs{\mathcal{H}}}$. So weak learning says that for \textit{any} distribution $q$ on $\mathcal{X}$
\textbf{TODO: Finish this proof}
\end{proof}

	
\section{TODO}
\begin{itemize}
\item Shortest path LP (lecture 18)
\end{itemize}
	
	
\end{document}
	
	